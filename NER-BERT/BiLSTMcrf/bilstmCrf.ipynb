{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置config\n",
    "class TrainingConfig(object):\n",
    "    epoches = 15\n",
    "    evaluate_every = 100\n",
    "    checkpoint_every = 100\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "\n",
    "class ModelConfig(object):\n",
    "    embedding_size = 100\n",
    "    \n",
    "    hidden_sizes = [128]  # LSTM结构的神经元个数，量实现多层可以在这个列表中添加hiddenSize，如[256, 128]\n",
    "    \n",
    "    layers = [128]  # 全连接层的hidden_size，支持多层全连接层和只有softmax层，若只有softmax层，则列表为空\n",
    "    \n",
    "    dropout_keep_prob = 0.5\n",
    "    l2_reg_lambda = 0.0\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    batch_size = 64\n",
    "    \n",
    "    train_data_source = \"../data/train.txt\"\n",
    "    eval_data_source = \"../data/dev.txt\"\n",
    "    test_data_source = \"../data/test.txt\"\n",
    "    \n",
    "    num_classes = 7\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "    \n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class DataGenerator(object):\n",
    "    def __init__(self, config):\n",
    "        self._train_data_source = config.train_data_source\n",
    "        self._eval_data_source = config.eval_data_source\n",
    "        \n",
    "        self._embedding_size = config.model.embedding_size\n",
    "        self._batch_size = config.batch_size\n",
    "        self._num_classes = config.num_classes\n",
    "        \n",
    "        self.train_datas = []\n",
    "        self.train_labels = []\n",
    "        \n",
    "        self.eval_datas = []\n",
    "        self.eval_labels = []\n",
    "        \n",
    "        self.word_embedding =None\n",
    "        \n",
    "        self.vocab_len = 0 \n",
    "        self._word_to_index = {}\n",
    "        self.label_to_index = {}\n",
    "        \n",
    "    def _read_file(self, file_path):\n",
    "        # 读取原始数据\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentence = []\n",
    "            label = []\n",
    "            for line in f.readlines():\n",
    "                if line != \"\\n\":\n",
    "                    char, target = line.strip().split()\n",
    "                    sentence.append(char)\n",
    "                    label.append(target)\n",
    "                else:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "        \n",
    "        return sentences, labels\n",
    "    \n",
    "    \n",
    "    def _read_vocab(self, sentences, labels):\n",
    "        # 创建的词汇表\n",
    "        all_words = [word for sentence in sentences for word in sentence]\n",
    "        word_count = Counter(all_words)\n",
    "        sort_word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [\"<PAD>\", \"<UNK>\"] + [item[0] for item in sort_word_count if item[1] > 0] \n",
    "        self.vocab_len = len(words)\n",
    "        self._word_to_index = dict(zip(words, range(len(words))))\n",
    "        \n",
    "        with open(\"../data/json/word_to_index.json\", \"w\") as f:\n",
    "            json.dump(self._word_to_index, f)\n",
    "        \n",
    "        all_lables = list(set([item for label in labels for item in label]))\n",
    "        self.label_to_index = dict(zip(all_lables, range(len(all_lables))))\n",
    "        \n",
    "        with open(\"../data/json/label_to_index.json\", \"w\") as f:\n",
    "            json.dump(self.label_to_index, f)\n",
    "        \n",
    "    def _process_data(self, x, is_data=True):\n",
    "        # 将词或者标签转换成数字\n",
    "        if is_data:\n",
    "            newX = [self._word_to_index[item] if item in self._word_to_index else self._word_to_index[\"<UNK>\"] for item in x]\n",
    "            \n",
    "        if not is_data:\n",
    "            newX = [self.label_to_index[item] for item in x]\n",
    "        \n",
    "        return newX\n",
    "        \n",
    "    def _gen_train_eval(self, datas, labels):\n",
    "        # 生成训练集和验证集\n",
    "        new_datas = [self._process_data(data, is_data=True) for data in datas]\n",
    "        new_labels = [self._process_data(label, is_data=False) for label in labels]\n",
    "        \n",
    "        return new_datas, new_labels\n",
    "        \n",
    "        \n",
    "    def gen_data(self):\n",
    "        # 调用内部方法，生成训练集和测试集\n",
    "        \n",
    "        # 读取txt文件\n",
    "        train_datas, train_labels = self._read_file(self._train_data_source)\n",
    "        eval_datas, eval_labels = self._read_file(self._eval_data_source)\n",
    "        \n",
    "        # 利用训练集生成映射表\n",
    "        self._read_vocab(train_datas, train_labels)\n",
    "        \n",
    "        # 生成训练集和测试集\n",
    "        train_datas, train_labels = self._gen_train_eval(train_datas, train_labels)\n",
    "        eval_datas, eval_labels = self._gen_train_eval(eval_datas, eval_labels)\n",
    "        \n",
    "        self.train_datas = train_datas\n",
    "        self.train_labels = train_labels\n",
    "        self.eval_datas = eval_datas\n",
    "        self.eval_labels = eval_labels\n",
    "        \n",
    "    def _one_hot(self, y):\n",
    "        \"\"\"\n",
    "        对每个位置的标签用one-hot的形式表示\n",
    "        \"\"\"\n",
    "        label_vec = [0] * self._num_classes\n",
    "        new_y = []\n",
    "        for labels in y:\n",
    "            sub_y = []\n",
    "            for label in labels:\n",
    "                label_vec = [0] * self._num_classes\n",
    "                label_vec[label] = 1\n",
    "                sub_y.append(label_vec)\n",
    "                \n",
    "            new_y.append(sub_y)\n",
    "        \n",
    "        return new_y\n",
    "                \n",
    "\n",
    "    def _format_data(self, x, y):\n",
    "        \"\"\"\n",
    "        对输入的数据按照最大的长度补齐\n",
    "        \"\"\"\n",
    "        sequence_len = [len(data) for data in x]\n",
    "        max_len = max(sequence_len)\n",
    "        new_x = [x[i] + [self._word_to_index[\"<PAD>\"]] * (max_len - sequence_len[i]) for i in range(len(x))]\n",
    "        new_y = self._one_hot([y[i] + [self.label_to_index[\"O\"]] * (max_len - sequence_len[i]) for i in range(len(y))])\n",
    "        \n",
    "        return dict(input_x=new_x, input_y = new_y, max_len=max_len, sequence_len=sequence_len)\n",
    "   \n",
    "    def next_batch(self, x, y):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "        # 每一个epoch时，都要打乱数据集\n",
    "        mid_val = list(zip(x, y))\n",
    "        random.shuffle(mid_val)\n",
    "        x, y = zip(*mid_val)\n",
    "        x = list(x)\n",
    "        y = list(y)\n",
    "\n",
    "        num_batches = len(x) // self._batch_size\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start = i * self._batch_size\n",
    "            end = start + self._batch_size\n",
    "            batch_x =x[start: end]\n",
    "            batch_y = y[start: end]\n",
    "\n",
    "            yield self._format_data(batch_x, batch_y)    \n",
    "    \n",
    "\n",
    "dataGen = DataGenerator(config)\n",
    "dataGen.gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data length: 20864\n",
      "eval data length: 2318\n",
      "vocab length: 4314\n",
      "label numbers: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"train data length: {}\".format(len(dataGen.train_datas)))\n",
    "print(\"eval data length: {}\".format(len(dataGen.eval_datas)))\n",
    "print(\"vocab length: {}\".format(dataGen.vocab_len))\n",
    "print(\"label numbers: {}\".format(len(dataGen.label_to_index.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTMCrf(object):\n",
    "    \"\"\"\n",
    "    Bi-LSTM+crf用于命名实体识别\n",
    "    \"\"\"\n",
    "    def __init__(self, config, vocab_len, use_crf=False):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, None], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, None, config.num_classes], name=\"input_y\")\n",
    "        self.sequence_len = tf.placeholder(tf.int32, [None], name=\"sequence_len\")\n",
    "        self.max_len = tf.placeholder(tf.int32, name=\"max_len\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_len, config.model.embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            \n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embedded_words = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            \n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "\n",
    "            for idx, hidden_size in enumerate(config.model.hidden_sizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hidden_size, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropout_keep_prob)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hidden_size, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropout_keep_prob)\n",
    "\n",
    "\n",
    "                    # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "                    # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "                    # current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "                    outputs, current_state = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, \n",
    "                                                                             self.embedded_words, dtype=tf.float32,\n",
    "                                                                             scope=\"bi-lstm\" + str(idx))\n",
    "        \n",
    "                    # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2]\n",
    "                    self.embedded_words = tf.concat(outputs, 2)\n",
    "        \n",
    "        output_size = config.model.hidden_sizes[-1] * 2  # 因为是双向LSTM，最终的输出值是fw和bw的拼接，因此要乘以2\n",
    "        output = tf.reshape(self.embedded_words, [-1, output_size])  # reshape成全连接层的输入维度\n",
    "        \n",
    "        with tf.name_scope(\"hidden_layer\"):\n",
    "            for idx, hidden_size in enumerate(config.model.layers):\n",
    "                with tf.name_scope(\"hidden_layer\" + str(idx)):\n",
    "                    hidden_w = tf.get_variable(\"hidden_w\",\n",
    "                                              shape=[output_size, hidden_size],\n",
    "                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                    hidden_b = tf.Variable(tf.constant(0.1, shape=[hidden_size]), name=\"hidden_b\")\n",
    "                    l2_loss += tf.nn.l2_loss(hidden_w)\n",
    "                    l2_loss += tf.nn.l2_loss(hidden_b)\n",
    "                    # 加一层tanh激活函数\n",
    "                    output = tf.tanh(tf.nn.xw_plus_b(output, hidden_w, hidden_b, name=\"hidden_output\"))\n",
    "                    output_size = hidden_size\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output_layer\"):\n",
    "            output_w = tf.get_variable(\n",
    "                \"output_w\",\n",
    "                shape=[output_size, config.num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            output_b= tf.Variable(tf.constant(0.1, shape=[config.num_classes]), name=\"output_b\")\n",
    "            l2_loss += tf.nn.l2_loss(output_w)\n",
    "            l2_loss += tf.nn.l2_loss(output_b)\n",
    "            self.logits = tf.nn.xw_plus_b(output, output_w, output_b, name=\"logits\")\n",
    "            self.new_logits = tf.reshape(self.logits, [-1, self.max_len, config.num_classes])\n",
    "            \n",
    "            # 根据传入的序列长度取出实际的序列，mask的维度[batchSize, maxLen]，mask中的值为布尔值\n",
    "            mask = tf.sequence_mask(self.sequence_len)\n",
    "            self.true_y = tf.argmax(self.input_y, -1, name=\"true_y\", output_type=tf.int32)\n",
    "            \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if use_crf:\n",
    "                log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(self.new_logits, self.true_y, self.sequence_len)\n",
    "                self.trans_params = trans_params  # 转移矩阵\n",
    "                self.loss = tf.reduce_mean(-log_likelihood)\n",
    "                \n",
    "            else:\n",
    "                losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.new_logits, labels=self.input_y)\n",
    "                # 根据布尔值来去除位置为True的值\n",
    "                losses = tf.boolean_mask(losses, mask)\n",
    "                self.loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        with tf.name_scope(\"masked_output\"):\n",
    "            if use_crf:\n",
    "                viterbi_sequence, viterbi_score = tf.contrib.crf.crf_decode(self.new_logits, self.trans_params, self.sequence_len)\n",
    "                self.pred_y = viterbi_sequence\n",
    "            else:\n",
    "                self.predictions = tf.argmax(self.logits, 1, name=\"predictions\", output_type=tf.int32)\n",
    "                self.pred_y = tf.reshape(self.predictions, [-1, self.max_len], name=\"pred_y\")\n",
    "            \n",
    "            # boolean_mask会将二维的tensor转换成一维，因为mask也是二维。\n",
    "            # 维度计算公式=self.trueY的维度 - mask的维度 + 1\n",
    "            self.true_y = tf.boolean_mask(self.true_y, mask, name=\"masked_true_y\")\n",
    "            self.pred_y = tf.boolean_mask(self.pred_y, mask, name=\"masked_pred_y\")\n",
    "            \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # 计算accuracy\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.true_y, self.pred_y), \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def get_chunk_type(index, index_to_label):\n",
    "    \"\"\"\n",
    "    对实体的标签进行分割，返回实体的位置和实体的名称\n",
    "    \"\"\"\n",
    "    label_name = index_to_label[index]\n",
    "    label_class, label_type = label_name.split(\"-\")\n",
    "\n",
    "    return label_name, label_type\n",
    "\n",
    "\n",
    "def get_chunk(sequence, label_to_index):\n",
    "    \"\"\"\n",
    "    给定一个标注序列，将实体和位置组合起来，放置在一个列表中\n",
    "    \"\"\"\n",
    "    unentry = [label_to_index[\"O\"]]\n",
    "    index_to_label = {index: label for label, index in label_to_index.items()}\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for index, label in enumerate(sequence):\n",
    "        if label in unentry:\n",
    "            # 如果非实体词\n",
    "            if chunk_type is None:\n",
    "                # 若chunk_type为None，表明上一个词是非实体，继续跳过\n",
    "                continue\n",
    "            else:\n",
    "                # 若chunkType非None，则上面的是一个实体，而当前非实体，则将上一个实体chunk加入到chunks中\n",
    "                # 主要为序列中的这种情况，O,B-PER,I-PER,O 这也是最常见的情况\n",
    "                chunk = (chunk_type, chunk_start, index-1)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = None, None\n",
    "\n",
    "        if label not in unentry:\n",
    "            # 如果是实体词，在这里的label是索引表示的label\n",
    "            label_chunk_class, label_chunk_type = get_chunk_type(label, index_to_label)\n",
    "            if chunk_type is None:\n",
    "                # 若当前chunk_type为None，则表明上一个词是非实体词\n",
    "                chunk_type, chunk_start = label_chunk_type, index\n",
    "            elif label_chunk_type == chunk_type:\n",
    "                # 若实体类型和上一个相同，则做如下判断\n",
    "                if index == (len(sequence) - 1):\n",
    "                    # 若当前词是序列中的最后一个词，则直接返回chunk\n",
    "                    chunk = (chunk_type, chunk_start, index)\n",
    "                    chunks.append(chunk)\n",
    "                else:\n",
    "                    # 若当前非最后一个词，则跳过\n",
    "                    continue\n",
    "            elif label_chunk_type != chunk_type:\n",
    "                # 若当前词和上一个词类型不同，则将上一个实体chunk加入到chunks中，接着继续下一个chunk\n",
    "                # 主要体现在两个实体相连的序列中，如B-PER,I-PER,B-LOC,I-LOC\n",
    "                chunk = (chunk_type, chunk_start, index-1)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = label_chunk_type, index\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def gen_metrics(true_y, pred_y, label_to_index):\n",
    "    \"\"\"\n",
    "    生成f1值，recall, precision\n",
    "    precision = 识别的正确实体数/识别出的实体数\n",
    "    recall = 识别的正确实体数/样本的实体数\n",
    "    \"\"\"\n",
    "    correct_preds = 0  # 识别出的正确实体数\n",
    "    all_preds = 0  # 识别出的实体数\n",
    "    all_trues = 0  # 样本的真实实体数\n",
    "\n",
    "    true_chunks = get_chunk(true_y.tolist(), label_to_index)\n",
    "    pred_chunks = get_chunk(pred_y.tolist(), label_to_index)\n",
    "    correct_preds += len(set(true_chunks) & set(pred_chunks))\n",
    "    all_preds += len(pred_chunks)\n",
    "    all_trues += len(true_chunks)\n",
    "\n",
    "    precision = correct_preds / all_preds if correct_preds > 0 else 0\n",
    "    recall = correct_preds / all_trues if correct_preds > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if correct_preds > 0 else 0\n",
    "\n",
    "    return round(f1, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training model\n",
      "2019-01-24T10:11:04.467656, step: 1, loss: 83.59038543701172, acc: 0.12784899771213531, f1: 0.002, precision: 0.0011, recall: 0.0109\n",
      "2019-01-24T10:11:04.795430, step: 2, loss: 75.69819641113281, acc: 0.3560681939125061, f1: 0.0039, precision: 0.0021, recall: 0.0235\n",
      "2019-01-24T10:11:05.456501, step: 3, loss: 73.52793884277344, acc: 0.5631009340286255, f1: 0.0094, precision: 0.0054, recall: 0.037\n",
      "2019-01-24T10:11:05.942013, step: 4, loss: 55.293800354003906, acc: 0.7597693800926208, f1: 0.0093, precision: 0.006, recall: 0.0211\n",
      "2019-01-24T10:11:06.562464, step: 5, loss: 38.790428161621094, acc: 0.8548153638839722, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:06.963134, step: 6, loss: 34.26868438720703, acc: 0.8697394728660583, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:08.452968, step: 7, loss: 38.830909729003906, acc: 0.8178199529647827, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:08.801356, step: 8, loss: 27.227787017822266, acc: 0.8896216750144958, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:09.091760, step: 9, loss: 34.528038024902344, acc: 0.87346351146698, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:09.507139, step: 10, loss: 28.027099609375, acc: 0.8959217071533203, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:09.870554, step: 11, loss: 37.256961822509766, acc: 0.8602257370948792, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:10.264454, step: 12, loss: 27.330493927001953, acc: 0.8963799476623535, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:10.803481, step: 13, loss: 30.312084197998047, acc: 0.8724456429481506, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:11.266439, step: 14, loss: 23.463520050048828, acc: 0.8954440951347351, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:11.566993, step: 15, loss: 20.810699462890625, acc: 0.9016162753105164, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:11.975662, step: 16, loss: 26.321388244628906, acc: 0.8683298826217651, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:12.473830, step: 17, loss: 24.742563247680664, acc: 0.8969137668609619, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:12.867897, step: 18, loss: 27.215700149536133, acc: 0.8640081882476807, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:13.114883, step: 19, loss: 24.059898376464844, acc: 0.8850698471069336, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:13.482279, step: 20, loss: 22.02112579345703, acc: 0.8883737325668335, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:14.077707, step: 21, loss: 25.4965877532959, acc: 0.881737470626831, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:14.440185, step: 22, loss: 30.491287231445312, acc: 0.8337300419807434, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:14.700636, step: 23, loss: 22.620649337768555, acc: 0.8725970387458801, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:15.047792, step: 24, loss: 20.978391647338867, acc: 0.905537486076355, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:15.498135, step: 25, loss: 17.291240692138672, acc: 0.9240219593048096, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:16.096659, step: 26, loss: 20.832796096801758, acc: 0.9028244614601135, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:16.571932, step: 27, loss: 21.147785186767578, acc: 0.8976697325706482, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:16.901718, step: 28, loss: 21.71968650817871, acc: 0.8845897316932678, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:17.536494, step: 29, loss: 24.591110229492188, acc: 0.8819223642349243, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:17.990528, step: 30, loss: 25.15861701965332, acc: 0.8829431533813477, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:18.373643, step: 31, loss: 19.205530166625977, acc: 0.9075602293014526, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:18.801446, step: 32, loss: 24.392654418945312, acc: 0.8657740950584412, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:19.428373, step: 33, loss: 21.094135284423828, acc: 0.8939952850341797, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:19.807664, step: 34, loss: 18.136686325073242, acc: 0.9108044505119324, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:20.153511, step: 35, loss: 22.090621948242188, acc: 0.8759098649024963, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:20.488185, step: 36, loss: 17.302715301513672, acc: 0.893730878829956, f1: 0.0222, precision: 0.125, recall: 0.0122\n",
      "2019-01-24T10:11:20.999659, step: 37, loss: 21.52655029296875, acc: 0.8906961679458618, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:21.514585, step: 38, loss: 20.73882293701172, acc: 0.8820282816886902, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:21.895603, step: 39, loss: 26.92926025390625, acc: 0.8675308227539062, f1: 0.0134, precision: 0.1111, recall: 0.0071\n",
      "2019-01-24T10:11:22.334554, step: 40, loss: 24.290260314941406, acc: 0.8656240105628967, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:22.856857, step: 41, loss: 20.29671287536621, acc: 0.8856304883956909, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:23.196355, step: 42, loss: 20.480918884277344, acc: 0.8992927074432373, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:23.603211, step: 43, loss: 20.28704261779785, acc: 0.8903095722198486, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:23.956977, step: 44, loss: 21.614639282226562, acc: 0.8897535800933838, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:24.289789, step: 45, loss: 20.24443817138672, acc: 0.8962987661361694, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:24.661595, step: 46, loss: 20.331218719482422, acc: 0.8815099596977234, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:25.220873, step: 47, loss: 17.715728759765625, acc: 0.9166666865348816, f1: 0.022, precision: 0.1, recall: 0.0123\n",
      "2019-01-24T10:11:25.641483, step: 48, loss: 18.591583251953125, acc: 0.908594012260437, f1: 0.0198, precision: 0.0769, recall: 0.0114\n",
      "2019-01-24T10:11:26.259636, step: 49, loss: 20.77703857421875, acc: 0.8985507488250732, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:26.755460, step: 50, loss: 23.4212646484375, acc: 0.8550881147384644, f1: 0.0152, precision: 0.0769, recall: 0.0084\n",
      "2019-01-24T10:11:27.256719, step: 51, loss: 21.712648391723633, acc: 0.889140248298645, f1: 0.0317, precision: 0.1176, recall: 0.0183\n",
      "2019-01-24T10:11:27.708132, step: 52, loss: 18.079557418823242, acc: 0.8966971039772034, f1: 0.0192, precision: 0.0769, recall: 0.011\n",
      "2019-01-24T10:11:29.458718, step: 53, loss: 25.213607788085938, acc: 0.8905069828033447, f1: 0.0112, precision: 0.0526, recall: 0.0063\n",
      "2019-01-24T10:11:29.905274, step: 54, loss: 21.573143005371094, acc: 0.8876644968986511, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:30.271797, step: 55, loss: 16.248823165893555, acc: 0.9109123349189758, f1: 0.0198, precision: 0.0833, recall: 0.0112\n",
      "2019-01-24T10:11:30.672533, step: 56, loss: 17.280733108520508, acc: 0.885968029499054, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:31.326917, step: 57, loss: 14.6150484085083, acc: 0.9280622005462646, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:31.923244, step: 58, loss: 17.48659324645996, acc: 0.8962025046348572, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:32.453044, step: 59, loss: 20.969676971435547, acc: 0.8703510761260986, f1: 0.0274, precision: 0.0513, recall: 0.0187\n",
      "2019-01-24T10:11:32.890381, step: 60, loss: 16.533611297607422, acc: 0.8939902782440186, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:33.281256, step: 61, loss: 18.264904022216797, acc: 0.8731744885444641, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:33.813108, step: 62, loss: 21.79840850830078, acc: 0.8577194809913635, f1: 0.0107, precision: 0.0167, recall: 0.0079\n",
      "2019-01-24T10:11:35.366220, step: 63, loss: 24.316375732421875, acc: 0.872218906879425, f1: 0.0121, precision: 0.0323, recall: 0.0075\n",
      "2019-01-24T10:11:36.259070, step: 64, loss: 21.549091339111328, acc: 0.8901131749153137, f1: 0.0137, precision: 0.0233, recall: 0.0097\n",
      "2019-01-24T10:11:36.698079, step: 65, loss: 18.79323387145996, acc: 0.8836974501609802, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:37.369287, step: 66, loss: 18.141380310058594, acc: 0.8927038908004761, f1: 0.0145, precision: 0.0278, recall: 0.0098\n",
      "2019-01-24T10:11:37.802508, step: 67, loss: 16.352157592773438, acc: 0.9002476334571838, f1: 0.0441, precision: 0.0667, recall: 0.033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:11:38.560686, step: 68, loss: 20.347476959228516, acc: 0.8904503583908081, f1: 0.0255, precision: 0.05, recall: 0.0171\n",
      "2019-01-24T10:11:38.834244, step: 69, loss: 16.353452682495117, acc: 0.8919222354888916, f1: 0.0357, precision: 0.0833, recall: 0.0227\n",
      "2019-01-24T10:11:39.354665, step: 70, loss: 19.312713623046875, acc: 0.9001782536506653, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:39.740496, step: 71, loss: 22.19683074951172, acc: 0.870347797870636, f1: 0.0127, precision: 0.0227, recall: 0.0088\n",
      "2019-01-24T10:11:40.074765, step: 72, loss: 17.679569244384766, acc: 0.8913357257843018, f1: 0.0303, precision: 0.0588, recall: 0.0204\n",
      "2019-01-24T10:11:40.469641, step: 73, loss: 19.829833984375, acc: 0.8883216381072998, f1: 0.0146, precision: 0.0303, recall: 0.0096\n",
      "2019-01-24T10:11:40.795292, step: 74, loss: 19.325138092041016, acc: 0.8873653411865234, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:41.331859, step: 75, loss: 27.236433029174805, acc: 0.8499407172203064, f1: 0.0388, precision: 0.0889, recall: 0.0248\n",
      "2019-01-24T10:11:41.784190, step: 76, loss: 21.102256774902344, acc: 0.8788493871688843, f1: 0.015, precision: 0.0333, recall: 0.0097\n",
      "2019-01-24T10:11:42.197370, step: 77, loss: 16.35417366027832, acc: 0.9120762944221497, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:42.794141, step: 78, loss: 19.070749282836914, acc: 0.8879941701889038, f1: 0.0323, precision: 0.0833, recall: 0.02\n",
      "2019-01-24T10:11:44.415363, step: 79, loss: 42.94847869873047, acc: 0.7867836952209473, f1: 0.036, precision: 0.0455, recall: 0.0298\n",
      "2019-01-24T10:11:44.848516, step: 80, loss: 17.280868530273438, acc: 0.9139255881309509, f1: 0.0172, precision: 0.0294, recall: 0.0122\n",
      "2019-01-24T10:11:45.553844, step: 81, loss: 23.210338592529297, acc: 0.8593429327011108, f1: 0.0231, precision: 0.0588, recall: 0.0144\n",
      "2019-01-24T10:11:46.019056, step: 82, loss: 18.091020584106445, acc: 0.905990481376648, f1: 0.0145, precision: 0.0357, recall: 0.0091\n",
      "2019-01-24T10:11:46.324080, step: 83, loss: 19.60239601135254, acc: 0.8951563239097595, f1: 0.0209, precision: 0.0377, recall: 0.0145\n",
      "2019-01-24T10:11:46.738183, step: 84, loss: 17.300273895263672, acc: 0.9204866886138916, f1: 0.0345, precision: 0.069, recall: 0.023\n",
      "2019-01-24T10:11:47.247060, step: 85, loss: 21.067012786865234, acc: 0.8676190376281738, f1: 0.025, precision: 0.0465, recall: 0.0171\n",
      "2019-01-24T10:11:47.752184, step: 86, loss: 13.65633773803711, acc: 0.9160813093185425, f1: 0.0179, precision: 0.0333, recall: 0.0122\n",
      "2019-01-24T10:11:48.129172, step: 87, loss: 14.065774917602539, acc: 0.8981693387031555, f1: 0.0189, precision: 0.0385, recall: 0.0125\n",
      "2019-01-24T10:11:48.564545, step: 88, loss: 15.89515209197998, acc: 0.8996319770812988, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:48.853518, step: 89, loss: 15.720243453979492, acc: 0.904777467250824, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:49.448613, step: 90, loss: 15.751245498657227, acc: 0.8990400433540344, f1: 0, precision: 0, recall: 0\n",
      "2019-01-24T10:11:49.997716, step: 91, loss: 16.777523040771484, acc: 0.8929230570793152, f1: 0.0377, precision: 0.05, recall: 0.0303\n",
      "2019-01-24T10:11:50.426700, step: 92, loss: 19.43407440185547, acc: 0.8855673670768738, f1: 0.061, precision: 0.0962, recall: 0.0446\n",
      "2019-01-24T10:11:50.935981, step: 93, loss: 16.495567321777344, acc: 0.904338538646698, f1: 0.0168, precision: 0.0333, recall: 0.0112\n",
      "2019-01-24T10:11:52.671943, step: 94, loss: 19.669370651245117, acc: 0.8893722891807556, f1: 0.0619, precision: 0.1132, recall: 0.0426\n",
      "2019-01-24T10:11:53.074714, step: 95, loss: 15.36326789855957, acc: 0.905460774898529, f1: 0.0294, precision: 0.0476, recall: 0.0213\n",
      "2019-01-24T10:11:53.546462, step: 96, loss: 13.949508666992188, acc: 0.9177852272987366, f1: 0.0488, precision: 0.0882, recall: 0.0337\n",
      "2019-01-24T10:11:53.777510, step: 97, loss: 16.95151138305664, acc: 0.8682681322097778, f1: 0.0429, precision: 0.0566, recall: 0.0345\n",
      "2019-01-24T10:11:54.057659, step: 98, loss: 17.689285278320312, acc: 0.8767669200897217, f1: 0.0606, precision: 0.0759, recall: 0.0504\n",
      "2019-01-24T10:11:54.515255, step: 99, loss: 15.189173698425293, acc: 0.9040420651435852, f1: 0.0408, precision: 0.0517, recall: 0.0337\n",
      "2019-01-24T10:11:54.888944, step: 100, loss: 14.200223922729492, acc: 0.8963553309440613, f1: 0.1314, precision: 0.1731, recall: 0.1059\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:12:09.346875, step: 100, loss: 15.925696028603447, acc:0.9029251502619849, f1: 0.0957722222222222, precision: 0.13, recall: 0.07681111111111112\n",
      "\n",
      "\n",
      "2019-01-24T10:12:09.944576, step: 101, loss: 16.345096588134766, acc: 0.8987662196159363, f1: 0.0678, precision: 0.0769, recall: 0.0606\n",
      "2019-01-24T10:12:10.346970, step: 102, loss: 19.096704483032227, acc: 0.8737961053848267, f1: 0.0718, precision: 0.0972, recall: 0.0569\n",
      "2019-01-24T10:12:10.576670, step: 103, loss: 15.191790580749512, acc: 0.900688648223877, f1: 0.0548, precision: 0.0667, recall: 0.0465\n",
      "2019-01-24T10:12:10.877061, step: 104, loss: 16.488359451293945, acc: 0.9035256505012512, f1: 0.0411, precision: 0.0566, recall: 0.0323\n",
      "2019-01-24T10:12:11.220215, step: 105, loss: 18.662151336669922, acc: 0.880219042301178, f1: 0.0513, precision: 0.0704, recall: 0.0403\n",
      "2019-01-24T10:12:11.648538, step: 106, loss: 18.844501495361328, acc: 0.8933212161064148, f1: 0.0718, precision: 0.0854, recall: 0.0619\n",
      "2019-01-24T10:12:12.245482, step: 107, loss: 12.890783309936523, acc: 0.9153439402580261, f1: 0.1034, precision: 0.1429, recall: 0.0811\n",
      "2019-01-24T10:12:12.921039, step: 108, loss: 16.549213409423828, acc: 0.8944937586784363, f1: 0.0927, precision: 0.1296, recall: 0.0722\n",
      "2019-01-24T10:12:13.398369, step: 109, loss: 14.899612426757812, acc: 0.9119669795036316, f1: 0.155, precision: 0.2222, recall: 0.119\n",
      "2019-01-24T10:12:14.014436, step: 110, loss: 16.27520751953125, acc: 0.8995828628540039, f1: 0.0112, precision: 0.012, recall: 0.0104\n",
      "2019-01-24T10:12:14.457591, step: 111, loss: 13.826265335083008, acc: 0.8960875272750854, f1: 0.1194, precision: 0.1667, recall: 0.093\n",
      "2019-01-24T10:12:14.980684, step: 112, loss: 17.63323974609375, acc: 0.8880746364593506, f1: 0.0231, precision: 0.0312, recall: 0.0183\n",
      "2019-01-24T10:12:15.776977, step: 113, loss: 18.27178955078125, acc: 0.8869370818138123, f1: 0.1087, precision: 0.1695, recall: 0.08\n",
      "2019-01-24T10:12:16.219763, step: 114, loss: 13.776655197143555, acc: 0.9157266020774841, f1: 0.1192, precision: 0.1579, recall: 0.0957\n",
      "2019-01-24T10:12:16.555430, step: 115, loss: 17.48448371887207, acc: 0.8835851550102234, f1: 0.0989, precision: 0.1268, recall: 0.0811\n",
      "2019-01-24T10:12:17.054247, step: 116, loss: 14.4287691116333, acc: 0.9026581645011902, f1: 0.0833, precision: 0.1077, recall: 0.068\n",
      "2019-01-24T10:12:17.414053, step: 117, loss: 14.11684799194336, acc: 0.9177049398422241, f1: 0.1497, precision: 0.2157, recall: 0.1146\n",
      "2019-01-24T10:12:17.823263, step: 118, loss: 14.912878036499023, acc: 0.8982924818992615, f1: 0.0933, precision: 0.1186, recall: 0.0769\n",
      "2019-01-24T10:12:18.419611, step: 119, loss: 15.11776351928711, acc: 0.9036345481872559, f1: 0.1205, precision: 0.1471, recall: 0.102\n",
      "2019-01-24T10:12:19.322898, step: 120, loss: 13.403755187988281, acc: 0.9154165387153625, f1: 0.1045, precision: 0.1296, recall: 0.0875\n",
      "2019-01-24T10:12:19.591804, step: 121, loss: 15.001044273376465, acc: 0.9051473140716553, f1: 0.0423, precision: 0.0577, recall: 0.0333\n",
      "2019-01-24T10:12:20.028835, step: 122, loss: 15.108983993530273, acc: 0.88692307472229, f1: 0.0976, precision: 0.125, recall: 0.08\n",
      "2019-01-24T10:12:20.367578, step: 123, loss: 12.167741775512695, acc: 0.9135196208953857, f1: 0.0917, precision: 0.125, recall: 0.0725\n",
      "2019-01-24T10:12:20.829870, step: 124, loss: 15.59978199005127, acc: 0.895218014717102, f1: 0.0787, precision: 0.1029, recall: 0.0636\n",
      "2019-01-24T10:12:21.580108, step: 125, loss: 16.55815315246582, acc: 0.883013904094696, f1: 0.1319, precision: 0.1644, recall: 0.1101\n",
      "2019-01-24T10:12:23.378258, step: 126, loss: 18.190109252929688, acc: 0.899833083152771, f1: 0.0965, precision: 0.1236, recall: 0.0791\n",
      "2019-01-24T10:12:23.692263, step: 127, loss: 15.614275932312012, acc: 0.8916638493537903, f1: 0.08, precision: 0.1071, recall: 0.0638\n",
      "2019-01-24T10:12:23.994343, step: 128, loss: 13.477636337280273, acc: 0.8922772407531738, f1: 0.0927, precision: 0.1061, recall: 0.0824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:12:24.299726, step: 129, loss: 16.442401885986328, acc: 0.8755655884742737, f1: 0.1279, precision: 0.1443, recall: 0.1148\n",
      "2019-01-24T10:12:24.747681, step: 130, loss: 13.915472030639648, acc: 0.913913905620575, f1: 0.1637, precision: 0.1867, recall: 0.1458\n",
      "2019-01-24T10:12:25.340785, step: 131, loss: 15.023956298828125, acc: 0.9092387557029724, f1: 0.1333, precision: 0.1477, recall: 0.1215\n",
      "2019-01-24T10:12:25.633431, step: 132, loss: 13.909459114074707, acc: 0.9068493247032166, f1: 0.1828, precision: 0.2125, recall: 0.1604\n",
      "2019-01-24T10:12:26.214140, step: 133, loss: 17.706741333007812, acc: 0.9045284390449524, f1: 0.141, precision: 0.1553, recall: 0.129\n",
      "2019-01-24T10:12:26.679242, step: 134, loss: 17.092544555664062, acc: 0.8928241729736328, f1: 0.1475, precision: 0.1702, recall: 0.1301\n",
      "2019-01-24T10:12:27.033923, step: 135, loss: 17.265050888061523, acc: 0.8984974026679993, f1: 0.125, precision: 0.1548, recall: 0.1048\n",
      "2019-01-24T10:12:27.698008, step: 136, loss: 10.597932815551758, acc: 0.9315875768661499, f1: 0.1061, precision: 0.1129, recall: 0.1\n",
      "2019-01-24T10:12:28.151872, step: 137, loss: 14.261655807495117, acc: 0.9000701308250427, f1: 0.1641, precision: 0.172, recall: 0.1569\n",
      "2019-01-24T10:12:28.721407, step: 138, loss: 11.94605827331543, acc: 0.9179726839065552, f1: 0.1437, precision: 0.1558, recall: 0.1333\n",
      "2019-01-24T10:12:28.964687, step: 139, loss: 10.532527923583984, acc: 0.9260106682777405, f1: 0.1513, precision: 0.1731, recall: 0.1343\n",
      "2019-01-24T10:12:29.708424, step: 140, loss: 12.320934295654297, acc: 0.9209656119346619, f1: 0.1375, precision: 0.1341, recall: 0.141\n",
      "2019-01-24T10:12:30.212026, step: 141, loss: 18.30845069885254, acc: 0.8815279006958008, f1: 0.1034, precision: 0.1304, recall: 0.0857\n",
      "2019-01-24T10:12:30.695678, step: 142, loss: 13.495702743530273, acc: 0.917399525642395, f1: 0.1639, precision: 0.1724, recall: 0.1562\n",
      "2019-01-24T10:12:31.080383, step: 143, loss: 16.65045738220215, acc: 0.9007541537284851, f1: 0.1005, precision: 0.1087, recall: 0.0935\n",
      "2019-01-24T10:12:31.434654, step: 144, loss: 15.561777114868164, acc: 0.9080532789230347, f1: 0.1453, precision: 0.1711, recall: 0.1262\n",
      "2019-01-24T10:12:31.721722, step: 145, loss: 15.17422103881836, acc: 0.9076305031776428, f1: 0.22, precision: 0.2418, recall: 0.2018\n",
      "2019-01-24T10:12:32.101886, step: 146, loss: 18.028928756713867, acc: 0.8698557615280151, f1: 0.1818, precision: 0.2059, recall: 0.1628\n",
      "2019-01-24T10:12:32.677572, step: 147, loss: 14.389704704284668, acc: 0.9034190773963928, f1: 0.1326, precision: 0.1519, recall: 0.1176\n",
      "2019-01-24T10:12:33.060079, step: 148, loss: 13.20134162902832, acc: 0.9258304834365845, f1: 0.1562, precision: 0.1596, recall: 0.1531\n",
      "2019-01-24T10:12:33.537969, step: 149, loss: 12.742228507995605, acc: 0.9245165586471558, f1: 0.1444, precision: 0.1461, recall: 0.1429\n",
      "2019-01-24T10:12:33.874966, step: 150, loss: 11.991485595703125, acc: 0.903583288192749, f1: 0.1605, precision: 0.1806, recall: 0.1444\n",
      "2019-01-24T10:12:34.260220, step: 151, loss: 12.723917007446289, acc: 0.9138321876525879, f1: 0.1489, precision: 0.1489, recall: 0.1489\n",
      "2019-01-24T10:12:34.574630, step: 152, loss: 10.287888526916504, acc: 0.9187086820602417, f1: 0.1418, precision: 0.1613, recall: 0.1266\n",
      "2019-01-24T10:12:34.856838, step: 153, loss: 11.466754913330078, acc: 0.9162303805351257, f1: 0.1928, precision: 0.2388, recall: 0.1616\n",
      "2019-01-24T10:12:35.426509, step: 154, loss: 11.85551643371582, acc: 0.9239784479141235, f1: 0.2011, precision: 0.24, recall: 0.1731\n",
      "2019-01-24T10:12:35.703608, step: 155, loss: 13.473871231079102, acc: 0.9107204675674438, f1: 0.1648, precision: 0.1807, recall: 0.1515\n",
      "2019-01-24T10:12:36.066622, step: 156, loss: 11.65502643585205, acc: 0.9232258200645447, f1: 0.1566, precision: 0.1912, recall: 0.1327\n",
      "2019-01-24T10:12:36.447540, step: 157, loss: 15.513903617858887, acc: 0.8850383162498474, f1: 0.1429, precision: 0.1505, recall: 0.1359\n",
      "2019-01-24T10:12:36.847837, step: 158, loss: 12.262554168701172, acc: 0.9264978170394897, f1: 0.1258, precision: 0.1408, recall: 0.1136\n",
      "2019-01-24T10:12:37.280912, step: 159, loss: 16.3565616607666, acc: 0.8940503001213074, f1: 0.1783, precision: 0.1783, recall: 0.1783\n",
      "2019-01-24T10:12:37.876281, step: 160, loss: 12.050146102905273, acc: 0.9232736825942993, f1: 0.0946, precision: 0.1077, recall: 0.0843\n",
      "2019-01-24T10:12:38.169707, step: 161, loss: 11.614646911621094, acc: 0.9151009321212769, f1: 0.0645, precision: 0.0667, recall: 0.0625\n",
      "2019-01-24T10:12:38.744589, step: 162, loss: 9.747421264648438, acc: 0.9304908514022827, f1: 0.1639, precision: 0.1961, recall: 0.1408\n",
      "2019-01-24T10:12:39.266567, step: 163, loss: 12.315484046936035, acc: 0.9166138172149658, f1: 0.178, precision: 0.2024, recall: 0.1589\n",
      "2019-01-24T10:12:39.752210, step: 164, loss: 16.410064697265625, acc: 0.8978394865989685, f1: 0.2385, precision: 0.2653, recall: 0.2167\n",
      "2019-01-24T10:12:40.310639, step: 165, loss: 12.930758476257324, acc: 0.8969719409942627, f1: 0.15, precision: 0.1935, recall: 0.1224\n",
      "2019-01-24T10:12:40.862195, step: 166, loss: 14.598941802978516, acc: 0.89232337474823, f1: 0.0905, precision: 0.0926, recall: 0.0885\n",
      "2019-01-24T10:12:41.239126, step: 167, loss: 13.660236358642578, acc: 0.902402937412262, f1: 0.1342, precision: 0.1471, recall: 0.1235\n",
      "2019-01-24T10:12:41.657772, step: 168, loss: 15.636821746826172, acc: 0.8934633731842041, f1: 0.1429, precision: 0.1573, recall: 0.1308\n",
      "2019-01-24T10:12:42.157796, step: 169, loss: 9.775131225585938, acc: 0.932555615901947, f1: 0.24, precision: 0.2571, recall: 0.225\n",
      "2019-01-24T10:12:42.508889, step: 170, loss: 14.558177947998047, acc: 0.8995956778526306, f1: 0.12, precision: 0.1304, recall: 0.1111\n",
      "2019-01-24T10:12:43.234960, step: 171, loss: 10.641361236572266, acc: 0.9296765327453613, f1: 0.1154, precision: 0.1216, recall: 0.1098\n",
      "2019-01-24T10:12:43.473058, step: 172, loss: 12.020074844360352, acc: 0.9242372512817383, f1: 0.25, precision: 0.2817, recall: 0.2247\n",
      "2019-01-24T10:12:43.836101, step: 173, loss: 9.702014923095703, acc: 0.9346572756767273, f1: 0.1635, precision: 0.1646, recall: 0.1625\n",
      "2019-01-24T10:12:44.217408, step: 174, loss: 12.597370147705078, acc: 0.9135925769805908, f1: 0.1463, precision: 0.1714, recall: 0.1277\n",
      "2019-01-24T10:12:45.505674, step: 175, loss: 9.077762603759766, acc: 0.9559204578399658, f1: 0.2581, precision: 0.2564, recall: 0.2597\n",
      "2019-01-24T10:12:45.862432, step: 176, loss: 12.098949432373047, acc: 0.9239926934242249, f1: 0.21, precision: 0.2019, recall: 0.2188\n",
      "2019-01-24T10:12:46.221914, step: 177, loss: 10.509005546569824, acc: 0.9181150794029236, f1: 0.1239, precision: 0.1556, recall: 0.1029\n",
      "2019-01-24T10:12:46.510727, step: 178, loss: 11.273185729980469, acc: 0.9111030697822571, f1: 0.2038, precision: 0.2254, recall: 0.186\n",
      "2019-01-24T10:12:46.883260, step: 179, loss: 12.73806095123291, acc: 0.9106107354164124, f1: 0.2252, precision: 0.2381, recall: 0.2137\n",
      "2019-01-24T10:12:47.206905, step: 180, loss: 14.464407920837402, acc: 0.8944162726402283, f1: 0.1792, precision: 0.1863, recall: 0.1727\n",
      "2019-01-24T10:12:47.785189, step: 181, loss: 10.414419174194336, acc: 0.922995388507843, f1: 0.15, precision: 0.1558, recall: 0.1446\n",
      "2019-01-24T10:12:48.174947, step: 182, loss: 16.732498168945312, acc: 0.8952702879905701, f1: 0.1473, precision: 0.157, recall: 0.1387\n",
      "2019-01-24T10:12:48.455333, step: 183, loss: 9.377321243286133, acc: 0.9276389479637146, f1: 0.1358, precision: 0.1358, recall: 0.1358\n",
      "2019-01-24T10:12:48.784435, step: 184, loss: 9.395851135253906, acc: 0.9364140629768372, f1: 0.2603, precision: 0.2969, recall: 0.2317\n",
      "2019-01-24T10:12:49.264418, step: 185, loss: 11.636907577514648, acc: 0.9161656498908997, f1: 0.2165, precision: 0.2336, recall: 0.2016\n",
      "2019-01-24T10:12:50.038554, step: 186, loss: 13.202317237854004, acc: 0.928528368473053, f1: 0.2514, precision: 0.2857, recall: 0.2245\n",
      "2019-01-24T10:12:50.311144, step: 187, loss: 11.439519882202148, acc: 0.9164912104606628, f1: 0.1957, precision: 0.2022, recall: 0.1895\n",
      "2019-01-24T10:12:50.707848, step: 188, loss: 12.892772674560547, acc: 0.917560338973999, f1: 0.1818, precision: 0.1889, recall: 0.1753\n",
      "2019-01-24T10:12:52.441220, step: 189, loss: 17.97905921936035, acc: 0.8962817788124084, f1: 0.2164, precision: 0.2231, recall: 0.2101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:12:52.926563, step: 190, loss: 9.346099853515625, acc: 0.9285714030265808, f1: 0.2333, precision: 0.2234, recall: 0.2442\n",
      "2019-01-24T10:12:53.470445, step: 191, loss: 9.912862777709961, acc: 0.9354838728904724, f1: 0.2532, precision: 0.2817, recall: 0.2299\n",
      "2019-01-24T10:12:53.770770, step: 192, loss: 9.200638771057129, acc: 0.936796247959137, f1: 0.2375, precision: 0.2436, recall: 0.2317\n",
      "2019-01-24T10:12:54.091702, step: 193, loss: 13.177605628967285, acc: 0.9035874605178833, f1: 0.1925, precision: 0.1901, recall: 0.1949\n",
      "2019-01-24T10:12:54.531587, step: 194, loss: 11.692618370056152, acc: 0.9120956659317017, f1: 0.2515, precision: 0.2763, recall: 0.2308\n",
      "2019-01-24T10:12:54.907279, step: 195, loss: 11.049695014953613, acc: 0.9242724776268005, f1: 0.2353, precision: 0.2623, recall: 0.2133\n",
      "2019-01-24T10:12:55.163429, step: 196, loss: 8.704599380493164, acc: 0.9427279829978943, f1: 0.2794, precision: 0.2923, recall: 0.2676\n",
      "2019-01-24T10:12:55.712120, step: 197, loss: 13.252699851989746, acc: 0.9128329157829285, f1: 0.1875, precision: 0.1944, recall: 0.181\n",
      "2019-01-24T10:12:56.282361, step: 198, loss: 10.823041915893555, acc: 0.9285477995872498, f1: 0.3048, precision: 0.3107, recall: 0.2991\n",
      "2019-01-24T10:12:56.648009, step: 199, loss: 9.747396469116211, acc: 0.9333561062812805, f1: 0.2289, precision: 0.2235, recall: 0.2346\n",
      "2019-01-24T10:12:57.001896, step: 200, loss: 8.28457260131836, acc: 0.9333071708679199, f1: 0.3399, precision: 0.3768, recall: 0.3095\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:13:22.405030, step: 200, loss: 11.18391083346473, acc:0.9234430425696902, f1: 0.28964999999999996, precision: 0.3433638888888889, recall: 0.25184166666666674\n",
      "\n",
      "\n",
      "2019-01-24T10:13:23.177151, step: 201, loss: 14.185384750366211, acc: 0.9092298150062561, f1: 0.233, precision: 0.2857, recall: 0.1967\n",
      "2019-01-24T10:13:24.142797, step: 202, loss: 10.184757232666016, acc: 0.9221991896629333, f1: 0.2567, precision: 0.2474, recall: 0.2667\n",
      "2019-01-24T10:13:25.310262, step: 203, loss: 14.43480110168457, acc: 0.904856264591217, f1: 0.3085, precision: 0.3494, recall: 0.2762\n",
      "2019-01-24T10:13:25.973909, step: 204, loss: 10.052387237548828, acc: 0.937279999256134, f1: 0.2014, precision: 0.2258, recall: 0.1818\n",
      "2019-01-24T10:13:26.747325, step: 205, loss: 10.741399765014648, acc: 0.9156496524810791, f1: 0.2449, precision: 0.25, recall: 0.24\n",
      "2019-01-24T10:13:27.271161, step: 206, loss: 9.55372428894043, acc: 0.9344436526298523, f1: 0.3333, precision: 0.31, recall: 0.3605\n",
      "2019-01-24T10:13:27.821179, step: 207, loss: 12.992547988891602, acc: 0.9031105041503906, f1: 0.2419, precision: 0.2574, recall: 0.2281\n",
      "2019-01-24T10:13:30.340940, step: 208, loss: 13.470706939697266, acc: 0.9241499304771423, f1: 0.3249, precision: 0.3285, recall: 0.3214\n",
      "2019-01-24T10:13:30.868984, step: 209, loss: 9.314830780029297, acc: 0.9298998713493347, f1: 0.2308, precision: 0.2169, recall: 0.2466\n",
      "2019-01-24T10:13:31.692122, step: 210, loss: 12.721198081970215, acc: 0.9052323698997498, f1: 0.2022, precision: 0.2278, recall: 0.1818\n",
      "2019-01-24T10:13:32.464333, step: 211, loss: 9.088828086853027, acc: 0.9390363693237305, f1: 0.3438, precision: 0.3667, recall: 0.3235\n",
      "2019-01-24T10:13:33.439826, step: 212, loss: 9.637506484985352, acc: 0.9306666851043701, f1: 0.1977, precision: 0.2099, recall: 0.1868\n",
      "2019-01-24T10:13:34.277520, step: 213, loss: 11.398605346679688, acc: 0.92954021692276, f1: 0.2452, precision: 0.2714, recall: 0.2235\n",
      "2019-01-24T10:13:35.179229, step: 214, loss: 10.768264770507812, acc: 0.9218375086784363, f1: 0.2179, precision: 0.2576, recall: 0.1889\n",
      "2019-01-24T10:13:35.734754, step: 215, loss: 13.865055084228516, acc: 0.9106035828590393, f1: 0.2557, precision: 0.2828, recall: 0.2333\n",
      "2019-01-24T10:13:36.387044, step: 216, loss: 12.604036331176758, acc: 0.9212836623191833, f1: 0.2095, precision: 0.2366, recall: 0.188\n",
      "2019-01-24T10:13:37.084458, step: 217, loss: 11.56756591796875, acc: 0.9076187014579773, f1: 0.2514, precision: 0.2911, recall: 0.2212\n",
      "2019-01-24T10:13:37.725299, step: 218, loss: 10.202672004699707, acc: 0.9223477840423584, f1: 0.2796, precision: 0.2826, recall: 0.2766\n",
      "2019-01-24T10:13:38.693912, step: 219, loss: 16.072425842285156, acc: 0.9112110137939453, f1: 0.2259, precision: 0.2389, recall: 0.2143\n",
      "2019-01-24T10:13:39.112636, step: 220, loss: 10.118050575256348, acc: 0.9316596984863281, f1: 0.3756, precision: 0.3814, recall: 0.37\n",
      "2019-01-24T10:13:39.922261, step: 221, loss: 9.208806991577148, acc: 0.9450171589851379, f1: 0.2771, precision: 0.2738, recall: 0.2805\n",
      "2019-01-24T10:13:40.828353, step: 222, loss: 7.229586124420166, acc: 0.9411534070968628, f1: 0.2154, precision: 0.2188, recall: 0.2121\n",
      "2019-01-24T10:13:41.637613, step: 223, loss: 12.556597709655762, acc: 0.9246054887771606, f1: 0.2557, precision: 0.2373, recall: 0.2772\n",
      "2019-01-24T10:13:42.297983, step: 224, loss: 11.857824325561523, acc: 0.9146950840950012, f1: 0.2143, precision: 0.2069, recall: 0.2222\n",
      "2019-01-24T10:13:42.829088, step: 225, loss: 10.368490219116211, acc: 0.9299003481864929, f1: 0.2679, precision: 0.2727, recall: 0.2632\n",
      "2019-01-24T10:13:43.329280, step: 226, loss: 7.743525981903076, acc: 0.9446380138397217, f1: 0.3743, precision: 0.3721, recall: 0.3765\n",
      "2019-01-24T10:13:44.374387, step: 227, loss: 11.57691764831543, acc: 0.9221741557121277, f1: 0.1915, precision: 0.2195, recall: 0.1698\n",
      "2019-01-24T10:13:45.090543, step: 228, loss: 12.15218734741211, acc: 0.904325008392334, f1: 0.2259, precision: 0.2328, recall: 0.2195\n",
      "2019-01-24T10:13:45.802780, step: 229, loss: 13.783519744873047, acc: 0.887794554233551, f1: 0.2024, precision: 0.2119, recall: 0.1938\n",
      "2019-01-24T10:13:46.930661, step: 230, loss: 11.564172744750977, acc: 0.9251048564910889, f1: 0.2473, precision: 0.2771, recall: 0.2233\n",
      "2019-01-24T10:13:48.329027, step: 231, loss: 8.676158905029297, acc: 0.9440828561782837, f1: 0.3459, precision: 0.3516, recall: 0.3404\n",
      "2019-01-24T10:13:50.966018, step: 232, loss: 10.561759948730469, acc: 0.9358128309249878, f1: 0.3608, precision: 0.3566, recall: 0.3651\n",
      "2019-01-24T10:13:51.476740, step: 233, loss: 8.260279655456543, acc: 0.9417025446891785, f1: 0.3117, precision: 0.3582, recall: 0.2759\n",
      "2019-01-24T10:13:52.016860, step: 234, loss: 9.640183448791504, acc: 0.9297415614128113, f1: 0.3007, precision: 0.3108, recall: 0.2911\n",
      "2019-01-24T10:13:52.911899, step: 235, loss: 9.5074462890625, acc: 0.931292712688446, f1: 0.2375, precision: 0.2405, recall: 0.2346\n",
      "2019-01-24T10:13:53.804343, step: 236, loss: 12.595341682434082, acc: 0.9268882274627686, f1: 0.2609, precision: 0.2872, recall: 0.2389\n",
      "2019-01-24T10:13:54.483542, step: 237, loss: 11.231817245483398, acc: 0.9210339784622192, f1: 0.31, precision: 0.3163, recall: 0.3039\n",
      "2019-01-24T10:13:55.003075, step: 238, loss: 13.450119018554688, acc: 0.8957312703132629, f1: 0.2727, precision: 0.26, recall: 0.2868\n",
      "2019-01-24T10:13:56.188778, step: 239, loss: 14.764432907104492, acc: 0.9130695462226868, f1: 0.2927, precision: 0.2791, recall: 0.3077\n",
      "2019-01-24T10:13:56.871402, step: 240, loss: 11.300167083740234, acc: 0.9198369383811951, f1: 0.203, precision: 0.2128, recall: 0.1942\n",
      "2019-01-24T10:13:57.682548, step: 241, loss: 13.339361190795898, acc: 0.9022409915924072, f1: 0.3366, precision: 0.3248, recall: 0.3493\n",
      "2019-01-24T10:13:58.135353, step: 242, loss: 9.701165199279785, acc: 0.9240366816520691, f1: 0.24, precision: 0.233, recall: 0.2474\n",
      "2019-01-24T10:13:58.952280, step: 243, loss: 9.0006742477417, acc: 0.9448022246360779, f1: 0.337, precision: 0.3563, recall: 0.3196\n",
      "2019-01-24T10:14:00.602731, step: 244, loss: 9.417835235595703, acc: 0.9444273114204407, f1: 0.2981, precision: 0.2824, recall: 0.3158\n",
      "2019-01-24T10:14:01.583860, step: 245, loss: 10.943046569824219, acc: 0.9327561259269714, f1: 0.2963, precision: 0.2963, recall: 0.2963\n",
      "2019-01-24T10:14:02.097970, step: 246, loss: 11.772308349609375, acc: 0.9057971239089966, f1: 0.2636, precision: 0.2544, recall: 0.2736\n",
      "2019-01-24T10:14:02.943149, step: 247, loss: 13.078877449035645, acc: 0.90439772605896, f1: 0.3042, precision: 0.3175, recall: 0.292\n",
      "2019-01-24T10:14:03.640949, step: 248, loss: 9.001056671142578, acc: 0.9465208053588867, f1: 0.284, precision: 0.2963, recall: 0.2727\n",
      "2019-01-24T10:14:06.167748, step: 249, loss: 9.77517032623291, acc: 0.9416342377662659, f1: 0.4229, precision: 0.4528, recall: 0.3967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:14:06.708686, step: 250, loss: 8.756786346435547, acc: 0.942756175994873, f1: 0.3611, precision: 0.4333, recall: 0.3095\n",
      "2019-01-24T10:14:07.420690, step: 251, loss: 8.113692283630371, acc: 0.9343236684799194, f1: 0.2857, precision: 0.2874, recall: 0.2841\n",
      "2019-01-24T10:14:08.225691, step: 252, loss: 10.389699935913086, acc: 0.932695209980011, f1: 0.2826, precision: 0.2989, recall: 0.268\n",
      "2019-01-24T10:14:09.040766, step: 253, loss: 9.365396499633789, acc: 0.9357551336288452, f1: 0.3593, precision: 0.3704, recall: 0.3488\n",
      "2019-01-24T10:14:09.664371, step: 254, loss: 10.805408477783203, acc: 0.93019038438797, f1: 0.274, precision: 0.2804, recall: 0.2679\n",
      "2019-01-24T10:14:10.250714, step: 255, loss: 8.68299388885498, acc: 0.9375629425048828, f1: 0.4356, precision: 0.419, recall: 0.4536\n",
      "2019-01-24T10:14:10.733681, step: 256, loss: 7.572927474975586, acc: 0.9507065415382385, f1: 0.3478, precision: 0.3111, recall: 0.3944\n",
      "2019-01-24T10:14:11.614458, step: 257, loss: 9.245231628417969, acc: 0.9372947216033936, f1: 0.2541, precision: 0.23, recall: 0.284\n",
      "2019-01-24T10:14:12.383552, step: 258, loss: 11.25369644165039, acc: 0.9141675233840942, f1: 0.3077, precision: 0.3146, recall: 0.3011\n",
      "2019-01-24T10:14:13.255027, step: 259, loss: 10.771732330322266, acc: 0.9217207431793213, f1: 0.2805, precision: 0.2696, recall: 0.2925\n",
      "2019-01-24T10:14:13.832801, step: 260, loss: 7.524553298950195, acc: 0.9355358481407166, f1: 0.3239, precision: 0.3651, recall: 0.2911\n",
      "2019-01-24T10:14:14.561498, step: 261, loss: 11.925552368164062, acc: 0.9122748374938965, f1: 0.2946, precision: 0.287, recall: 0.3028\n",
      "2019-01-24T10:14:15.049123, step: 262, loss: 10.40802001953125, acc: 0.9249759316444397, f1: 0.3248, precision: 0.314, recall: 0.3363\n",
      "2019-01-24T10:14:15.969278, step: 263, loss: 12.174814224243164, acc: 0.9220141172409058, f1: 0.31, precision: 0.2838, recall: 0.3415\n",
      "2019-01-24T10:14:16.412078, step: 264, loss: 7.5953216552734375, acc: 0.9361010789871216, f1: 0.331, precision: 0.3117, recall: 0.3529\n",
      "2019-01-24T10:14:17.482739, step: 265, loss: 9.66638469696045, acc: 0.9306378364562988, f1: 0.3815, precision: 0.4125, recall: 0.3548\n",
      "2019-01-24T10:14:20.073768, step: 266, loss: 10.692520141601562, acc: 0.9387635588645935, f1: 0.4, precision: 0.4118, recall: 0.3889\n",
      "2019-01-24T10:14:20.880186, step: 267, loss: 10.186664581298828, acc: 0.929446816444397, f1: 0.3617, precision: 0.382, recall: 0.3434\n",
      "2019-01-24T10:14:21.429592, step: 268, loss: 8.253028869628906, acc: 0.9379188418388367, f1: 0.3407, precision: 0.3966, recall: 0.2987\n",
      "2019-01-24T10:14:22.196151, step: 269, loss: 10.028759002685547, acc: 0.9190731048583984, f1: 0.2647, precision: 0.2727, recall: 0.2571\n",
      "2019-01-24T10:14:23.460187, step: 270, loss: 9.227015495300293, acc: 0.9353266358375549, f1: 0.3298, precision: 0.3523, recall: 0.31\n",
      "2019-01-24T10:14:23.974592, step: 271, loss: 8.76024055480957, acc: 0.9261922836303711, f1: 0.264, precision: 0.2453, recall: 0.2857\n",
      "2019-01-24T10:14:24.709861, step: 272, loss: 9.989416122436523, acc: 0.9310013055801392, f1: 0.2788, precision: 0.2788, recall: 0.2788\n",
      "2019-01-24T10:14:25.296770, step: 273, loss: 9.151927947998047, acc: 0.9290979504585266, f1: 0.3152, precision: 0.3118, recall: 0.3187\n",
      "2019-01-24T10:14:25.837143, step: 274, loss: 8.670061111450195, acc: 0.9380328059196472, f1: 0.3556, precision: 0.3441, recall: 0.3678\n",
      "2019-01-24T10:14:26.376198, step: 275, loss: 9.103143692016602, acc: 0.9389124512672424, f1: 0.3864, precision: 0.4146, recall: 0.3617\n",
      "2019-01-24T10:14:27.344227, step: 276, loss: 8.74876594543457, acc: 0.9331774115562439, f1: 0.2541, precision: 0.2527, recall: 0.2556\n",
      "2019-01-24T10:14:27.952222, step: 277, loss: 5.461888313293457, acc: 0.9664804339408875, f1: 0.35, precision: 0.375, recall: 0.3281\n",
      "2019-01-24T10:14:28.509620, step: 278, loss: 10.577861785888672, acc: 0.9309056997299194, f1: 0.2881, precision: 0.281, recall: 0.2957\n",
      "2019-01-24T10:14:29.454935, step: 279, loss: 8.623287200927734, acc: 0.9379817843437195, f1: 0.3372, precision: 0.3494, recall: 0.3258\n",
      "2019-01-24T10:14:30.261406, step: 280, loss: 7.095888137817383, acc: 0.947520911693573, f1: 0.3636, precision: 0.3896, recall: 0.3409\n",
      "2019-01-24T10:14:32.026982, step: 281, loss: 8.315671920776367, acc: 0.9375817179679871, f1: 0.3737, precision: 0.3737, recall: 0.3737\n",
      "2019-01-24T10:14:32.640047, step: 282, loss: 8.761663436889648, acc: 0.9214780330657959, f1: 0.1734, precision: 0.1667, recall: 0.1807\n",
      "2019-01-24T10:14:33.282438, step: 283, loss: 7.851836204528809, acc: 0.9382550120353699, f1: 0.255, precision: 0.2533, recall: 0.2568\n",
      "2019-01-24T10:14:33.975916, step: 284, loss: 10.999476432800293, acc: 0.9258896708488464, f1: 0.2843, precision: 0.3021, recall: 0.2685\n",
      "2019-01-24T10:14:34.554938, step: 285, loss: 8.433069229125977, acc: 0.9364569783210754, f1: 0.4135, precision: 0.4216, recall: 0.4057\n",
      "2019-01-24T10:14:35.130173, step: 286, loss: 10.725883483886719, acc: 0.9359803199768066, f1: 0.4156, precision: 0.4486, recall: 0.3871\n",
      "2019-01-24T10:14:35.739098, step: 287, loss: 10.482362747192383, acc: 0.9324693083763123, f1: 0.3137, precision: 0.3168, recall: 0.3107\n",
      "2019-01-24T10:14:36.521524, step: 288, loss: 7.1123199462890625, acc: 0.9532263278961182, f1: 0.4225, precision: 0.4286, recall: 0.4167\n",
      "2019-01-24T10:14:37.082159, step: 289, loss: 6.260782241821289, acc: 0.9576159119606018, f1: 0.3765, precision: 0.3556, recall: 0.4\n",
      "2019-01-24T10:14:37.644562, step: 290, loss: 9.040037155151367, acc: 0.9445676207542419, f1: 0.4457, precision: 0.4556, recall: 0.4362\n",
      "2019-01-24T10:14:38.195145, step: 291, loss: 9.264524459838867, acc: 0.9363895058631897, f1: 0.3647, precision: 0.3827, recall: 0.3483\n",
      "2019-01-24T10:14:39.017562, step: 292, loss: 8.079272270202637, acc: 0.9504317045211792, f1: 0.4497, precision: 0.4578, recall: 0.4419\n",
      "2019-01-24T10:14:41.892382, step: 293, loss: 8.462724685668945, acc: 0.942148745059967, f1: 0.4902, precision: 0.5376, recall: 0.4505\n",
      "2019-01-24T10:14:42.563171, step: 294, loss: 8.45937442779541, acc: 0.9427178502082825, f1: 0.3474, precision: 0.3587, recall: 0.3367\n",
      "2019-01-24T10:14:43.426943, step: 295, loss: 7.551760196685791, acc: 0.9480865001678467, f1: 0.4472, precision: 0.48, recall: 0.4186\n",
      "2019-01-24T10:14:44.040028, step: 296, loss: 6.870012283325195, acc: 0.9491586089134216, f1: 0.3648, precision: 0.3412, recall: 0.3919\n",
      "2019-01-24T10:14:44.615304, step: 297, loss: 12.090035438537598, acc: 0.9112405776977539, f1: 0.2871, precision: 0.2959, recall: 0.2788\n",
      "2019-01-24T10:14:45.177423, step: 298, loss: 8.213871955871582, acc: 0.9419928789138794, f1: 0.4167, precision: 0.4348, recall: 0.4\n",
      "2019-01-24T10:14:45.964910, step: 299, loss: 8.37644100189209, acc: 0.942314088344574, f1: 0.3216, precision: 0.3107, recall: 0.3333\n",
      "2019-01-24T10:14:48.580354, step: 300, loss: 9.609440803527832, acc: 0.9322034120559692, f1: 0.4364, precision: 0.4225, recall: 0.4511\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:15:14.553840, step: 300, loss: 8.28559562895033, acc:0.9422512137227588, f1: 0.4407499999999999, precision: 0.46056388888888883, recall: 0.42365833333333325\n",
      "\n",
      "\n",
      "2019-01-24T10:15:15.516173, step: 301, loss: 6.7181396484375, acc: 0.9402273297309875, f1: 0.3864, precision: 0.382, recall: 0.3908\n",
      "2019-01-24T10:15:16.034266, step: 302, loss: 9.07705307006836, acc: 0.939676821231842, f1: 0.3662, precision: 0.3679, recall: 0.3645\n",
      "2019-01-24T10:15:16.986575, step: 303, loss: 10.496625900268555, acc: 0.916167676448822, f1: 0.346, precision: 0.3228, recall: 0.3727\n",
      "2019-01-24T10:15:17.550099, step: 304, loss: 8.138543128967285, acc: 0.9338380694389343, f1: 0.425, precision: 0.4048, recall: 0.4474\n",
      "2019-01-24T10:15:18.067898, step: 305, loss: 7.357855319976807, acc: 0.9478315711021423, f1: 0.4951, precision: 0.5312, recall: 0.4636\n",
      "2019-01-24T10:15:18.727244, step: 306, loss: 5.882519721984863, acc: 0.9544639587402344, f1: 0.3053, precision: 0.3226, recall: 0.2899\n",
      "2019-01-24T10:15:19.562306, step: 307, loss: 6.413277626037598, acc: 0.9501991868019104, f1: 0.3077, precision: 0.2973, recall: 0.3188\n",
      "2019-01-24T10:15:20.136168, step: 308, loss: 6.760537147521973, acc: 0.9485443234443665, f1: 0.4224, precision: 0.4146, recall: 0.4304\n",
      "2019-01-24T10:15:20.786761, step: 309, loss: 11.826895713806152, acc: 0.9228423237800598, f1: 0.2844, precision: 0.3163, recall: 0.2583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:15:21.358966, step: 310, loss: 9.083611488342285, acc: 0.9264169931411743, f1: 0.3474, precision: 0.3814, recall: 0.319\n",
      "2019-01-24T10:15:22.213070, step: 311, loss: 8.369490623474121, acc: 0.9375792145729065, f1: 0.33, precision: 0.3333, recall: 0.3267\n",
      "2019-01-24T10:15:22.588447, step: 312, loss: 9.454914093017578, acc: 0.9238778948783875, f1: 0.313, precision: 0.2951, recall: 0.3333\n",
      "2019-01-24T10:15:23.603049, step: 313, loss: 8.403366088867188, acc: 0.9430894255638123, f1: 0.3171, precision: 0.3095, recall: 0.325\n",
      "2019-01-24T10:15:24.273683, step: 314, loss: 8.810419082641602, acc: 0.9401238560676575, f1: 0.4516, precision: 0.4375, recall: 0.4667\n",
      "2019-01-24T10:15:24.781887, step: 315, loss: 7.411932468414307, acc: 0.9496833086013794, f1: 0.3867, precision: 0.3571, recall: 0.4217\n",
      "2019-01-24T10:15:25.493360, step: 316, loss: 8.961029052734375, acc: 0.9417831301689148, f1: 0.4576, precision: 0.45, recall: 0.4655\n",
      "2019-01-24T10:15:26.348605, step: 317, loss: 8.48796272277832, acc: 0.9392474889755249, f1: 0.4817, precision: 0.4842, recall: 0.4792\n",
      "2019-01-24T10:15:26.926880, step: 318, loss: 8.167061805725098, acc: 0.9439338445663452, f1: 0.3473, precision: 0.3412, recall: 0.3537\n",
      "2019-01-24T10:15:27.796577, step: 319, loss: 8.885954856872559, acc: 0.9397435784339905, f1: 0.3464, precision: 0.3407, recall: 0.3523\n",
      "2019-01-24T10:15:28.244365, step: 320, loss: 7.434985160827637, acc: 0.9465430974960327, f1: 0.4, precision: 0.3951, recall: 0.4051\n",
      "2019-01-24T10:15:29.029105, step: 321, loss: 7.960492134094238, acc: 0.9336801171302795, f1: 0.3286, precision: 0.3302, recall: 0.3271\n",
      "2019-01-24T10:15:29.727370, step: 322, loss: 8.576220512390137, acc: 0.9337176084518433, f1: 0.4279, precision: 0.4526, recall: 0.4057\n",
      "2019-01-24T10:15:30.418461, step: 323, loss: 6.651402950286865, acc: 0.950236976146698, f1: 0.3972, precision: 0.459, recall: 0.35\n",
      "2019-01-24T10:15:30.938468, step: 324, loss: 6.554506301879883, acc: 0.9470677971839905, f1: 0.3185, precision: 0.3333, recall: 0.3049\n",
      "2019-01-24T10:15:32.024676, step: 325, loss: 8.564630508422852, acc: 0.9306503534317017, f1: 0.3953, precision: 0.4304, recall: 0.3656\n",
      "2019-01-24T10:15:32.866798, step: 326, loss: 10.908702850341797, acc: 0.9381608366966248, f1: 0.369, precision: 0.4247, recall: 0.3263\n",
      "start training model\n",
      "2019-01-24T10:15:33.915289, step: 327, loss: 7.17055606842041, acc: 0.9516587853431702, f1: 0.3657, precision: 0.3902, recall: 0.3441\n",
      "2019-01-24T10:15:34.750959, step: 328, loss: 7.9514055252075195, acc: 0.9459291696548462, f1: 0.4264, precision: 0.4158, recall: 0.4375\n",
      "2019-01-24T10:15:35.553881, step: 329, loss: 6.8138275146484375, acc: 0.9564393758773804, f1: 0.4417, precision: 0.4286, recall: 0.4557\n",
      "2019-01-24T10:15:36.095317, step: 330, loss: 9.832658767700195, acc: 0.9368453621864319, f1: 0.4436, precision: 0.4403, recall: 0.447\n",
      "2019-01-24T10:15:36.820000, step: 331, loss: 9.780722618103027, acc: 0.9341335892677307, f1: 0.375, precision: 0.3629, recall: 0.3879\n",
      "2019-01-24T10:15:37.479126, step: 332, loss: 9.482324600219727, acc: 0.9251928925514221, f1: 0.375, precision: 0.3462, recall: 0.4091\n",
      "2019-01-24T10:15:37.983152, step: 333, loss: 8.821754455566406, acc: 0.9238410592079163, f1: 0.481, precision: 0.4831, recall: 0.479\n",
      "2019-01-24T10:15:38.571255, step: 334, loss: 8.593148231506348, acc: 0.9348261952400208, f1: 0.3535, precision: 0.3333, recall: 0.3763\n",
      "2019-01-24T10:15:39.344897, step: 335, loss: 8.296672821044922, acc: 0.9416751265525818, f1: 0.3627, precision: 0.3524, recall: 0.3737\n",
      "2019-01-24T10:15:39.962945, step: 336, loss: 7.567107200622559, acc: 0.9504215717315674, f1: 0.4815, precision: 0.5417, recall: 0.4333\n",
      "2019-01-24T10:15:40.564232, step: 337, loss: 8.084463119506836, acc: 0.946252167224884, f1: 0.3313, precision: 0.3462, recall: 0.3176\n",
      "2019-01-24T10:15:41.267046, step: 338, loss: 8.494434356689453, acc: 0.9340388178825378, f1: 0.36, precision: 0.3711, recall: 0.3495\n",
      "2019-01-24T10:15:41.986140, step: 339, loss: 8.68614673614502, acc: 0.9302567839622498, f1: 0.3915, precision: 0.3866, recall: 0.3966\n",
      "2019-01-24T10:15:43.006683, step: 340, loss: 9.049321174621582, acc: 0.9337387681007385, f1: 0.4064, precision: 0.4113, recall: 0.4016\n",
      "2019-01-24T10:15:43.630483, step: 341, loss: 8.48335075378418, acc: 0.9376649260520935, f1: 0.3707, precision: 0.3654, recall: 0.3762\n",
      "2019-01-24T10:15:44.363411, step: 342, loss: 8.217283248901367, acc: 0.942520797252655, f1: 0.5179, precision: 0.5524, recall: 0.4874\n",
      "2019-01-24T10:15:45.449450, step: 343, loss: 8.517620086669922, acc: 0.9370334148406982, f1: 0.4, precision: 0.3983, recall: 0.4017\n",
      "2019-01-24T10:15:46.015943, step: 344, loss: 9.122794151306152, acc: 0.9323194026947021, f1: 0.392, precision: 0.3861, recall: 0.398\n",
      "2019-01-24T10:15:47.007972, step: 345, loss: 7.969696521759033, acc: 0.9447115659713745, f1: 0.4044, precision: 0.3854, recall: 0.4253\n",
      "2019-01-24T10:15:47.728830, step: 346, loss: 7.4837141036987305, acc: 0.9382669925689697, f1: 0.4124, precision: 0.396, recall: 0.4301\n",
      "2019-01-24T10:15:48.705417, step: 347, loss: 6.610699653625488, acc: 0.9482576847076416, f1: 0.5, precision: 0.5, recall: 0.5\n",
      "2019-01-24T10:15:49.410620, step: 348, loss: 6.773882865905762, acc: 0.9518644213676453, f1: 0.4231, precision: 0.44, recall: 0.4074\n",
      "2019-01-24T10:15:50.188882, step: 349, loss: 10.158241271972656, acc: 0.9285493493080139, f1: 0.4177, precision: 0.4127, recall: 0.4228\n",
      "2019-01-24T10:15:50.724877, step: 350, loss: 6.8694868087768555, acc: 0.9435935616493225, f1: 0.4444, precision: 0.4932, recall: 0.4045\n",
      "2019-01-24T10:15:51.335069, step: 351, loss: 8.772589683532715, acc: 0.9357602000236511, f1: 0.4377, precision: 0.4361, recall: 0.4394\n",
      "2019-01-24T10:15:51.805795, step: 352, loss: 6.704354763031006, acc: 0.9423374533653259, f1: 0.3804, precision: 0.3846, recall: 0.3763\n",
      "2019-01-24T10:15:54.159687, step: 353, loss: 8.752878189086914, acc: 0.9453973770141602, f1: 0.5375, precision: 0.5244, recall: 0.5513\n",
      "2019-01-24T10:15:54.608977, step: 354, loss: 7.933587551116943, acc: 0.9264822006225586, f1: 0.4577, precision: 0.4894, recall: 0.4299\n",
      "2019-01-24T10:15:55.242469, step: 355, loss: 7.293325424194336, acc: 0.9430356621742249, f1: 0.4492, precision: 0.4719, recall: 0.4286\n",
      "2019-01-24T10:15:56.156174, step: 356, loss: 9.260368347167969, acc: 0.9323036670684814, f1: 0.3889, precision: 0.3784, recall: 0.4\n",
      "2019-01-24T10:15:56.785203, step: 357, loss: 8.071501731872559, acc: 0.9391332268714905, f1: 0.4037, precision: 0.3793, recall: 0.4314\n",
      "2019-01-24T10:15:58.432899, step: 358, loss: 8.007097244262695, acc: 0.9410756826400757, f1: 0.5, precision: 0.5234, recall: 0.4786\n",
      "2019-01-24T10:15:58.995713, step: 359, loss: 8.581567764282227, acc: 0.9230210781097412, f1: 0.3767, precision: 0.359, recall: 0.3962\n",
      "2019-01-24T10:16:00.437580, step: 360, loss: 7.147430419921875, acc: 0.9406424760818481, f1: 0.4221, precision: 0.4038, recall: 0.4421\n",
      "2019-01-24T10:16:00.952290, step: 361, loss: 7.1514892578125, acc: 0.9388908743858337, f1: 0.3781, precision: 0.38, recall: 0.3762\n",
      "2019-01-24T10:16:01.714101, step: 362, loss: 9.52454662322998, acc: 0.9276568293571472, f1: 0.4298, precision: 0.4188, recall: 0.4414\n",
      "2019-01-24T10:16:02.980663, step: 363, loss: 7.99589204788208, acc: 0.9298449754714966, f1: 0.3733, precision: 0.4308, recall: 0.3294\n",
      "2019-01-24T10:16:03.716715, step: 364, loss: 7.882052421569824, acc: 0.9468782544136047, f1: 0.455, precision: 0.4725, recall: 0.4388\n",
      "2019-01-24T10:16:04.448281, step: 365, loss: 8.33358097076416, acc: 0.9320859313011169, f1: 0.3455, precision: 0.3363, recall: 0.3551\n",
      "2019-01-24T10:16:04.994327, step: 366, loss: 8.238333702087402, acc: 0.9351214170455933, f1: 0.3843, precision: 0.3729, recall: 0.3964\n",
      "2019-01-24T10:16:05.750466, step: 367, loss: 8.009133338928223, acc: 0.9390243887901306, f1: 0.3619, precision: 0.3689, recall: 0.3551\n",
      "2019-01-24T10:16:06.544841, step: 368, loss: 9.175262451171875, acc: 0.9244412779808044, f1: 0.4019, precision: 0.3772, recall: 0.43\n",
      "2019-01-24T10:16:07.264846, step: 369, loss: 8.142337799072266, acc: 0.9541397094726562, f1: 0.4205, precision: 0.4253, recall: 0.4157\n",
      "2019-01-24T10:16:07.853533, step: 370, loss: 7.4882354736328125, acc: 0.9511315226554871, f1: 0.4532, precision: 0.46, recall: 0.4466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:16:08.547605, step: 371, loss: 8.385440826416016, acc: 0.9402746558189392, f1: 0.4833, precision: 0.464, recall: 0.5043\n",
      "2019-01-24T10:16:09.299106, step: 372, loss: 6.438626289367676, acc: 0.9529665112495422, f1: 0.464, precision: 0.4394, recall: 0.4915\n",
      "2019-01-24T10:16:09.938286, step: 373, loss: 8.063287734985352, acc: 0.9261155724525452, f1: 0.4107, precision: 0.4107, recall: 0.4107\n",
      "2019-01-24T10:16:10.645357, step: 374, loss: 4.479620456695557, acc: 0.9658090472221375, f1: 0.3721, precision: 0.3582, recall: 0.3871\n",
      "2019-01-24T10:16:11.453498, step: 375, loss: 7.569798946380615, acc: 0.9401260614395142, f1: 0.4286, precision: 0.44, recall: 0.4177\n",
      "2019-01-24T10:16:11.935705, step: 376, loss: 8.544492721557617, acc: 0.9363957643508911, f1: 0.4087, precision: 0.3852, recall: 0.4352\n",
      "2019-01-24T10:16:14.514405, step: 377, loss: 10.554919242858887, acc: 0.9303977489471436, f1: 0.4675, precision: 0.4586, recall: 0.4768\n",
      "2019-01-24T10:16:15.072205, step: 378, loss: 6.404965877532959, acc: 0.9482815265655518, f1: 0.5085, precision: 0.5172, recall: 0.5\n",
      "2019-01-24T10:16:15.621045, step: 379, loss: 6.183886528015137, acc: 0.9504519701004028, f1: 0.3774, precision: 0.3846, recall: 0.3704\n",
      "2019-01-24T10:16:17.491827, step: 380, loss: 9.450252532958984, acc: 0.9415384531021118, f1: 0.3558, precision: 0.3776, recall: 0.3364\n",
      "2019-01-24T10:16:18.053482, step: 381, loss: 6.643319129943848, acc: 0.9483293294906616, f1: 0.4405, precision: 0.4353, recall: 0.4458\n",
      "2019-01-24T10:16:19.869525, step: 382, loss: 14.951730728149414, acc: 0.8965622186660767, f1: 0.3254, precision: 0.247, recall: 0.4767\n",
      "2019-01-24T10:16:20.484925, step: 383, loss: 7.703228950500488, acc: 0.9389525055885315, f1: 0.3529, precision: 0.3364, recall: 0.3711\n",
      "2019-01-24T10:16:21.300985, step: 384, loss: 7.988580226898193, acc: 0.9510104060173035, f1: 0.4911, precision: 0.5, recall: 0.4825\n",
      "2019-01-24T10:16:22.024487, step: 385, loss: 9.706321716308594, acc: 0.93000328540802, f1: 0.4372, precision: 0.4426, recall: 0.432\n",
      "2019-01-24T10:16:22.794854, step: 386, loss: 6.67079496383667, acc: 0.9506051540374756, f1: 0.5503, precision: 0.5909, recall: 0.5149\n",
      "2019-01-24T10:16:23.336552, step: 387, loss: 7.3139729499816895, acc: 0.9409368634223938, f1: 0.3958, precision: 0.4043, recall: 0.3878\n",
      "2019-01-24T10:16:23.802450, step: 388, loss: 6.2079010009765625, acc: 0.9472823739051819, f1: 0.4636, precision: 0.4322, recall: 0.5\n",
      "2019-01-24T10:16:25.026123, step: 389, loss: 6.309196472167969, acc: 0.9535614252090454, f1: 0.413, precision: 0.3762, recall: 0.4578\n",
      "2019-01-24T10:16:25.724062, step: 390, loss: 6.303621292114258, acc: 0.9440203309059143, f1: 0.4492, precision: 0.4667, recall: 0.433\n",
      "2019-01-24T10:16:26.843148, step: 391, loss: 7.348086357116699, acc: 0.9509740471839905, f1: 0.5158, precision: 0.5326, recall: 0.5\n",
      "2019-01-24T10:16:27.375939, step: 392, loss: 6.787631988525391, acc: 0.9496021270751953, f1: 0.4, precision: 0.413, recall: 0.3878\n",
      "2019-01-24T10:16:28.039241, step: 393, loss: 6.260180473327637, acc: 0.9539749026298523, f1: 0.5, precision: 0.5109, recall: 0.4896\n",
      "2019-01-24T10:16:28.652763, step: 394, loss: 8.021614074707031, acc: 0.9389209151268005, f1: 0.4455, precision: 0.4476, recall: 0.4434\n",
      "2019-01-24T10:16:29.552590, step: 395, loss: 7.833471298217773, acc: 0.9322592616081238, f1: 0.3736, precision: 0.3864, recall: 0.3617\n",
      "2019-01-24T10:16:30.127296, step: 396, loss: 8.831136703491211, acc: 0.9372730255126953, f1: 0.4184, precision: 0.402, recall: 0.4362\n",
      "2019-01-24T10:16:30.919440, step: 397, loss: 8.68847370147705, acc: 0.934761106967926, f1: 0.4251, precision: 0.4151, recall: 0.4356\n",
      "2019-01-24T10:16:31.972629, step: 398, loss: 5.701409339904785, acc: 0.9587291479110718, f1: 0.4258, precision: 0.4074, recall: 0.4459\n",
      "2019-01-24T10:16:32.555077, step: 399, loss: 8.904362678527832, acc: 0.933682382106781, f1: 0.3791, precision: 0.3774, recall: 0.381\n",
      "2019-01-24T10:16:33.341079, step: 400, loss: 8.019145965576172, acc: 0.9333813190460205, f1: 0.3195, precision: 0.3176, recall: 0.3214\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:16:58.545206, step: 400, loss: 7.0116111569934425, acc:0.9481591648525662, f1: 0.5154611111111111, precision: 0.5421861111111114, recall: 0.49267777777777777\n",
      "\n",
      "\n",
      "2019-01-24T10:16:59.204692, step: 401, loss: 6.394990921020508, acc: 0.9438584446907043, f1: 0.3708, precision: 0.3511, recall: 0.3929\n",
      "2019-01-24T10:16:59.706958, step: 402, loss: 7.0578765869140625, acc: 0.9532908797264099, f1: 0.4944, precision: 0.4835, recall: 0.5057\n",
      "2019-01-24T10:17:00.250102, step: 403, loss: 7.023829460144043, acc: 0.9369911551475525, f1: 0.4727, precision: 0.4937, recall: 0.4535\n",
      "2019-01-24T10:17:00.826235, step: 404, loss: 7.110712051391602, acc: 0.9411971569061279, f1: 0.3176, precision: 0.3253, recall: 0.3103\n",
      "2019-01-24T10:17:01.318634, step: 405, loss: 6.024535655975342, acc: 0.950647234916687, f1: 0.5114, precision: 0.5294, recall: 0.4945\n",
      "2019-01-24T10:17:01.953934, step: 406, loss: 6.6621599197387695, acc: 0.9494086503982544, f1: 0.3944, precision: 0.4242, recall: 0.3684\n",
      "2019-01-24T10:17:02.571801, step: 407, loss: 9.520430564880371, acc: 0.934345006942749, f1: 0.424, precision: 0.4344, recall: 0.4141\n",
      "2019-01-24T10:17:03.303213, step: 408, loss: 9.912571907043457, acc: 0.933747410774231, f1: 0.4455, precision: 0.4592, recall: 0.4327\n",
      "2019-01-24T10:17:03.827631, step: 409, loss: 6.287606239318848, acc: 0.9434167742729187, f1: 0.4845, precision: 0.4845, recall: 0.4845\n",
      "2019-01-24T10:17:06.473542, step: 410, loss: 7.538357734680176, acc: 0.9510179758071899, f1: 0.5206, precision: 0.519, recall: 0.5223\n",
      "2019-01-24T10:17:07.589247, step: 411, loss: 8.038795471191406, acc: 0.9424720406532288, f1: 0.449, precision: 0.4583, recall: 0.44\n",
      "2019-01-24T10:17:08.328804, step: 412, loss: 5.735308647155762, acc: 0.9553054571151733, f1: 0.5677, precision: 0.5328, recall: 0.6075\n",
      "2019-01-24T10:17:09.002752, step: 413, loss: 4.537896633148193, acc: 0.9729006290435791, f1: 0.5691, precision: 0.5556, recall: 0.5833\n",
      "2019-01-24T10:17:09.864753, step: 414, loss: 5.9217352867126465, acc: 0.9478001594543457, f1: 0.4516, precision: 0.4158, recall: 0.4941\n",
      "2019-01-24T10:17:10.386296, step: 415, loss: 4.8177642822265625, acc: 0.9586437940597534, f1: 0.4892, precision: 0.4789, recall: 0.5\n",
      "2019-01-24T10:17:10.954602, step: 416, loss: 5.176389694213867, acc: 0.959080159664154, f1: 0.4459, precision: 0.4521, recall: 0.44\n",
      "2019-01-24T10:17:12.025187, step: 417, loss: 8.629005432128906, acc: 0.9361842274665833, f1: 0.4018, precision: 0.4074, recall: 0.3964\n",
      "2019-01-24T10:17:12.579632, step: 418, loss: 4.986184120178223, acc: 0.9561175107955933, f1: 0.5, precision: 0.5114, recall: 0.4891\n",
      "2019-01-24T10:17:13.545553, step: 419, loss: 7.743723392486572, acc: 0.9465892314910889, f1: 0.4663, precision: 0.5, recall: 0.4369\n",
      "2019-01-24T10:17:14.971881, step: 420, loss: 6.967667102813721, acc: 0.9508245587348938, f1: 0.4422, precision: 0.449, recall: 0.4356\n",
      "2019-01-24T10:17:15.744466, step: 421, loss: 6.658894062042236, acc: 0.9494729042053223, f1: 0.4286, precision: 0.4648, recall: 0.3976\n",
      "2019-01-24T10:17:16.708114, step: 422, loss: 4.625401973724365, acc: 0.9661354422569275, f1: 0.5038, precision: 0.5238, recall: 0.4853\n",
      "2019-01-24T10:17:17.338191, step: 423, loss: 6.027103424072266, acc: 0.9497800469398499, f1: 0.4929, precision: 0.5098, recall: 0.4771\n",
      "2019-01-24T10:17:18.287633, step: 424, loss: 5.819683074951172, acc: 0.9473161101341248, f1: 0.3687, precision: 0.3667, recall: 0.3708\n",
      "2019-01-24T10:17:19.229358, step: 425, loss: 7.491074085235596, acc: 0.9528796076774597, f1: 0.6536, precision: 0.6536, recall: 0.6536\n",
      "2019-01-24T10:17:20.329690, step: 426, loss: 6.4028730392456055, acc: 0.9517487287521362, f1: 0.3771, precision: 0.3793, recall: 0.375\n",
      "2019-01-24T10:17:20.948077, step: 427, loss: 6.540760040283203, acc: 0.9451913237571716, f1: 0.5611, precision: 0.5688, recall: 0.5536\n",
      "2019-01-24T10:17:21.743320, step: 428, loss: 5.384481430053711, acc: 0.9642361402511597, f1: 0.5868, precision: 0.5833, recall: 0.5904\n",
      "2019-01-24T10:17:22.520732, step: 429, loss: 7.096053123474121, acc: 0.9482306838035583, f1: 0.5411, precision: 0.5234, recall: 0.56\n",
      "2019-01-24T10:17:23.237963, step: 430, loss: 9.234817504882812, acc: 0.9314980506896973, f1: 0.3644, precision: 0.3596, recall: 0.3694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:17:23.836701, step: 431, loss: 5.267063617706299, acc: 0.9563636183738708, f1: 0.5632, precision: 0.5506, recall: 0.5765\n",
      "2019-01-24T10:17:24.384949, step: 432, loss: 6.62562894821167, acc: 0.9497072100639343, f1: 0.5333, precision: 0.5263, recall: 0.5405\n",
      "2019-01-24T10:17:24.906218, step: 433, loss: 5.68382453918457, acc: 0.9532029032707214, f1: 0.4029, precision: 0.3784, recall: 0.4308\n",
      "2019-01-24T10:17:25.437450, step: 434, loss: 5.939169883728027, acc: 0.9563555121421814, f1: 0.4596, precision: 0.4868, recall: 0.4353\n",
      "2019-01-24T10:17:26.701372, step: 435, loss: 8.994407653808594, acc: 0.9366501569747925, f1: 0.4211, precision: 0.4528, recall: 0.3934\n",
      "2019-01-24T10:17:27.348847, step: 436, loss: 7.371400833129883, acc: 0.9394648671150208, f1: 0.3871, precision: 0.3853, recall: 0.3889\n",
      "2019-01-24T10:17:28.280854, step: 437, loss: 7.464181900024414, acc: 0.9465131759643555, f1: 0.4599, precision: 0.4574, recall: 0.4624\n",
      "2019-01-24T10:17:28.830927, step: 438, loss: 7.527116298675537, acc: 0.9373297095298767, f1: 0.5297, precision: 0.5421, recall: 0.5179\n",
      "2019-01-24T10:17:29.469608, step: 439, loss: 6.480099678039551, acc: 0.9492419362068176, f1: 0.4422, precision: 0.4444, recall: 0.44\n",
      "2019-01-24T10:17:30.533360, step: 440, loss: 6.552935600280762, acc: 0.947169840335846, f1: 0.4457, precision: 0.4382, recall: 0.4535\n",
      "2019-01-24T10:17:33.041603, step: 441, loss: 9.989097595214844, acc: 0.9313441514968872, f1: 0.5145, precision: 0.5338, recall: 0.4965\n",
      "2019-01-24T10:17:33.611727, step: 442, loss: 6.449237823486328, acc: 0.948932945728302, f1: 0.4757, precision: 0.4731, recall: 0.4783\n",
      "2019-01-24T10:17:36.451929, step: 443, loss: 9.244643211364746, acc: 0.9393589496612549, f1: 0.5372, precision: 0.5253, recall: 0.5497\n",
      "2019-01-24T10:17:37.165322, step: 444, loss: 5.915788173675537, acc: 0.960811972618103, f1: 0.4972, precision: 0.4737, recall: 0.5233\n",
      "2019-01-24T10:17:39.797684, step: 445, loss: 8.293519973754883, acc: 0.9561452269554138, f1: 0.6021, precision: 0.5918, recall: 0.6127\n",
      "2019-01-24T10:17:40.505648, step: 446, loss: 6.341190338134766, acc: 0.945936381816864, f1: 0.5302, precision: 0.5, recall: 0.5644\n",
      "2019-01-24T10:17:41.019513, step: 447, loss: 5.974847793579102, acc: 0.9524275064468384, f1: 0.52, precision: 0.5361, recall: 0.5049\n",
      "2019-01-24T10:17:41.684190, step: 448, loss: 8.171149253845215, acc: 0.9432664513587952, f1: 0.4299, precision: 0.4259, recall: 0.434\n",
      "2019-01-24T10:17:42.379579, step: 449, loss: 6.1609954833984375, acc: 0.9583888053894043, f1: 0.4809, precision: 0.5057, recall: 0.4583\n",
      "2019-01-24T10:17:43.188473, step: 450, loss: 6.836406707763672, acc: 0.9509483575820923, f1: 0.4699, precision: 0.5, recall: 0.4433\n",
      "2019-01-24T10:17:43.998150, step: 451, loss: 7.038403511047363, acc: 0.9500985145568848, f1: 0.5234, precision: 0.5437, recall: 0.5045\n",
      "2019-01-24T10:17:44.747711, step: 452, loss: 6.118574142456055, acc: 0.9634296298027039, f1: 0.5929, precision: 0.5982, recall: 0.5877\n",
      "2019-01-24T10:17:45.973847, step: 453, loss: 7.533428192138672, acc: 0.9446355700492859, f1: 0.4339, precision: 0.4607, recall: 0.41\n",
      "2019-01-24T10:17:47.060281, step: 454, loss: 6.890659809112549, acc: 0.9486697912216187, f1: 0.4766, precision: 0.4667, recall: 0.487\n",
      "2019-01-24T10:17:47.632829, step: 455, loss: 6.571528434753418, acc: 0.9473110437393188, f1: 0.4541, precision: 0.4516, recall: 0.4565\n",
      "2019-01-24T10:17:48.705880, step: 456, loss: 5.048748970031738, acc: 0.9696676135063171, f1: 0.5769, precision: 0.6164, recall: 0.5422\n",
      "2019-01-24T10:17:49.499653, step: 457, loss: 7.221360206604004, acc: 0.9416472315788269, f1: 0.4242, precision: 0.3962, recall: 0.4565\n",
      "2019-01-24T10:17:50.292119, step: 458, loss: 6.6682233810424805, acc: 0.9575572609901428, f1: 0.4396, precision: 0.4082, recall: 0.4762\n",
      "2019-01-24T10:17:51.240316, step: 459, loss: 9.4891996383667, acc: 0.9292520880699158, f1: 0.3077, precision: 0.2906, recall: 0.3269\n",
      "2019-01-24T10:17:54.107896, step: 460, loss: 5.856377601623535, acc: 0.9570021033287048, f1: 0.6316, precision: 0.6142, recall: 0.65\n",
      "2019-01-24T10:17:54.667549, step: 461, loss: 6.3679351806640625, acc: 0.9487813711166382, f1: 0.5111, precision: 0.4842, recall: 0.5412\n",
      "2019-01-24T10:17:55.346202, step: 462, loss: 8.364633560180664, acc: 0.9211788773536682, f1: 0.3612, precision: 0.3565, recall: 0.3661\n",
      "2019-01-24T10:17:55.921241, step: 463, loss: 9.273853302001953, acc: 0.933935821056366, f1: 0.4481, precision: 0.4737, recall: 0.4252\n",
      "2019-01-24T10:17:56.697679, step: 464, loss: 5.562345504760742, acc: 0.9570614099502563, f1: 0.5438, precision: 0.5463, recall: 0.5413\n",
      "2019-01-24T10:17:57.546329, step: 465, loss: 6.3083600997924805, acc: 0.956070065498352, f1: 0.455, precision: 0.4571, recall: 0.4528\n",
      "2019-01-24T10:17:58.500817, step: 466, loss: 7.148266792297363, acc: 0.949429988861084, f1: 0.4835, precision: 0.5301, recall: 0.4444\n",
      "2019-01-24T10:17:59.272446, step: 467, loss: 7.164388656616211, acc: 0.9456589818000793, f1: 0.6045, precision: 0.587, recall: 0.6231\n",
      "2019-01-24T10:17:59.852941, step: 468, loss: 4.957314491271973, acc: 0.9650349617004395, f1: 0.5658, precision: 0.5513, recall: 0.5811\n",
      "2019-01-24T10:18:00.541802, step: 469, loss: 6.0456085205078125, acc: 0.9503311514854431, f1: 0.4804, precision: 0.4623, recall: 0.5\n",
      "2019-01-24T10:18:01.501886, step: 470, loss: 6.858769416809082, acc: 0.9525547623634338, f1: 0.515, precision: 0.4839, recall: 0.5505\n",
      "2019-01-24T10:18:02.240846, step: 471, loss: 6.0647735595703125, acc: 0.9513960480690002, f1: 0.4025, precision: 0.381, recall: 0.4267\n",
      "2019-01-24T10:18:02.806333, step: 472, loss: 6.420166015625, acc: 0.9523809552192688, f1: 0.5341, precision: 0.5402, recall: 0.5281\n",
      "2019-01-24T10:18:03.699910, step: 473, loss: 4.856721878051758, acc: 0.9628638625144958, f1: 0.4605, precision: 0.4605, recall: 0.4605\n",
      "2019-01-24T10:18:04.721469, step: 474, loss: 6.217303276062012, acc: 0.954257071018219, f1: 0.6051, precision: 0.6344, recall: 0.5784\n",
      "2019-01-24T10:18:05.275955, step: 475, loss: 5.501946449279785, acc: 0.9545949697494507, f1: 0.5608, precision: 0.5579, recall: 0.5638\n",
      "2019-01-24T10:18:06.082595, step: 476, loss: 6.063858985900879, acc: 0.9479267001152039, f1: 0.4974, precision: 0.5053, recall: 0.4898\n",
      "2019-01-24T10:18:06.403327, step: 477, loss: 7.3241777420043945, acc: 0.9330217838287354, f1: 0.4074, precision: 0.44, recall: 0.3793\n",
      "2019-01-24T10:18:06.878187, step: 478, loss: 6.067453861236572, acc: 0.9479451775550842, f1: 0.5408, precision: 0.5521, recall: 0.53\n",
      "2019-01-24T10:18:07.634239, step: 479, loss: 7.766725063323975, acc: 0.9367960095405579, f1: 0.4938, precision: 0.4918, recall: 0.4959\n",
      "2019-01-24T10:18:08.298379, step: 480, loss: 7.585683822631836, acc: 0.9467884302139282, f1: 0.502, precision: 0.5203, recall: 0.4848\n",
      "2019-01-24T10:18:09.374004, step: 481, loss: 6.6918487548828125, acc: 0.9565625190734863, f1: 0.4431, precision: 0.4353, recall: 0.4512\n",
      "2019-01-24T10:18:10.176972, step: 482, loss: 7.678905487060547, acc: 0.9451219439506531, f1: 0.4783, precision: 0.4622, recall: 0.4955\n",
      "2019-01-24T10:18:10.844007, step: 483, loss: 5.14744234085083, acc: 0.9559613466262817, f1: 0.475, precision: 0.4691, recall: 0.481\n",
      "2019-01-24T10:18:11.503805, step: 484, loss: 5.245433807373047, acc: 0.953812301158905, f1: 0.3975, precision: 0.3951, recall: 0.4\n",
      "2019-01-24T10:18:12.176038, step: 485, loss: 5.416746139526367, acc: 0.9529792070388794, f1: 0.3978, precision: 0.383, recall: 0.4138\n",
      "2019-01-24T10:18:13.035318, step: 486, loss: 4.964808940887451, acc: 0.9698931574821472, f1: 0.6034, precision: 0.6429, recall: 0.5684\n",
      "2019-01-24T10:18:13.530748, step: 487, loss: 5.910663604736328, acc: 0.9451737403869629, f1: 0.5302, precision: 0.5089, recall: 0.5534\n",
      "2019-01-24T10:18:14.263160, step: 488, loss: 7.952937126159668, acc: 0.9341846704483032, f1: 0.4505, precision: 0.4274, recall: 0.4762\n",
      "2019-01-24T10:18:14.983436, step: 489, loss: 9.793328285217285, acc: 0.9238064289093018, f1: 0.4711, precision: 0.5, recall: 0.4453\n",
      "2019-01-24T10:18:15.745665, step: 490, loss: 6.678063869476318, acc: 0.9480169415473938, f1: 0.4746, precision: 0.4828, recall: 0.4667\n",
      "2019-01-24T10:18:16.358768, step: 491, loss: 7.807077407836914, acc: 0.9364407062530518, f1: 0.381, precision: 0.3902, recall: 0.3721\n",
      "2019-01-24T10:18:16.866014, step: 492, loss: 6.33750057220459, acc: 0.9418982267379761, f1: 0.4977, precision: 0.4783, recall: 0.5189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:18:17.374667, step: 493, loss: 5.926025867462158, acc: 0.9467476606369019, f1: 0.4632, precision: 0.449, recall: 0.4783\n",
      "2019-01-24T10:18:17.981146, step: 494, loss: 6.266489028930664, acc: 0.9522398114204407, f1: 0.4416, precision: 0.4146, recall: 0.4722\n",
      "2019-01-24T10:18:19.208941, step: 495, loss: 5.547704219818115, acc: 0.9577370882034302, f1: 0.4727, precision: 0.4483, recall: 0.5\n",
      "2019-01-24T10:18:19.878789, step: 496, loss: 6.158569812774658, acc: 0.9489760398864746, f1: 0.5631, precision: 0.5631, recall: 0.5631\n",
      "2019-01-24T10:18:20.581453, step: 497, loss: 5.82193660736084, acc: 0.9571149945259094, f1: 0.5486, precision: 0.5393, recall: 0.5581\n",
      "2019-01-24T10:18:21.693882, step: 498, loss: 6.266043663024902, acc: 0.9637613296508789, f1: 0.5714, precision: 0.5567, recall: 0.587\n",
      "2019-01-24T10:18:22.182348, step: 499, loss: 5.574204444885254, acc: 0.9556313753128052, f1: 0.5806, precision: 0.6, recall: 0.5625\n",
      "2019-01-24T10:18:23.159945, step: 500, loss: 5.795050144195557, acc: 0.9561904668807983, f1: 0.5287, precision: 0.561, recall: 0.5\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:18:48.025691, step: 500, loss: 6.047402328915066, acc:0.9535629202922186, f1: 0.5606611111111112, precision: 0.5975583333333332, recall: 0.5286138888888889\n",
      "\n",
      "\n",
      "2019-01-24T10:18:48.542273, step: 501, loss: 7.424444198608398, acc: 0.9425806403160095, f1: 0.5272, precision: 0.5339, recall: 0.5207\n",
      "2019-01-24T10:18:49.369087, step: 502, loss: 6.067561626434326, acc: 0.9520272612571716, f1: 0.4751, precision: 0.4778, recall: 0.4725\n",
      "2019-01-24T10:18:50.034121, step: 503, loss: 5.491511344909668, acc: 0.9592518210411072, f1: 0.551, precision: 0.5745, recall: 0.5294\n",
      "2019-01-24T10:18:50.635760, step: 504, loss: 7.074220180511475, acc: 0.9503214359283447, f1: 0.4476, precision: 0.4608, recall: 0.4352\n",
      "2019-01-24T10:18:53.151591, step: 505, loss: 5.79652214050293, acc: 0.9586288332939148, f1: 0.5938, precision: 0.5984, recall: 0.5891\n",
      "2019-01-24T10:18:53.740215, step: 506, loss: 6.791730880737305, acc: 0.9535759091377258, f1: 0.601, precision: 0.6162, recall: 0.5865\n",
      "2019-01-24T10:18:54.479599, step: 507, loss: 4.447260856628418, acc: 0.9623942375183105, f1: 0.4865, precision: 0.4865, recall: 0.4865\n",
      "2019-01-24T10:18:55.178488, step: 508, loss: 6.258204460144043, acc: 0.9526563286781311, f1: 0.5683, precision: 0.5532, recall: 0.5843\n",
      "2019-01-24T10:18:55.668113, step: 509, loss: 5.772237777709961, acc: 0.9597488641738892, f1: 0.5409, precision: 0.5658, recall: 0.5181\n",
      "2019-01-24T10:18:56.219578, step: 510, loss: 6.087503433227539, acc: 0.9540376663208008, f1: 0.4718, precision: 0.4742, recall: 0.4694\n",
      "2019-01-24T10:18:56.801499, step: 511, loss: 6.425483703613281, acc: 0.9493580460548401, f1: 0.552, precision: 0.5351, recall: 0.5701\n",
      "2019-01-24T10:18:58.146287, step: 512, loss: 4.6322197914123535, acc: 0.9632520079612732, f1: 0.5153, precision: 0.506, recall: 0.525\n",
      "2019-01-24T10:18:58.802540, step: 513, loss: 4.750847816467285, acc: 0.9636678099632263, f1: 0.474, precision: 0.4505, recall: 0.5\n",
      "2019-01-24T10:18:59.482585, step: 514, loss: 7.249142646789551, acc: 0.9470151662826538, f1: 0.46, precision: 0.4423, recall: 0.4792\n",
      "2019-01-24T10:19:00.123745, step: 515, loss: 6.16392707824707, acc: 0.9438738226890564, f1: 0.4605, precision: 0.493, recall: 0.4321\n",
      "2019-01-24T10:19:00.591310, step: 516, loss: 4.988359451293945, acc: 0.9619771838188171, f1: 0.5604, precision: 0.5604, recall: 0.5604\n",
      "2019-01-24T10:19:01.356605, step: 517, loss: 6.415569305419922, acc: 0.9442216753959656, f1: 0.5312, precision: 0.5604, recall: 0.505\n",
      "2019-01-24T10:19:01.841642, step: 518, loss: 6.675164222717285, acc: 0.9385809898376465, f1: 0.45, precision: 0.4444, recall: 0.4557\n",
      "2019-01-24T10:19:02.353687, step: 519, loss: 4.747264862060547, acc: 0.9589917063713074, f1: 0.5468, precision: 0.5205, recall: 0.5758\n",
      "2019-01-24T10:19:02.871391, step: 520, loss: 5.635446071624756, acc: 0.9569678902626038, f1: 0.5241, precision: 0.4949, recall: 0.5568\n",
      "2019-01-24T10:19:03.584285, step: 521, loss: 3.8151168823242188, acc: 0.9750341773033142, f1: 0.5692, precision: 0.5441, recall: 0.5968\n",
      "2019-01-24T10:19:04.154730, step: 522, loss: 7.462023735046387, acc: 0.9312261343002319, f1: 0.4264, precision: 0.4242, recall: 0.4286\n",
      "2019-01-24T10:19:04.688033, step: 523, loss: 4.851740837097168, acc: 0.9598085880279541, f1: 0.5028, precision: 0.4945, recall: 0.5114\n",
      "2019-01-24T10:19:05.463156, step: 524, loss: 5.8598456382751465, acc: 0.9516331553459167, f1: 0.4813, precision: 0.4688, recall: 0.4945\n",
      "2019-01-24T10:19:06.167194, step: 525, loss: 5.594970703125, acc: 0.9533730149269104, f1: 0.506, precision: 0.5316, recall: 0.4828\n",
      "2019-01-24T10:19:07.068877, step: 526, loss: 6.725955009460449, acc: 0.9452542662620544, f1: 0.4898, precision: 0.5106, recall: 0.4706\n",
      "2019-01-24T10:19:07.716183, step: 527, loss: 4.573502540588379, acc: 0.9647254347801208, f1: 0.4713, precision: 0.4744, recall: 0.4684\n",
      "2019-01-24T10:19:08.297368, step: 528, loss: 7.313271522521973, acc: 0.9448275566101074, f1: 0.4933, precision: 0.5286, recall: 0.4625\n",
      "2019-01-24T10:19:09.092476, step: 529, loss: 4.210638523101807, acc: 0.9649354815483093, f1: 0.5571, precision: 0.6094, recall: 0.5132\n",
      "2019-01-24T10:19:10.049467, step: 530, loss: 6.747921943664551, acc: 0.9474689364433289, f1: 0.504, precision: 0.5207, recall: 0.4884\n",
      "2019-01-24T10:19:10.737698, step: 531, loss: 6.097790718078613, acc: 0.9522162675857544, f1: 0.5689, precision: 0.5766, recall: 0.5614\n",
      "2019-01-24T10:19:12.976558, step: 532, loss: 5.092988967895508, acc: 0.9655282497406006, f1: 0.5846, precision: 0.5876, recall: 0.5816\n",
      "2019-01-24T10:19:13.468946, step: 533, loss: 5.332342147827148, acc: 0.9612974524497986, f1: 0.5466, precision: 0.6027, recall: 0.5\n",
      "2019-01-24T10:19:14.145216, step: 534, loss: 7.936540126800537, acc: 0.9426590800285339, f1: 0.5725, precision: 0.562, recall: 0.5833\n",
      "2019-01-24T10:19:14.724967, step: 535, loss: 6.176294326782227, acc: 0.9454778432846069, f1: 0.4622, precision: 0.4602, recall: 0.4643\n",
      "2019-01-24T10:19:15.354003, step: 536, loss: 4.297173500061035, acc: 0.9679203629493713, f1: 0.589, precision: 0.5647, recall: 0.6154\n",
      "2019-01-24T10:19:16.095695, step: 537, loss: 7.4628586769104, acc: 0.9332401156425476, f1: 0.5064, precision: 0.4836, recall: 0.5315\n",
      "2019-01-24T10:19:17.119028, step: 538, loss: 4.640142917633057, acc: 0.9640856385231018, f1: 0.5402, precision: 0.5165, recall: 0.5663\n",
      "2019-01-24T10:19:17.800312, step: 539, loss: 6.033037185668945, acc: 0.9602199196815491, f1: 0.5692, precision: 0.5625, recall: 0.576\n",
      "2019-01-24T10:19:18.670237, step: 540, loss: 6.025233745574951, acc: 0.9481815099716187, f1: 0.4532, precision: 0.451, recall: 0.4554\n",
      "2019-01-24T10:19:19.593182, step: 541, loss: 7.063172340393066, acc: 0.9483203887939453, f1: 0.5049, precision: 0.5361, recall: 0.4771\n",
      "2019-01-24T10:19:20.148658, step: 542, loss: 7.358419895172119, acc: 0.9424251914024353, f1: 0.4528, precision: 0.4324, recall: 0.4752\n",
      "2019-01-24T10:19:20.807995, step: 543, loss: 6.00258731842041, acc: 0.9535027742385864, f1: 0.6346, precision: 0.6346, recall: 0.6346\n",
      "2019-01-24T10:19:21.583926, step: 544, loss: 6.8519182205200195, acc: 0.9529523253440857, f1: 0.561, precision: 0.5702, recall: 0.552\n",
      "2019-01-24T10:19:22.083813, step: 545, loss: 4.695608139038086, acc: 0.9581378698348999, f1: 0.5063, precision: 0.5263, recall: 0.4878\n",
      "2019-01-24T10:19:22.592742, step: 546, loss: 4.726372718811035, acc: 0.9548558592796326, f1: 0.5033, precision: 0.5352, recall: 0.475\n",
      "2019-01-24T10:19:23.607973, step: 547, loss: 5.165407180786133, acc: 0.9622455835342407, f1: 0.5765, precision: 0.6125, recall: 0.5444\n",
      "2019-01-24T10:19:24.158485, step: 548, loss: 4.218703269958496, acc: 0.960841178894043, f1: 0.535, precision: 0.56, recall: 0.5122\n",
      "2019-01-24T10:19:24.933305, step: 549, loss: 6.1380133628845215, acc: 0.953871488571167, f1: 0.5072, precision: 0.5146, recall: 0.5\n",
      "2019-01-24T10:19:25.780610, step: 550, loss: 4.076623916625977, acc: 0.9699709415435791, f1: 0.4812, precision: 0.4776, recall: 0.4848\n",
      "2019-01-24T10:19:26.262716, step: 551, loss: 5.20149040222168, acc: 0.947326123714447, f1: 0.3459, precision: 0.3485, recall: 0.3433\n",
      "2019-01-24T10:19:26.839446, step: 552, loss: 4.995843887329102, acc: 0.9529898166656494, f1: 0.5756, precision: 0.6082, recall: 0.5463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:19:27.323157, step: 553, loss: 6.133538246154785, acc: 0.950596272945404, f1: 0.5241, precision: 0.5269, recall: 0.5213\n",
      "2019-01-24T10:19:28.104004, step: 554, loss: 5.209930896759033, acc: 0.9568094611167908, f1: 0.5, precision: 0.4815, recall: 0.52\n",
      "2019-01-24T10:19:28.661678, step: 555, loss: 6.563994407653809, acc: 0.9493585228919983, f1: 0.5283, precision: 0.5333, recall: 0.5234\n",
      "2019-01-24T10:19:31.265420, step: 556, loss: 5.14765739440918, acc: 0.9610834121704102, f1: 0.6182, precision: 0.6355, recall: 0.6018\n",
      "2019-01-24T10:19:32.055221, step: 557, loss: 6.585561752319336, acc: 0.9515528082847595, f1: 0.5098, precision: 0.5, recall: 0.52\n",
      "2019-01-24T10:19:32.824333, step: 558, loss: 7.3457441329956055, acc: 0.9471532106399536, f1: 0.4956, precision: 0.4786, recall: 0.5138\n",
      "2019-01-24T10:19:33.305844, step: 559, loss: 5.1442413330078125, acc: 0.9544740915298462, f1: 0.6063, precision: 0.6442, recall: 0.5726\n",
      "2019-01-24T10:19:34.365264, step: 560, loss: 6.35007905960083, acc: 0.9492629170417786, f1: 0.4686, precision: 0.4713, recall: 0.4659\n",
      "2019-01-24T10:19:35.362841, step: 561, loss: 6.439552307128906, acc: 0.9489123225212097, f1: 0.534, precision: 0.5238, recall: 0.5446\n",
      "2019-01-24T10:19:36.090245, step: 562, loss: 5.618390083312988, acc: 0.9566009044647217, f1: 0.6452, precision: 0.64, recall: 0.6504\n",
      "2019-01-24T10:19:36.810835, step: 563, loss: 5.78610372543335, acc: 0.9518197774887085, f1: 0.5333, precision: 0.5128, recall: 0.5556\n",
      "2019-01-24T10:19:37.684462, step: 564, loss: 7.26265811920166, acc: 0.9457364082336426, f1: 0.5, precision: 0.5, recall: 0.5\n",
      "2019-01-24T10:19:38.291643, step: 565, loss: 5.590199947357178, acc: 0.9497487545013428, f1: 0.543, precision: 0.5172, recall: 0.5714\n",
      "2019-01-24T10:19:39.110943, step: 566, loss: 7.193029403686523, acc: 0.9481880068778992, f1: 0.4883, precision: 0.5149, recall: 0.4643\n",
      "2019-01-24T10:19:39.988045, step: 567, loss: 5.251772403717041, acc: 0.9627937078475952, f1: 0.5698, precision: 0.5632, recall: 0.5765\n",
      "2019-01-24T10:19:40.898579, step: 568, loss: 4.769949436187744, acc: 0.9649897813796997, f1: 0.5667, precision: 0.573, recall: 0.5604\n",
      "2019-01-24T10:19:41.402880, step: 569, loss: 5.61175537109375, acc: 0.954325258731842, f1: 0.5301, precision: 0.55, recall: 0.5116\n",
      "2019-01-24T10:19:41.930121, step: 570, loss: 6.478377342224121, acc: 0.9473330974578857, f1: 0.5049, precision: 0.5532, recall: 0.4643\n",
      "2019-01-24T10:19:42.620497, step: 571, loss: 6.319361686706543, acc: 0.947317361831665, f1: 0.5525, precision: 0.6024, recall: 0.5102\n",
      "2019-01-24T10:19:43.416942, step: 572, loss: 5.253487586975098, acc: 0.9612599611282349, f1: 0.5629, precision: 0.5663, recall: 0.5595\n",
      "2019-01-24T10:19:44.844076, step: 573, loss: 5.953246116638184, acc: 0.9477517604827881, f1: 0.4842, precision: 0.451, recall: 0.5227\n",
      "2019-01-24T10:19:45.600458, step: 574, loss: 5.677770614624023, acc: 0.9482188820838928, f1: 0.5104, precision: 0.5104, recall: 0.5104\n",
      "2019-01-24T10:19:46.016402, step: 575, loss: 8.276951789855957, acc: 0.9170664548873901, f1: 0.412, precision: 0.4068, recall: 0.4174\n",
      "2019-01-24T10:19:46.515338, step: 576, loss: 8.698171615600586, acc: 0.9282432198524475, f1: 0.4615, precision: 0.5233, recall: 0.4128\n",
      "2019-01-24T10:19:47.148722, step: 577, loss: 5.623253345489502, acc: 0.9557291865348816, f1: 0.6389, precision: 0.6389, recall: 0.6389\n",
      "2019-01-24T10:19:47.932092, step: 578, loss: 5.834911346435547, acc: 0.950504720211029, f1: 0.5376, precision: 0.5102, recall: 0.5682\n",
      "2019-01-24T10:19:48.669019, step: 579, loss: 6.992223739624023, acc: 0.9521573185920715, f1: 0.5205, precision: 0.5089, recall: 0.5327\n",
      "2019-01-24T10:19:49.184331, step: 580, loss: 6.25076961517334, acc: 0.9509097337722778, f1: 0.5308, precision: 0.5385, recall: 0.5234\n",
      "2019-01-24T10:19:49.914178, step: 581, loss: 4.847715377807617, acc: 0.9619341492652893, f1: 0.4937, precision: 0.4937, recall: 0.4937\n",
      "2019-01-24T10:19:50.582257, step: 582, loss: 3.5725409984588623, acc: 0.9639440774917603, f1: 0.5802, precision: 0.5802, recall: 0.5802\n",
      "2019-01-24T10:19:51.499589, step: 583, loss: 6.414514064788818, acc: 0.9509771466255188, f1: 0.494, precision: 0.5395, recall: 0.4556\n",
      "2019-01-24T10:19:52.564604, step: 584, loss: 6.374660491943359, acc: 0.9371448755264282, f1: 0.442, precision: 0.4494, recall: 0.4348\n",
      "2019-01-24T10:19:53.336416, step: 585, loss: 6.868846893310547, acc: 0.9467280507087708, f1: 0.4653, precision: 0.5109, recall: 0.4273\n",
      "2019-01-24T10:19:54.347919, step: 586, loss: 5.636224746704102, acc: 0.9551569223403931, f1: 0.5026, precision: 0.5158, recall: 0.49\n",
      "2019-01-24T10:19:55.389858, step: 587, loss: 6.889955520629883, acc: 0.9551906585693359, f1: 0.4257, precision: 0.4343, recall: 0.4175\n",
      "2019-01-24T10:19:56.071256, step: 588, loss: 5.821504592895508, acc: 0.9531717300415039, f1: 0.5026, precision: 0.5106, recall: 0.4948\n",
      "2019-01-24T10:19:56.701273, step: 589, loss: 4.58526086807251, acc: 0.9642159938812256, f1: 0.5535, precision: 0.5641, recall: 0.5432\n",
      "2019-01-24T10:19:57.232861, step: 590, loss: 5.07137393951416, acc: 0.965775728225708, f1: 0.5761, precision: 0.5761, recall: 0.5761\n",
      "2019-01-24T10:19:58.179538, step: 591, loss: 5.978570938110352, acc: 0.95233553647995, f1: 0.5887, precision: 0.562, recall: 0.6182\n",
      "2019-01-24T10:19:58.713495, step: 592, loss: 4.795543193817139, acc: 0.9533431529998779, f1: 0.5549, precision: 0.5647, recall: 0.5455\n",
      "2019-01-24T10:19:59.978321, step: 593, loss: 6.19800329208374, acc: 0.9508973956108093, f1: 0.5474, precision: 0.5357, recall: 0.5597\n",
      "2019-01-24T10:20:00.673227, step: 594, loss: 4.891192436218262, acc: 0.957446813583374, f1: 0.4667, precision: 0.4545, recall: 0.4795\n",
      "2019-01-24T10:20:01.763190, step: 595, loss: 6.613651752471924, acc: 0.951212465763092, f1: 0.4955, precision: 0.4825, recall: 0.5093\n",
      "2019-01-24T10:20:02.302582, step: 596, loss: 5.420251369476318, acc: 0.9507502913475037, f1: 0.6132, precision: 0.619, recall: 0.6075\n",
      "2019-01-24T10:20:03.119364, step: 597, loss: 4.317790985107422, acc: 0.9700520634651184, f1: 0.6387, precision: 0.6224, recall: 0.6559\n",
      "2019-01-24T10:20:03.884079, step: 598, loss: 7.259481430053711, acc: 0.9521924257278442, f1: 0.5561, precision: 0.5636, recall: 0.5487\n",
      "2019-01-24T10:20:04.709033, step: 599, loss: 5.900245189666748, acc: 0.9485981464385986, f1: 0.5732, precision: 0.5529, recall: 0.5949\n",
      "2019-01-24T10:20:05.354028, step: 600, loss: 6.426012992858887, acc: 0.9485121965408325, f1: 0.466, precision: 0.4615, recall: 0.4706\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:20:30.111301, step: 600, loss: 5.507859574423896, acc:0.9572591549820371, f1: 0.6028, precision: 0.6399027777777777, recall: 0.5711916666666664\n",
      "\n",
      "\n",
      "2019-01-24T10:20:30.692296, step: 601, loss: 4.403719902038574, acc: 0.966300368309021, f1: 0.6543, precision: 0.6709, recall: 0.6386\n",
      "2019-01-24T10:20:31.611945, step: 602, loss: 5.8233513832092285, acc: 0.9608227014541626, f1: 0.4724, precision: 0.4608, recall: 0.4845\n",
      "2019-01-24T10:20:32.492550, step: 603, loss: 3.372527599334717, acc: 0.9712802767753601, f1: 0.642, precision: 0.6341, recall: 0.65\n",
      "2019-01-24T10:20:33.302243, step: 604, loss: 7.274921894073486, acc: 0.9464788436889648, f1: 0.5892, precision: 0.6339, recall: 0.5504\n",
      "2019-01-24T10:20:35.915565, step: 605, loss: 5.645689964294434, acc: 0.9611957669258118, f1: 0.7212, precision: 0.7185, recall: 0.7239\n",
      "2019-01-24T10:20:36.781231, step: 606, loss: 5.1970720291137695, acc: 0.96278315782547, f1: 0.4615, precision: 0.4643, recall: 0.4588\n",
      "2019-01-24T10:20:37.687348, step: 607, loss: 6.894444942474365, acc: 0.9454763531684875, f1: 0.4976, precision: 0.52, recall: 0.4771\n",
      "2019-01-24T10:20:38.194344, step: 608, loss: 3.985938787460327, acc: 0.9624548554420471, f1: 0.586, precision: 0.6053, recall: 0.5679\n",
      "2019-01-24T10:20:38.800081, step: 609, loss: 7.378990173339844, acc: 0.9354058504104614, f1: 0.5093, precision: 0.5093, recall: 0.5093\n",
      "2019-01-24T10:20:39.450893, step: 610, loss: 6.442115306854248, acc: 0.9500674605369568, f1: 0.4524, precision: 0.4368, recall: 0.4691\n",
      "2019-01-24T10:20:40.053837, step: 611, loss: 5.48886775970459, acc: 0.9510669708251953, f1: 0.4125, precision: 0.4074, recall: 0.4177\n",
      "2019-01-24T10:20:40.620778, step: 612, loss: 5.029872417449951, acc: 0.9616487622261047, f1: 0.4478, precision: 0.4762, recall: 0.4225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:20:42.193858, step: 613, loss: 6.7744669914245605, acc: 0.9554292559623718, f1: 0.4947, precision: 0.4845, recall: 0.5054\n",
      "2019-01-24T10:20:42.873762, step: 614, loss: 7.211940765380859, acc: 0.9449983835220337, f1: 0.5505, precision: 0.566, recall: 0.5357\n",
      "2019-01-24T10:20:43.779920, step: 615, loss: 5.687655448913574, acc: 0.9596773982048035, f1: 0.5053, precision: 0.5275, recall: 0.4848\n",
      "2019-01-24T10:20:44.399572, step: 616, loss: 4.632360458374023, acc: 0.9649941921234131, f1: 0.5348, precision: 0.5319, recall: 0.5376\n",
      "2019-01-24T10:20:45.293917, step: 617, loss: 7.467626571655273, acc: 0.9443961977958679, f1: 0.4615, precision: 0.4752, recall: 0.4486\n",
      "2019-01-24T10:20:45.847429, step: 618, loss: 4.04349422454834, acc: 0.9707602262496948, f1: 0.5918, precision: 0.5979, recall: 0.5859\n",
      "2019-01-24T10:20:46.402183, step: 619, loss: 5.136196136474609, acc: 0.9627676010131836, f1: 0.5904, precision: 0.6049, recall: 0.5765\n",
      "2019-01-24T10:20:47.030525, step: 620, loss: 4.44102668762207, acc: 0.9628422260284424, f1: 0.6738, precision: 0.6774, recall: 0.6702\n",
      "2019-01-24T10:20:48.066174, step: 621, loss: 4.507443428039551, acc: 0.9687978029251099, f1: 0.5979, precision: 0.617, recall: 0.58\n",
      "2019-01-24T10:20:49.008842, step: 622, loss: 4.588805675506592, acc: 0.9568845629692078, f1: 0.4173, precision: 0.4328, recall: 0.4028\n",
      "2019-01-24T10:20:49.778576, step: 623, loss: 5.429039478302002, acc: 0.9571942687034607, f1: 0.6129, precision: 0.6628, recall: 0.57\n",
      "2019-01-24T10:20:50.496784, step: 624, loss: 6.316949367523193, acc: 0.9509312510490417, f1: 0.5444, precision: 0.5506, recall: 0.5385\n",
      "2019-01-24T10:20:50.891763, step: 625, loss: 5.832127571105957, acc: 0.9585669636726379, f1: 0.555, precision: 0.5638, recall: 0.5464\n",
      "2019-01-24T10:20:51.624122, step: 626, loss: 5.905092239379883, acc: 0.9606879353523254, f1: 0.6943, precision: 0.7077, recall: 0.6815\n",
      "2019-01-24T10:20:52.107210, step: 627, loss: 5.842443466186523, acc: 0.9486615061759949, f1: 0.4748, precision: 0.4648, recall: 0.4853\n",
      "2019-01-24T10:20:52.639255, step: 628, loss: 5.134411334991455, acc: 0.9575220942497253, f1: 0.5584, precision: 0.5288, recall: 0.5914\n",
      "2019-01-24T10:20:53.195258, step: 629, loss: 5.635470867156982, acc: 0.9541507363319397, f1: 0.5752, precision: 0.5652, recall: 0.5856\n",
      "2019-01-24T10:20:53.961723, step: 630, loss: 4.633650779724121, acc: 0.9579741358757019, f1: 0.4588, precision: 0.4815, recall: 0.4382\n",
      "2019-01-24T10:20:54.706399, step: 631, loss: 4.307785511016846, acc: 0.9689174890518188, f1: 0.5541, precision: 0.5694, recall: 0.5395\n",
      "2019-01-24T10:20:57.365769, step: 632, loss: 6.045802116394043, acc: 0.9548331499099731, f1: 0.5833, precision: 0.6063, recall: 0.562\n",
      "2019-01-24T10:20:58.821554, step: 633, loss: 4.991470813751221, acc: 0.9639550447463989, f1: 0.5341, precision: 0.5595, recall: 0.5109\n",
      "2019-01-24T10:20:59.617555, step: 634, loss: 6.493253707885742, acc: 0.948019802570343, f1: 0.4545, precision: 0.4639, recall: 0.4455\n",
      "2019-01-24T10:21:00.451189, step: 635, loss: 6.564883232116699, acc: 0.9455977082252502, f1: 0.5285, precision: 0.5484, recall: 0.51\n",
      "2019-01-24T10:21:01.180490, step: 636, loss: 6.014227867126465, acc: 0.9439818263053894, f1: 0.5592, precision: 0.59, recall: 0.5315\n",
      "2019-01-24T10:21:02.161563, step: 637, loss: 5.023421287536621, acc: 0.966625452041626, f1: 0.6473, precision: 0.6442, recall: 0.6505\n",
      "2019-01-24T10:21:02.843031, step: 638, loss: 4.635186195373535, acc: 0.9658551812171936, f1: 0.5109, precision: 0.5, recall: 0.5222\n",
      "2019-01-24T10:21:03.570255, step: 639, loss: 6.230854034423828, acc: 0.9462592005729675, f1: 0.5225, precision: 0.537, recall: 0.5088\n",
      "2019-01-24T10:21:04.228029, step: 640, loss: 4.85703182220459, acc: 0.9631507992744446, f1: 0.62, precision: 0.6139, recall: 0.6263\n",
      "2019-01-24T10:21:04.886612, step: 641, loss: 5.631532669067383, acc: 0.9586374759674072, f1: 0.5771, precision: 0.5421, recall: 0.617\n",
      "2019-01-24T10:21:07.633792, step: 642, loss: 6.0828046798706055, acc: 0.9635231494903564, f1: 0.726, precision: 0.731, recall: 0.7211\n",
      "2019-01-24T10:21:08.501738, step: 643, loss: 5.064853668212891, acc: 0.9582079648971558, f1: 0.5155, precision: 0.5051, recall: 0.5263\n",
      "2019-01-24T10:21:09.057449, step: 644, loss: 5.803393363952637, acc: 0.9558568000793457, f1: 0.5414, precision: 0.5385, recall: 0.5444\n",
      "2019-01-24T10:21:10.350752, step: 645, loss: 7.113531112670898, acc: 0.9540161490440369, f1: 0.605, precision: 0.6159, recall: 0.5944\n",
      "2019-01-24T10:21:10.898026, step: 646, loss: 5.387140274047852, acc: 0.9593440294265747, f1: 0.5977, precision: 0.642, recall: 0.5591\n",
      "2019-01-24T10:21:11.798345, step: 647, loss: 5.241586685180664, acc: 0.9543261528015137, f1: 0.5514, precision: 0.5514, recall: 0.5514\n",
      "2019-01-24T10:21:12.634506, step: 648, loss: 4.447450637817383, acc: 0.9654635787010193, f1: 0.5933, precision: 0.6139, recall: 0.5741\n",
      "2019-01-24T10:21:13.409659, step: 649, loss: 6.914427280426025, acc: 0.9498762488365173, f1: 0.5198, precision: 0.513, recall: 0.5268\n",
      "2019-01-24T10:21:14.012705, step: 650, loss: 7.175872325897217, acc: 0.9397711753845215, f1: 0.4362, precision: 0.4457, recall: 0.4271\n",
      "2019-01-24T10:21:14.779692, step: 651, loss: 4.276708126068115, acc: 0.9707037210464478, f1: 0.5543, precision: 0.5604, recall: 0.5484\n",
      "2019-01-24T10:21:15.432854, step: 652, loss: 6.632726669311523, acc: 0.9508196711540222, f1: 0.5841, precision: 0.5946, recall: 0.5739\n",
      "start training model\n",
      "2019-01-24T10:21:16.077459, step: 653, loss: 5.699346542358398, acc: 0.9423145651817322, f1: 0.4974, precision: 0.4948, recall: 0.5\n",
      "2019-01-24T10:21:16.714518, step: 654, loss: 5.608308792114258, acc: 0.9526453018188477, f1: 0.4713, precision: 0.4607, recall: 0.4824\n",
      "2019-01-24T10:21:17.876234, step: 655, loss: 6.0770745277404785, acc: 0.9460254907608032, f1: 0.5702, precision: 0.6053, recall: 0.5391\n",
      "2019-01-24T10:21:18.395058, step: 656, loss: 3.906442165374756, acc: 0.9747676253318787, f1: 0.6154, precision: 0.6471, recall: 0.5867\n",
      "2019-01-24T10:21:18.922290, step: 657, loss: 5.310196876525879, acc: 0.9528908133506775, f1: 0.5674, precision: 0.5865, recall: 0.5495\n",
      "2019-01-24T10:21:19.653110, step: 658, loss: 3.704989433288574, acc: 0.9719917178153992, f1: 0.5839, precision: 0.5802, recall: 0.5875\n",
      "2019-01-24T10:21:20.302172, step: 659, loss: 5.006763935089111, acc: 0.9596690535545349, f1: 0.6316, precision: 0.6471, recall: 0.6168\n",
      "2019-01-24T10:21:20.979713, step: 660, loss: 4.314254283905029, acc: 0.9668635129928589, f1: 0.5943, precision: 0.5843, recall: 0.6047\n",
      "2019-01-24T10:21:21.605546, step: 661, loss: 5.3089494705200195, acc: 0.9574400782585144, f1: 0.6957, precision: 0.704, recall: 0.6875\n",
      "2019-01-24T10:21:22.446139, step: 662, loss: 5.362361907958984, acc: 0.9564015865325928, f1: 0.5903, precision: 0.5944, recall: 0.5862\n",
      "2019-01-24T10:21:22.811744, step: 663, loss: 3.708871841430664, acc: 0.9717420339584351, f1: 0.5067, precision: 0.5135, recall: 0.5\n",
      "2019-01-24T10:21:24.671963, step: 664, loss: 6.059203147888184, acc: 0.958707332611084, f1: 0.6667, precision: 0.6885, recall: 0.6462\n",
      "2019-01-24T10:21:25.370788, step: 665, loss: 3.688119888305664, acc: 0.9734096527099609, f1: 0.6434, precision: 0.6866, recall: 0.6053\n",
      "2019-01-24T10:21:26.026532, step: 666, loss: 5.319421768188477, acc: 0.9527449607849121, f1: 0.5385, precision: 0.5698, recall: 0.5104\n",
      "2019-01-24T10:21:26.884415, step: 667, loss: 5.0562357902526855, acc: 0.9559265375137329, f1: 0.4974, precision: 0.4947, recall: 0.5\n",
      "2019-01-24T10:21:27.699289, step: 668, loss: 4.660993576049805, acc: 0.9603736996650696, f1: 0.5946, precision: 0.6111, recall: 0.5789\n",
      "2019-01-24T10:21:28.381213, step: 669, loss: 4.716433525085449, acc: 0.9572463631629944, f1: 0.4458, precision: 0.4302, recall: 0.4625\n",
      "2019-01-24T10:21:28.979335, step: 670, loss: 4.4941630363464355, acc: 0.9617555141448975, f1: 0.5236, precision: 0.5, recall: 0.5495\n",
      "2019-01-24T10:21:29.690837, step: 671, loss: 4.622002124786377, acc: 0.9644399881362915, f1: 0.619, precision: 0.6075, recall: 0.6311\n",
      "2019-01-24T10:21:30.450392, step: 672, loss: 3.4475157260894775, acc: 0.96617192029953, f1: 0.5814, precision: 0.5682, recall: 0.5952\n",
      "2019-01-24T10:21:31.448400, step: 673, loss: 6.960807800292969, acc: 0.954921543598175, f1: 0.5662, precision: 0.6019, recall: 0.5345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:21:31.945908, step: 674, loss: 4.362788200378418, acc: 0.9622147679328918, f1: 0.7065, precision: 0.7065, recall: 0.7065\n",
      "2019-01-24T10:21:32.927321, step: 675, loss: 3.9362282752990723, acc: 0.9672644734382629, f1: 0.5614, precision: 0.5455, recall: 0.5783\n",
      "2019-01-24T10:21:35.446243, step: 676, loss: 6.105254650115967, acc: 0.9613654613494873, f1: 0.6854, precision: 0.6707, recall: 0.7006\n",
      "2019-01-24T10:21:36.027017, step: 677, loss: 5.220301151275635, acc: 0.9605447053909302, f1: 0.6129, precision: 0.6333, recall: 0.5938\n",
      "2019-01-24T10:21:36.929177, step: 678, loss: 4.999642372131348, acc: 0.962598443031311, f1: 0.5596, precision: 0.5684, recall: 0.551\n",
      "2019-01-24T10:21:37.722500, step: 679, loss: 3.9081218242645264, acc: 0.9668035507202148, f1: 0.6286, precision: 0.6471, recall: 0.6111\n",
      "2019-01-24T10:21:38.423933, step: 680, loss: 4.874238014221191, acc: 0.960304856300354, f1: 0.5561, precision: 0.5481, recall: 0.5644\n",
      "2019-01-24T10:21:39.472151, step: 681, loss: 6.162211894989014, acc: 0.95351642370224, f1: 0.5464, precision: 0.5889, recall: 0.5096\n",
      "2019-01-24T10:21:40.233325, step: 682, loss: 5.32539701461792, acc: 0.9591636657714844, f1: 0.6517, precision: 0.6397, recall: 0.6641\n",
      "2019-01-24T10:21:41.391123, step: 683, loss: 4.133582592010498, acc: 0.962613582611084, f1: 0.5509, precision: 0.5542, recall: 0.5476\n",
      "2019-01-24T10:21:42.482314, step: 684, loss: 5.6226348876953125, acc: 0.9589451551437378, f1: 0.6644, precision: 0.6621, recall: 0.6667\n",
      "2019-01-24T10:21:43.063187, step: 685, loss: 4.196489334106445, acc: 0.9606879353523254, f1: 0.5714, precision: 0.5946, recall: 0.55\n",
      "2019-01-24T10:21:43.986686, step: 686, loss: 4.663495063781738, acc: 0.964047908782959, f1: 0.7174, precision: 0.7174, recall: 0.7174\n",
      "2019-01-24T10:21:44.515988, step: 687, loss: 4.029014587402344, acc: 0.9603347778320312, f1: 0.5395, precision: 0.5616, recall: 0.519\n",
      "2019-01-24T10:21:45.180098, step: 688, loss: 4.485084533691406, acc: 0.9642730951309204, f1: 0.6203, precision: 0.6282, recall: 0.6125\n",
      "2019-01-24T10:21:45.828567, step: 689, loss: 4.785467147827148, acc: 0.9578729271888733, f1: 0.5758, precision: 0.57, recall: 0.5816\n",
      "2019-01-24T10:21:46.433480, step: 690, loss: 5.287755012512207, acc: 0.9578431248664856, f1: 0.6667, precision: 0.6757, recall: 0.6579\n",
      "2019-01-24T10:21:47.436457, step: 691, loss: 4.510784149169922, acc: 0.9641148447990417, f1: 0.5455, precision: 0.506, recall: 0.5915\n",
      "2019-01-24T10:21:48.065764, step: 692, loss: 4.086307048797607, acc: 0.9633233547210693, f1: 0.6433, precision: 0.6322, recall: 0.6548\n",
      "2019-01-24T10:21:48.837290, step: 693, loss: 4.388975620269775, acc: 0.9601551294326782, f1: 0.5714, precision: 0.5843, recall: 0.5591\n",
      "2019-01-24T10:21:49.360852, step: 694, loss: 5.373198509216309, acc: 0.9446775913238525, f1: 0.5, precision: 0.4951, recall: 0.505\n",
      "2019-01-24T10:21:50.210056, step: 695, loss: 3.9356448650360107, acc: 0.9671780467033386, f1: 0.6203, precision: 0.6533, recall: 0.5904\n",
      "2019-01-24T10:21:50.880934, step: 696, loss: 4.433825492858887, acc: 0.9635863900184631, f1: 0.6486, precision: 0.6429, recall: 0.6545\n",
      "2019-01-24T10:21:51.674784, step: 697, loss: 4.683070182800293, acc: 0.9533132314682007, f1: 0.5306, precision: 0.5361, recall: 0.5253\n",
      "2019-01-24T10:21:52.417663, step: 698, loss: 5.469585418701172, acc: 0.9522597193717957, f1: 0.5702, precision: 0.5537, recall: 0.5877\n",
      "2019-01-24T10:21:53.437597, step: 699, loss: 5.6049628257751465, acc: 0.9595533013343811, f1: 0.5888, precision: 0.6117, recall: 0.5676\n",
      "2019-01-24T10:21:54.769949, step: 700, loss: 4.650095462799072, acc: 0.9636363387107849, f1: 0.693, precision: 0.693, recall: 0.693\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:22:20.237809, step: 700, loss: 4.8777639799647865, acc:0.9601493759287728, f1: 0.6362055555555556, precision: 0.6597805555555558, recall: 0.6153666666666666\n",
      "\n",
      "\n",
      "2019-01-24T10:22:22.144444, step: 701, loss: 11.180224418640137, acc: 0.9166916608810425, f1: 0.5255, precision: 0.4737, recall: 0.5902\n",
      "2019-01-24T10:22:23.048198, step: 702, loss: 4.09510612487793, acc: 0.9704530239105225, f1: 0.6569, precision: 0.6979, recall: 0.6204\n",
      "2019-01-24T10:22:23.454813, step: 703, loss: 5.199415683746338, acc: 0.946110188961029, f1: 0.5333, precision: 0.5366, recall: 0.5301\n",
      "2019-01-24T10:22:24.738834, step: 704, loss: 3.9721760749816895, acc: 0.971019446849823, f1: 0.674, precision: 0.7093, recall: 0.6421\n",
      "2019-01-24T10:22:25.489390, step: 705, loss: 6.449817657470703, acc: 0.9452887773513794, f1: 0.5854, precision: 0.595, recall: 0.576\n",
      "2019-01-24T10:22:26.138531, step: 706, loss: 5.322724342346191, acc: 0.9598844647407532, f1: 0.6814, precision: 0.6875, recall: 0.6754\n",
      "2019-01-24T10:22:26.685119, step: 707, loss: 5.530267238616943, acc: 0.9475627541542053, f1: 0.5833, precision: 0.6058, recall: 0.5625\n",
      "2019-01-24T10:22:27.234978, step: 708, loss: 4.70319938659668, acc: 0.96711665391922, f1: 0.5752, precision: 0.6286, recall: 0.5301\n",
      "2019-01-24T10:22:27.872701, step: 709, loss: 4.757262706756592, acc: 0.9586592316627502, f1: 0.5033, precision: 0.5067, recall: 0.5\n",
      "2019-01-24T10:22:28.625639, step: 710, loss: 4.687923431396484, acc: 0.9634937047958374, f1: 0.6479, precision: 0.6509, recall: 0.6449\n",
      "2019-01-24T10:22:29.246666, step: 711, loss: 5.200553894042969, acc: 0.9534345865249634, f1: 0.5946, precision: 0.5946, recall: 0.5946\n",
      "2019-01-24T10:22:30.255561, step: 712, loss: 3.65891695022583, acc: 0.9744666814804077, f1: 0.6413, precision: 0.6344, recall: 0.6484\n",
      "2019-01-24T10:22:30.972415, step: 713, loss: 5.326363563537598, acc: 0.9639830589294434, f1: 0.6634, precision: 0.6667, recall: 0.6602\n",
      "2019-01-24T10:22:31.959356, step: 714, loss: 4.143443584442139, acc: 0.9686627388000488, f1: 0.5946, precision: 0.567, recall: 0.625\n",
      "2019-01-24T10:22:32.466642, step: 715, loss: 4.68576717376709, acc: 0.9542557001113892, f1: 0.538, precision: 0.575, recall: 0.5055\n",
      "2019-01-24T10:22:33.434015, step: 716, loss: 3.886232852935791, acc: 0.967169463634491, f1: 0.6629, precision: 0.6556, recall: 0.6705\n",
      "2019-01-24T10:22:34.418623, step: 717, loss: 4.182229995727539, acc: 0.9725856781005859, f1: 0.699, precision: 0.72, recall: 0.6792\n",
      "2019-01-24T10:22:34.854507, step: 718, loss: 4.9254045486450195, acc: 0.9644507765769958, f1: 0.6393, precision: 0.6542, recall: 0.625\n",
      "2019-01-24T10:22:35.867069, step: 719, loss: 3.355678081512451, acc: 0.9779834151268005, f1: 0.6826, precision: 0.6951, recall: 0.6706\n",
      "2019-01-24T10:22:36.524466, step: 720, loss: 6.202895164489746, acc: 0.9521468281745911, f1: 0.5069, precision: 0.5238, recall: 0.4911\n",
      "2019-01-24T10:22:37.364853, step: 721, loss: 7.001182556152344, acc: 0.9543668627738953, f1: 0.6254, precision: 0.6067, recall: 0.6454\n",
      "2019-01-24T10:22:38.011788, step: 722, loss: 4.375802516937256, acc: 0.9699222445487976, f1: 0.6269, precision: 0.6364, recall: 0.6176\n",
      "2019-01-24T10:22:38.855239, step: 723, loss: 5.120449542999268, acc: 0.9614325165748596, f1: 0.4607, precision: 0.4607, recall: 0.4607\n",
      "2019-01-24T10:22:39.355763, step: 724, loss: 5.390982151031494, acc: 0.9552067518234253, f1: 0.6341, precision: 0.6311, recall: 0.6373\n",
      "2019-01-24T10:22:41.040839, step: 725, loss: 4.801587104797363, acc: 0.9627836346626282, f1: 0.5628, precision: 0.5372, recall: 0.5909\n",
      "2019-01-24T10:22:41.588319, step: 726, loss: 4.476273536682129, acc: 0.9637491703033447, f1: 0.5577, precision: 0.5421, recall: 0.5743\n",
      "2019-01-24T10:22:42.397893, step: 727, loss: 6.077439308166504, acc: 0.9478205442428589, f1: 0.5133, precision: 0.4957, recall: 0.5321\n",
      "2019-01-24T10:22:43.101064, step: 728, loss: 5.288185119628906, acc: 0.9554161429405212, f1: 0.6532, precision: 0.6532, recall: 0.6532\n",
      "2019-01-24T10:22:43.634826, step: 729, loss: 4.362552165985107, acc: 0.9627816081047058, f1: 0.5521, precision: 0.5422, recall: 0.5625\n",
      "2019-01-24T10:22:44.459266, step: 730, loss: 5.398096084594727, acc: 0.9531662464141846, f1: 0.5341, precision: 0.5341, recall: 0.5341\n",
      "2019-01-24T10:22:45.251395, step: 731, loss: 4.538104057312012, acc: 0.9667385220527649, f1: 0.6842, precision: 0.7222, recall: 0.65\n",
      "2019-01-24T10:22:45.749811, step: 732, loss: 4.383759498596191, acc: 0.963004469871521, f1: 0.6226, precision: 0.6408, recall: 0.6055\n",
      "2019-01-24T10:22:46.293909, step: 733, loss: 4.070409297943115, acc: 0.9619952440261841, f1: 0.6607, precision: 0.6379, recall: 0.6852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:22:47.178367, step: 734, loss: 5.260274887084961, acc: 0.960667073726654, f1: 0.5111, precision: 0.5287, recall: 0.4946\n",
      "2019-01-24T10:22:47.770844, step: 735, loss: 5.410822868347168, acc: 0.9624682068824768, f1: 0.6432, precision: 0.6737, recall: 0.6154\n",
      "2019-01-24T10:22:48.588597, step: 736, loss: 5.515463352203369, acc: 0.9529529809951782, f1: 0.5121, precision: 0.53, recall: 0.4953\n",
      "2019-01-24T10:22:49.306908, step: 737, loss: 4.343118667602539, acc: 0.9660113453865051, f1: 0.5625, precision: 0.5696, recall: 0.5556\n",
      "2019-01-24T10:22:49.932054, step: 738, loss: 6.628147125244141, acc: 0.9465500712394714, f1: 0.6239, precision: 0.6348, recall: 0.6134\n",
      "2019-01-24T10:22:50.536952, step: 739, loss: 5.2799811363220215, acc: 0.9652090668678284, f1: 0.6559, precision: 0.7093, recall: 0.61\n",
      "2019-01-24T10:22:50.923743, step: 740, loss: 3.675049304962158, acc: 0.9647374749183655, f1: 0.593, precision: 0.5604, recall: 0.6296\n",
      "2019-01-24T10:22:51.800921, step: 741, loss: 4.701016902923584, acc: 0.962277352809906, f1: 0.5497, precision: 0.5402, recall: 0.5595\n",
      "2019-01-24T10:22:52.345785, step: 742, loss: 6.100276470184326, acc: 0.9491685032844543, f1: 0.5551, precision: 0.5354, recall: 0.5763\n",
      "2019-01-24T10:22:53.584185, step: 743, loss: 6.87778377532959, acc: 0.9459811449050903, f1: 0.5377, precision: 0.5481, recall: 0.5278\n",
      "2019-01-24T10:22:54.294826, step: 744, loss: 4.615148067474365, acc: 0.9613207578659058, f1: 0.5743, precision: 0.5686, recall: 0.58\n",
      "2019-01-24T10:22:54.892302, step: 745, loss: 4.316054344177246, acc: 0.9657331109046936, f1: 0.6215, precision: 0.6322, recall: 0.6111\n",
      "2019-01-24T10:22:55.855234, step: 746, loss: 5.5650715827941895, acc: 0.9580819010734558, f1: 0.4615, precision: 0.4588, recall: 0.4643\n",
      "2019-01-24T10:22:56.451243, step: 747, loss: 4.330068588256836, acc: 0.9665122628211975, f1: 0.7, precision: 0.7071, recall: 0.6931\n",
      "2019-01-24T10:22:57.203862, step: 748, loss: 5.296324253082275, acc: 0.9553423523902893, f1: 0.6435, precision: 0.6789, recall: 0.6116\n",
      "2019-01-24T10:22:57.973827, step: 749, loss: 5.053767681121826, acc: 0.9600396156311035, f1: 0.6256, precision: 0.6289, recall: 0.6224\n",
      "2019-01-24T10:22:58.803957, step: 750, loss: 2.567115306854248, acc: 0.9794910550117493, f1: 0.6861, precision: 0.6912, recall: 0.6812\n",
      "2019-01-24T10:22:59.493879, step: 751, loss: 4.739340782165527, acc: 0.9617540240287781, f1: 0.598, precision: 0.6162, recall: 0.581\n",
      "2019-01-24T10:23:00.027863, step: 752, loss: 4.633530139923096, acc: 0.9542936086654663, f1: 0.5029, precision: 0.5, recall: 0.5059\n",
      "2019-01-24T10:23:00.906746, step: 753, loss: 3.059055805206299, acc: 0.9778012633323669, f1: 0.6905, precision: 0.7073, recall: 0.6744\n",
      "2019-01-24T10:23:01.572213, step: 754, loss: 3.493344783782959, acc: 0.961330235004425, f1: 0.5241, precision: 0.481, recall: 0.5758\n",
      "2019-01-24T10:23:02.092368, step: 755, loss: 3.7802419662475586, acc: 0.9595505595207214, f1: 0.6699, precision: 0.6667, recall: 0.6731\n",
      "2019-01-24T10:23:02.611313, step: 756, loss: 6.05858039855957, acc: 0.9526298642158508, f1: 0.625, precision: 0.6303, recall: 0.6198\n",
      "2019-01-24T10:23:03.370925, step: 757, loss: 4.539580821990967, acc: 0.9636170864105225, f1: 0.5556, precision: 0.6154, recall: 0.5063\n",
      "2019-01-24T10:23:04.015797, step: 758, loss: 4.731597900390625, acc: 0.9610729217529297, f1: 0.5967, precision: 0.6279, recall: 0.5684\n",
      "2019-01-24T10:23:04.882282, step: 759, loss: 7.056913375854492, acc: 0.9343159198760986, f1: 0.56, precision: 0.6034, recall: 0.5224\n",
      "2019-01-24T10:23:05.675769, step: 760, loss: 4.932243347167969, acc: 0.9552335143089294, f1: 0.5294, precision: 0.5696, recall: 0.4945\n",
      "2019-01-24T10:23:06.198673, step: 761, loss: 4.749031066894531, acc: 0.9585654735565186, f1: 0.538, precision: 0.5476, recall: 0.5287\n",
      "2019-01-24T10:23:06.822124, step: 762, loss: 4.564325332641602, acc: 0.9576719403266907, f1: 0.5402, precision: 0.5222, recall: 0.5595\n",
      "2019-01-24T10:23:07.262763, step: 763, loss: 6.6143341064453125, acc: 0.9447550177574158, f1: 0.4933, precision: 0.5046, recall: 0.4825\n",
      "2019-01-24T10:23:07.929547, step: 764, loss: 5.496932029724121, acc: 0.9522672891616821, f1: 0.5241, precision: 0.5104, recall: 0.5385\n",
      "2019-01-24T10:23:08.848596, step: 765, loss: 3.8268842697143555, acc: 0.9705304503440857, f1: 0.6279, precision: 0.6136, recall: 0.6429\n",
      "2019-01-24T10:23:09.895083, step: 766, loss: 3.198639392852783, acc: 0.9675157070159912, f1: 0.6887, precision: 0.65, recall: 0.7324\n",
      "2019-01-24T10:23:10.533740, step: 767, loss: 6.269203186035156, acc: 0.94880211353302, f1: 0.5321, precision: 0.5577, recall: 0.5088\n",
      "2019-01-24T10:23:11.432694, step: 768, loss: 5.056314468383789, acc: 0.9589712023735046, f1: 0.6812, precision: 0.7027, recall: 0.661\n",
      "2019-01-24T10:23:11.966777, step: 769, loss: 6.8022050857543945, acc: 0.9464231133460999, f1: 0.5, precision: 0.4914, recall: 0.5089\n",
      "2019-01-24T10:23:12.846154, step: 770, loss: 4.492969512939453, acc: 0.9566763639450073, f1: 0.4211, precision: 0.4337, recall: 0.4091\n",
      "2019-01-24T10:23:13.346498, step: 771, loss: 4.2850189208984375, acc: 0.9659452438354492, f1: 0.5943, precision: 0.6047, recall: 0.5843\n",
      "2019-01-24T10:23:13.918389, step: 772, loss: 6.332281112670898, acc: 0.9479002356529236, f1: 0.5202, precision: 0.5625, recall: 0.4839\n",
      "2019-01-24T10:23:15.055250, step: 773, loss: 6.261514186859131, acc: 0.9531609416007996, f1: 0.5993, precision: 0.6015, recall: 0.597\n",
      "2019-01-24T10:23:15.794702, step: 774, loss: 5.9681396484375, acc: 0.9510117173194885, f1: 0.4746, precision: 0.5185, recall: 0.4375\n",
      "2019-01-24T10:23:16.310209, step: 775, loss: 4.104337692260742, acc: 0.9626436829566956, f1: 0.5442, precision: 0.5634, recall: 0.5263\n",
      "2019-01-24T10:23:16.833471, step: 776, loss: 3.641200304031372, acc: 0.9676887392997742, f1: 0.5938, precision: 0.6, recall: 0.5876\n",
      "2019-01-24T10:23:17.372957, step: 777, loss: 3.2506160736083984, acc: 0.9711574912071228, f1: 0.6164, precision: 0.625, recall: 0.6081\n",
      "2019-01-24T10:23:19.267547, step: 778, loss: 4.244081974029541, acc: 0.9661442041397095, f1: 0.5556, precision: 0.5682, recall: 0.5435\n",
      "2019-01-24T10:23:19.776524, step: 779, loss: 3.3636789321899414, acc: 0.9753347635269165, f1: 0.7122, precision: 0.7228, recall: 0.7019\n",
      "2019-01-24T10:23:20.525351, step: 780, loss: 7.0620269775390625, acc: 0.9545326828956604, f1: 0.5651, precision: 0.5714, recall: 0.5588\n",
      "2019-01-24T10:23:21.076224, step: 781, loss: 4.652737617492676, acc: 0.9568480253219604, f1: 0.6561, precision: 0.6596, recall: 0.6526\n",
      "2019-01-24T10:23:21.941764, step: 782, loss: 4.6344804763793945, acc: 0.9614630937576294, f1: 0.5325, precision: 0.4891, recall: 0.5844\n",
      "2019-01-24T10:23:22.284368, step: 783, loss: 3.541599750518799, acc: 0.96856290102005, f1: 0.7273, precision: 0.7547, recall: 0.7018\n",
      "2019-01-24T10:23:23.408301, step: 784, loss: 4.578824043273926, acc: 0.9592118859291077, f1: 0.6202, precision: 0.625, recall: 0.6154\n",
      "2019-01-24T10:23:24.129396, step: 785, loss: 4.358208656311035, acc: 0.9610602259635925, f1: 0.5894, precision: 0.5922, recall: 0.5865\n",
      "2019-01-24T10:23:25.168117, step: 786, loss: 5.639044284820557, acc: 0.9527005553245544, f1: 0.5685, precision: 0.5773, recall: 0.56\n",
      "2019-01-24T10:23:25.939269, step: 787, loss: 3.234675407409668, acc: 0.9745025038719177, f1: 0.5345, precision: 0.62, recall: 0.4697\n",
      "2019-01-24T10:23:26.762787, step: 788, loss: 4.557570457458496, acc: 0.9662958383560181, f1: 0.7182, precision: 0.7248, recall: 0.7117\n",
      "2019-01-24T10:23:27.389973, step: 789, loss: 4.228767395019531, acc: 0.9634608626365662, f1: 0.4966, precision: 0.4932, recall: 0.5\n",
      "2019-01-24T10:23:27.923097, step: 790, loss: 4.005242347717285, acc: 0.961376428604126, f1: 0.6494, precision: 0.6098, recall: 0.6944\n",
      "2019-01-24T10:23:28.475352, step: 791, loss: 5.359200954437256, acc: 0.9577603340148926, f1: 0.6459, precision: 0.6484, recall: 0.6434\n",
      "2019-01-24T10:23:29.157857, step: 792, loss: 5.166160583496094, acc: 0.9639415740966797, f1: 0.6732, precision: 0.69, recall: 0.6571\n",
      "2019-01-24T10:23:30.153389, step: 793, loss: 5.787688255310059, acc: 0.956337571144104, f1: 0.6364, precision: 0.6604, recall: 0.614\n",
      "2019-01-24T10:23:30.803768, step: 794, loss: 5.990269660949707, acc: 0.9529590010643005, f1: 0.49, precision: 0.4851, recall: 0.4949\n",
      "2019-01-24T10:23:31.721665, step: 795, loss: 4.718189239501953, acc: 0.9597135186195374, f1: 0.5326, precision: 0.5444, recall: 0.5213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:23:32.279333, step: 796, loss: 3.590794086456299, acc: 0.9723486304283142, f1: 0.6788, precision: 0.6914, recall: 0.6667\n",
      "2019-01-24T10:23:34.432947, step: 797, loss: 4.440337181091309, acc: 0.970596969127655, f1: 0.6634, precision: 0.6768, recall: 0.6505\n",
      "2019-01-24T10:23:35.404667, step: 798, loss: 5.207918643951416, acc: 0.959744393825531, f1: 0.6573, precision: 0.6542, recall: 0.6604\n",
      "2019-01-24T10:23:36.482789, step: 799, loss: 4.669976234436035, acc: 0.9613924026489258, f1: 0.5914, precision: 0.5851, recall: 0.5978\n",
      "2019-01-24T10:23:37.354542, step: 800, loss: 4.735496997833252, acc: 0.9565911293029785, f1: 0.5877, precision: 0.5982, recall: 0.5776\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:24:02.208461, step: 800, loss: 4.701364199320476, acc:0.9615892453326119, f1: 0.655588888888889, precision: 0.7118499999999999, recall: 0.6089055555555558\n",
      "\n",
      "\n",
      "2019-01-24T10:24:03.185098, step: 801, loss: 4.890924453735352, acc: 0.952507495880127, f1: 0.5276, precision: 0.589, recall: 0.4778\n",
      "2019-01-24T10:24:03.733809, step: 802, loss: 2.2723793983459473, acc: 0.9797248244285583, f1: 0.7867, precision: 0.8429, recall: 0.7375\n",
      "2019-01-24T10:24:04.318334, step: 803, loss: 3.0792713165283203, acc: 0.9762405753135681, f1: 0.5915, precision: 0.6364, recall: 0.5526\n",
      "2019-01-24T10:24:04.865967, step: 804, loss: 4.491545677185059, acc: 0.9616135358810425, f1: 0.5143, precision: 0.5056, recall: 0.5233\n",
      "2019-01-24T10:24:05.363867, step: 805, loss: 4.58758020401001, acc: 0.9597457647323608, f1: 0.5395, precision: 0.5125, recall: 0.5694\n",
      "2019-01-24T10:24:06.222328, step: 806, loss: 6.230508327484131, acc: 0.9541680216789246, f1: 0.5846, precision: 0.6064, recall: 0.5644\n",
      "2019-01-24T10:24:06.952721, step: 807, loss: 4.010120391845703, acc: 0.9621440768241882, f1: 0.5574, precision: 0.5543, recall: 0.5604\n",
      "2019-01-24T10:24:07.761339, step: 808, loss: 4.408656120300293, acc: 0.9647021889686584, f1: 0.5953, precision: 0.5926, recall: 0.5981\n",
      "2019-01-24T10:24:08.390945, step: 809, loss: 5.393166542053223, acc: 0.9576351642608643, f1: 0.7162, precision: 0.7593, recall: 0.6777\n",
      "2019-01-24T10:24:08.996810, step: 810, loss: 4.7311625480651855, acc: 0.9575908780097961, f1: 0.5775, precision: 0.5567, recall: 0.6\n",
      "2019-01-24T10:24:09.739912, step: 811, loss: 3.6708984375, acc: 0.9701727032661438, f1: 0.6526, precision: 0.6739, recall: 0.6327\n",
      "2019-01-24T10:24:10.270066, step: 812, loss: 3.164078712463379, acc: 0.9686184525489807, f1: 0.6845, precision: 0.6882, recall: 0.6809\n",
      "2019-01-24T10:24:10.820055, step: 813, loss: 4.847565174102783, acc: 0.9604100584983826, f1: 0.6728, precision: 0.6759, recall: 0.6697\n",
      "2019-01-24T10:24:11.355650, step: 814, loss: 3.231013774871826, acc: 0.9719662070274353, f1: 0.5769, precision: 0.5488, recall: 0.6081\n",
      "2019-01-24T10:24:11.951883, step: 815, loss: 3.990246295928955, acc: 0.9621365666389465, f1: 0.5217, precision: 0.4932, recall: 0.5538\n",
      "2019-01-24T10:24:12.678199, step: 816, loss: 5.19027042388916, acc: 0.9532374143600464, f1: 0.4862, precision: 0.5116, recall: 0.4632\n",
      "2019-01-24T10:24:13.259975, step: 817, loss: 3.7722129821777344, acc: 0.9671949744224548, f1: 0.6207, precision: 0.6364, recall: 0.6058\n",
      "2019-01-24T10:24:14.195989, step: 818, loss: 5.5679802894592285, acc: 0.9532520174980164, f1: 0.5758, precision: 0.6196, recall: 0.5377\n",
      "2019-01-24T10:24:15.681641, step: 819, loss: 3.406569242477417, acc: 0.9738250970840454, f1: 0.7, precision: 0.6848, recall: 0.7159\n",
      "2019-01-24T10:24:16.350952, step: 820, loss: 5.279282569885254, acc: 0.9477749466896057, f1: 0.5973, precision: 0.6226, recall: 0.5739\n",
      "2019-01-24T10:24:16.960089, step: 821, loss: 3.652576446533203, acc: 0.9708579182624817, f1: 0.6484, precision: 0.6413, recall: 0.6556\n",
      "2019-01-24T10:24:17.941834, step: 822, loss: 4.166599273681641, acc: 0.9618266224861145, f1: 0.5605, precision: 0.6027, recall: 0.5238\n",
      "2019-01-24T10:24:18.579297, step: 823, loss: 4.982678413391113, acc: 0.9527910947799683, f1: 0.6262, precision: 0.6321, recall: 0.6204\n",
      "2019-01-24T10:24:19.482260, step: 824, loss: 3.2981314659118652, acc: 0.9753825664520264, f1: 0.6742, precision: 0.6818, recall: 0.6667\n",
      "2019-01-24T10:24:19.974514, step: 825, loss: 4.733011722564697, acc: 0.9405131936073303, f1: 0.5054, precision: 0.4845, recall: 0.5281\n",
      "2019-01-24T10:24:20.815764, step: 826, loss: 3.6068623065948486, acc: 0.9721939563751221, f1: 0.6774, precision: 0.6923, recall: 0.6632\n",
      "2019-01-24T10:24:21.322182, step: 827, loss: 6.32023811340332, acc: 0.9468845725059509, f1: 0.5897, precision: 0.633, recall: 0.552\n",
      "2019-01-24T10:24:22.304659, step: 828, loss: 4.33967924118042, acc: 0.9640660285949707, f1: 0.6075, precision: 0.6075, recall: 0.6075\n",
      "2019-01-24T10:24:23.058965, step: 829, loss: 3.8978023529052734, acc: 0.9664204120635986, f1: 0.6826, precision: 0.6628, recall: 0.7037\n",
      "2019-01-24T10:24:23.591999, step: 830, loss: 4.52805757522583, acc: 0.962435245513916, f1: 0.6, precision: 0.6122, recall: 0.5882\n",
      "2019-01-24T10:24:24.133548, step: 831, loss: 4.291418552398682, acc: 0.9646791815757751, f1: 0.5914, precision: 0.5789, recall: 0.6044\n",
      "2019-01-24T10:24:24.862252, step: 832, loss: 4.735454559326172, acc: 0.9595896601676941, f1: 0.687, precision: 0.687, recall: 0.687\n",
      "2019-01-24T10:24:25.604348, step: 833, loss: 4.987043380737305, acc: 0.9581772685050964, f1: 0.6294, precision: 0.6458, recall: 0.6139\n",
      "2019-01-24T10:24:26.180600, step: 834, loss: 4.843137741088867, acc: 0.950138509273529, f1: 0.6256, precision: 0.6455, recall: 0.6068\n",
      "2019-01-24T10:24:28.879446, step: 835, loss: 5.885705471038818, acc: 0.9583218097686768, f1: 0.6711, precision: 0.6892, recall: 0.6538\n",
      "2019-01-24T10:24:29.423335, step: 836, loss: 4.887493133544922, acc: 0.9556494355201721, f1: 0.6509, precision: 0.6635, recall: 0.6389\n",
      "2019-01-24T10:24:30.095242, step: 837, loss: 4.82415246963501, acc: 0.9585320949554443, f1: 0.534, precision: 0.51, recall: 0.5604\n",
      "2019-01-24T10:24:30.794169, step: 838, loss: 5.339118003845215, acc: 0.958063542842865, f1: 0.6518, precision: 0.6518, recall: 0.6518\n",
      "2019-01-24T10:24:32.126740, step: 839, loss: 4.469542503356934, acc: 0.9680564999580383, f1: 0.6111, precision: 0.6044, recall: 0.618\n",
      "2019-01-24T10:24:34.778861, step: 840, loss: 5.629270076751709, acc: 0.9536109566688538, f1: 0.6219, precision: 0.6241, recall: 0.6197\n",
      "2019-01-24T10:24:35.641515, step: 841, loss: 4.538023948669434, acc: 0.9638897776603699, f1: 0.5628, precision: 0.5957, recall: 0.5333\n",
      "2019-01-24T10:24:36.070056, step: 842, loss: 3.3877172470092773, acc: 0.9681302905082703, f1: 0.5891, precision: 0.5205, recall: 0.6786\n",
      "2019-01-24T10:24:36.827665, step: 843, loss: 4.030008316040039, acc: 0.9584237337112427, f1: 0.5543, precision: 0.5258, recall: 0.5862\n",
      "2019-01-24T10:24:37.783710, step: 844, loss: 5.65424919128418, acc: 0.9501767754554749, f1: 0.6454, precision: 0.6894, recall: 0.6067\n",
      "2019-01-24T10:24:38.393796, step: 845, loss: 4.592610836029053, acc: 0.9606197476387024, f1: 0.685, precision: 0.7073, recall: 0.6641\n",
      "2019-01-24T10:24:38.957051, step: 846, loss: 4.293205738067627, acc: 0.9550296068191528, f1: 0.6405, precision: 0.6712, recall: 0.6125\n",
      "2019-01-24T10:24:39.586560, step: 847, loss: 4.785317420959473, acc: 0.9557117819786072, f1: 0.6193, precision: 0.6289, recall: 0.61\n",
      "2019-01-24T10:24:42.050655, step: 848, loss: 4.3792901039123535, acc: 0.9703989624977112, f1: 0.7601, precision: 0.7663, recall: 0.754\n",
      "2019-01-24T10:24:42.573787, step: 849, loss: 4.172728538513184, acc: 0.9580621719360352, f1: 0.6076, precision: 0.6857, recall: 0.5455\n",
      "2019-01-24T10:24:43.849437, step: 850, loss: 6.4340081214904785, acc: 0.9478021860122681, f1: 0.552, precision: 0.5169, recall: 0.5922\n",
      "2019-01-24T10:24:44.528578, step: 851, loss: 4.379909038543701, acc: 0.9588810205459595, f1: 0.5371, precision: 0.5402, recall: 0.5341\n",
      "2019-01-24T10:24:45.159702, step: 852, loss: 4.643559455871582, acc: 0.9529272317886353, f1: 0.6082, precision: 0.6211, recall: 0.596\n",
      "2019-01-24T10:24:45.817899, step: 853, loss: 3.443899631500244, acc: 0.9714385271072388, f1: 0.5417, precision: 0.527, recall: 0.5571\n",
      "2019-01-24T10:24:46.495394, step: 854, loss: 3.8767857551574707, acc: 0.9641434550285339, f1: 0.6629, precision: 0.6484, recall: 0.6782\n",
      "2019-01-24T10:24:47.118688, step: 855, loss: 4.094743728637695, acc: 0.9634099006652832, f1: 0.5765, precision: 0.5765, recall: 0.5765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:24:47.961239, step: 856, loss: 3.33127498626709, acc: 0.9729480743408203, f1: 0.6272, precision: 0.6625, recall: 0.5955\n",
      "2019-01-24T10:24:48.434788, step: 857, loss: 4.060461044311523, acc: 0.962449848651886, f1: 0.6275, precision: 0.64, recall: 0.6154\n",
      "2019-01-24T10:24:48.956463, step: 858, loss: 4.354087829589844, acc: 0.9537976980209351, f1: 0.6304, precision: 0.6667, recall: 0.5979\n",
      "2019-01-24T10:24:49.758932, step: 859, loss: 3.8995614051818848, acc: 0.9656084775924683, f1: 0.5263, precision: 0.5385, recall: 0.5147\n",
      "2019-01-24T10:24:50.533307, step: 860, loss: 5.737706661224365, acc: 0.9595823287963867, f1: 0.5955, precision: 0.6463, recall: 0.5521\n",
      "2019-01-24T10:24:51.340944, step: 861, loss: 4.939159393310547, acc: 0.9618425369262695, f1: 0.6605, precision: 0.6893, recall: 0.6339\n",
      "2019-01-24T10:24:52.003288, step: 862, loss: 4.291646480560303, acc: 0.961002767086029, f1: 0.6544, precision: 0.6514, recall: 0.6574\n",
      "2019-01-24T10:24:52.863938, step: 863, loss: 4.906106948852539, acc: 0.9589675664901733, f1: 0.6505, precision: 0.6442, recall: 0.6569\n",
      "2019-01-24T10:24:53.434138, step: 864, loss: 4.5229034423828125, acc: 0.9546384811401367, f1: 0.5385, precision: 0.5158, recall: 0.5632\n",
      "2019-01-24T10:24:53.842816, step: 865, loss: 4.025965690612793, acc: 0.9608132839202881, f1: 0.625, precision: 0.618, recall: 0.6322\n",
      "2019-01-24T10:24:54.651312, step: 866, loss: 5.4715118408203125, acc: 0.9480304718017578, f1: 0.6296, precision: 0.6538, recall: 0.6071\n",
      "2019-01-24T10:24:55.190766, step: 867, loss: 3.4618611335754395, acc: 0.9656946659088135, f1: 0.6734, precision: 0.67, recall: 0.6768\n",
      "2019-01-24T10:24:55.826225, step: 868, loss: 4.098147869110107, acc: 0.9649122953414917, f1: 0.6132, precision: 0.6311, recall: 0.5963\n",
      "2019-01-24T10:24:56.454281, step: 869, loss: 2.891112804412842, acc: 0.9765406250953674, f1: 0.687, precision: 0.6818, recall: 0.6923\n",
      "2019-01-24T10:24:57.122740, step: 870, loss: 4.904168128967285, acc: 0.9574099779129028, f1: 0.614, precision: 0.6111, recall: 0.6168\n",
      "2019-01-24T10:24:57.719808, step: 871, loss: 5.310450077056885, acc: 0.9572175145149231, f1: 0.6667, precision: 0.7128, recall: 0.6262\n",
      "2019-01-24T10:24:58.325661, step: 872, loss: 3.7298758029937744, acc: 0.9689011573791504, f1: 0.5902, precision: 0.6545, recall: 0.5373\n",
      "2019-01-24T10:24:58.820666, step: 873, loss: 3.7889535427093506, acc: 0.9583187103271484, f1: 0.6359, precision: 0.6327, recall: 0.6392\n",
      "2019-01-24T10:24:59.370444, step: 874, loss: 4.649539947509766, acc: 0.9634057879447937, f1: 0.6698, precision: 0.6893, recall: 0.6514\n",
      "2019-01-24T10:24:59.950550, step: 875, loss: 5.045193195343018, acc: 0.9635782837867737, f1: 0.6374, precision: 0.6591, recall: 0.617\n",
      "2019-01-24T10:25:00.755872, step: 876, loss: 3.2470388412475586, acc: 0.967130720615387, f1: 0.6667, precision: 0.7241, recall: 0.6176\n",
      "2019-01-24T10:25:01.382650, step: 877, loss: 4.198505401611328, acc: 0.9708426594734192, f1: 0.7263, precision: 0.7647, recall: 0.6915\n",
      "2019-01-24T10:25:01.926476, step: 878, loss: 3.9267942905426025, acc: 0.9624505639076233, f1: 0.6593, precision: 0.6742, recall: 0.6452\n",
      "2019-01-24T10:25:02.820514, step: 879, loss: 4.167357921600342, acc: 0.9649068117141724, f1: 0.6174, precision: 0.6479, recall: 0.5897\n",
      "2019-01-24T10:25:03.627557, step: 880, loss: 4.233752250671387, acc: 0.9633473753929138, f1: 0.6203, precision: 0.6042, recall: 0.6374\n",
      "2019-01-24T10:25:04.395098, step: 881, loss: 4.572550296783447, acc: 0.9726575613021851, f1: 0.6301, precision: 0.6133, recall: 0.6479\n",
      "2019-01-24T10:25:05.028300, step: 882, loss: 3.106271266937256, acc: 0.9698440432548523, f1: 0.6968, precision: 0.6667, recall: 0.7297\n",
      "2019-01-24T10:25:05.747281, step: 883, loss: 4.542813777923584, acc: 0.9541440010070801, f1: 0.5784, precision: 0.5673, recall: 0.59\n",
      "2019-01-24T10:25:06.248318, step: 884, loss: 3.7582058906555176, acc: 0.9658291339874268, f1: 0.6528, precision: 0.6238, recall: 0.6848\n",
      "2019-01-24T10:25:06.781730, step: 885, loss: 3.2699408531188965, acc: 0.9689629077911377, f1: 0.641, precision: 0.6667, recall: 0.6173\n",
      "2019-01-24T10:25:07.504591, step: 886, loss: 4.883925914764404, acc: 0.9578911066055298, f1: 0.6034, precision: 0.6067, recall: 0.6\n",
      "2019-01-24T10:25:08.012478, step: 887, loss: 4.302119731903076, acc: 0.9538160562515259, f1: 0.5578, precision: 0.6212, recall: 0.5062\n",
      "2019-01-24T10:25:09.061889, step: 888, loss: 8.075608253479004, acc: 0.9196726083755493, f1: 0.6, precision: 0.648, recall: 0.5586\n",
      "2019-01-24T10:25:11.627622, step: 889, loss: 3.710540771484375, acc: 0.9726390242576599, f1: 0.7609, precision: 0.7664, recall: 0.7554\n",
      "2019-01-24T10:25:12.336673, step: 890, loss: 3.177635669708252, acc: 0.9705492854118347, f1: 0.6, precision: 0.5735, recall: 0.629\n",
      "2019-01-24T10:25:13.246384, step: 891, loss: 4.027863502502441, acc: 0.967432975769043, f1: 0.6632, precision: 0.6632, recall: 0.6632\n",
      "2019-01-24T10:25:13.813162, step: 892, loss: 3.1453065872192383, acc: 0.9704450368881226, f1: 0.6968, precision: 0.675, recall: 0.72\n",
      "2019-01-24T10:25:14.576168, step: 893, loss: 4.136629581451416, acc: 0.9706982374191284, f1: 0.6354, precision: 0.6354, recall: 0.6354\n",
      "2019-01-24T10:25:15.141341, step: 894, loss: 3.8049182891845703, acc: 0.9594857692718506, f1: 0.5068, precision: 0.5068, recall: 0.5068\n",
      "2019-01-24T10:25:15.901444, step: 895, loss: 4.670422077178955, acc: 0.95611172914505, f1: 0.6432, precision: 0.6598, recall: 0.6275\n",
      "2019-01-24T10:25:16.803556, step: 896, loss: 4.49888277053833, acc: 0.9596485495567322, f1: 0.5059, precision: 0.5443, recall: 0.4725\n",
      "2019-01-24T10:25:17.458843, step: 897, loss: 6.405200004577637, acc: 0.9477832317352295, f1: 0.625, precision: 0.6796, recall: 0.5785\n",
      "2019-01-24T10:25:18.818116, step: 898, loss: 4.562990188598633, acc: 0.9572978615760803, f1: 0.6256, precision: 0.6289, recall: 0.6224\n",
      "2019-01-24T10:25:19.484520, step: 899, loss: 3.959275245666504, acc: 0.967183530330658, f1: 0.732, precision: 0.7474, recall: 0.7172\n",
      "2019-01-24T10:25:19.968819, step: 900, loss: 4.0063676834106445, acc: 0.9684684872627258, f1: 0.6049, precision: 0.6282, recall: 0.5833\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:25:44.021478, step: 900, loss: 4.27847742372089, acc:0.9637039850155512, f1: 0.6850944444444446, precision: 0.7012611111111111, recall: 0.6703722222222221\n",
      "\n",
      "\n",
      "2019-01-24T10:25:44.936293, step: 901, loss: 3.880743980407715, acc: 0.9690189361572266, f1: 0.6626, precision: 0.7013, recall: 0.6279\n",
      "2019-01-24T10:25:45.571891, step: 902, loss: 4.16788387298584, acc: 0.958949089050293, f1: 0.6061, precision: 0.5682, recall: 0.6494\n",
      "2019-01-24T10:25:46.060593, step: 903, loss: 4.057934284210205, acc: 0.9677197933197021, f1: 0.6344, precision: 0.6146, recall: 0.6556\n",
      "2019-01-24T10:25:46.688843, step: 904, loss: 3.9312119483947754, acc: 0.9686038494110107, f1: 0.6634, precision: 0.68, recall: 0.6476\n",
      "2019-01-24T10:25:47.220148, step: 905, loss: 3.7864859104156494, acc: 0.9667226672172546, f1: 0.6378, precision: 0.6629, recall: 0.6146\n",
      "2019-01-24T10:25:47.937453, step: 906, loss: 3.7314157485961914, acc: 0.9665480256080627, f1: 0.7179, precision: 0.7, recall: 0.7368\n",
      "2019-01-24T10:25:48.560058, step: 907, loss: 3.5213100910186768, acc: 0.9732620120048523, f1: 0.7251, precision: 0.7126, recall: 0.7381\n",
      "2019-01-24T10:25:49.397765, step: 908, loss: 3.7747108936309814, acc: 0.9626038670539856, f1: 0.6711, precision: 0.7083, recall: 0.6375\n",
      "2019-01-24T10:25:49.794929, step: 909, loss: 3.868572950363159, acc: 0.9631103277206421, f1: 0.5139, precision: 0.5362, recall: 0.4933\n",
      "2019-01-24T10:25:50.498373, step: 910, loss: 5.310024261474609, acc: 0.9572849869728088, f1: 0.6341, precision: 0.6903, recall: 0.5865\n",
      "2019-01-24T10:25:51.162079, step: 911, loss: 3.217205047607422, acc: 0.9709981083869934, f1: 0.5954, precision: 0.6094, recall: 0.5821\n",
      "2019-01-24T10:25:51.793544, step: 912, loss: 4.261106491088867, acc: 0.9626138806343079, f1: 0.5824, precision: 0.631, recall: 0.5408\n",
      "2019-01-24T10:25:52.275527, step: 913, loss: 4.06189489364624, acc: 0.9674876928329468, f1: 0.6474, precision: 0.6588, recall: 0.6364\n",
      "2019-01-24T10:25:53.034229, step: 914, loss: 4.155134201049805, acc: 0.9649780988693237, f1: 0.5604, precision: 0.6, recall: 0.5258\n",
      "2019-01-24T10:25:53.605095, step: 915, loss: 5.233912467956543, acc: 0.9492302536964417, f1: 0.5143, precision: 0.5094, recall: 0.5192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:25:54.181986, step: 916, loss: 5.11197566986084, acc: 0.9574738144874573, f1: 0.608, precision: 0.608, recall: 0.608\n",
      "2019-01-24T10:25:55.082111, step: 917, loss: 3.8109278678894043, acc: 0.9702523946762085, f1: 0.5578, precision: 0.5694, recall: 0.5467\n",
      "2019-01-24T10:25:55.624999, step: 918, loss: 3.2403507232666016, acc: 0.9752832055091858, f1: 0.7326, precision: 0.7412, recall: 0.7241\n",
      "2019-01-24T10:25:56.202621, step: 919, loss: 5.136133193969727, acc: 0.9566108584403992, f1: 0.6239, precision: 0.6186, recall: 0.6293\n",
      "2019-01-24T10:25:57.082317, step: 920, loss: 3.128620147705078, acc: 0.969043493270874, f1: 0.5946, precision: 0.6027, recall: 0.5867\n",
      "2019-01-24T10:25:58.127286, step: 921, loss: 4.29399299621582, acc: 0.9678475856781006, f1: 0.7273, precision: 0.7273, recall: 0.7273\n",
      "2019-01-24T10:25:58.627714, step: 922, loss: 3.6419050693511963, acc: 0.9690313339233398, f1: 0.6601, precision: 0.6569, recall: 0.6634\n",
      "2019-01-24T10:25:59.933333, step: 923, loss: 5.062896728515625, acc: 0.9529209733009338, f1: 0.6095, precision: 0.5818, recall: 0.64\n",
      "2019-01-24T10:26:00.474693, step: 924, loss: 4.911509990692139, acc: 0.9559664130210876, f1: 0.7105, precision: 0.6983, recall: 0.7232\n",
      "2019-01-24T10:26:01.037394, step: 925, loss: 4.626738548278809, acc: 0.9665757417678833, f1: 0.7005, precision: 0.69, recall: 0.7113\n",
      "2019-01-24T10:26:02.030103, step: 926, loss: 3.5276942253112793, acc: 0.9647015929222107, f1: 0.6626, precision: 0.6923, recall: 0.6353\n",
      "2019-01-24T10:26:02.607869, step: 927, loss: 4.286182880401611, acc: 0.9633370637893677, f1: 0.663, precision: 0.6593, recall: 0.6667\n",
      "2019-01-24T10:26:03.741545, step: 928, loss: 5.303044319152832, acc: 0.9515803456306458, f1: 0.6298, precision: 0.7037, recall: 0.57\n",
      "2019-01-24T10:26:04.822586, step: 929, loss: 4.8827056884765625, acc: 0.9620130062103271, f1: 0.6593, precision: 0.6667, recall: 0.6522\n",
      "2019-01-24T10:26:05.453414, step: 930, loss: 3.664553165435791, acc: 0.9673202633857727, f1: 0.6869, precision: 0.6939, recall: 0.68\n",
      "2019-01-24T10:26:06.374958, step: 931, loss: 4.7363200187683105, acc: 0.9669499397277832, f1: 0.7255, precision: 0.7327, recall: 0.7184\n",
      "2019-01-24T10:26:06.974503, step: 932, loss: 3.0918827056884766, acc: 0.9711369276046753, f1: 0.6667, precision: 0.6552, recall: 0.6786\n",
      "2019-01-24T10:26:07.886794, step: 933, loss: 3.7686209678649902, acc: 0.965965986251831, f1: 0.648, precision: 0.6444, recall: 0.6517\n",
      "2019-01-24T10:26:08.440467, step: 934, loss: 4.16791296005249, acc: 0.9608725905418396, f1: 0.6698, precision: 0.703, recall: 0.6396\n",
      "2019-01-24T10:26:09.279359, step: 935, loss: 3.184776782989502, acc: 0.9726190567016602, f1: 0.72, precision: 0.7297, recall: 0.7105\n",
      "2019-01-24T10:26:09.736656, step: 936, loss: 4.067707061767578, acc: 0.9641996622085571, f1: 0.6235, precision: 0.6625, recall: 0.5889\n",
      "2019-01-24T10:26:10.465514, step: 937, loss: 5.581038475036621, acc: 0.9519139528274536, f1: 0.5128, precision: 0.5051, recall: 0.5208\n",
      "2019-01-24T10:26:11.818605, step: 938, loss: 2.771392822265625, acc: 0.9804384708404541, f1: 0.7578, precision: 0.7531, recall: 0.7625\n",
      "2019-01-24T10:26:14.309978, step: 939, loss: 4.513854503631592, acc: 0.9592435359954834, f1: 0.72, precision: 0.7174, recall: 0.7226\n",
      "2019-01-24T10:26:16.889237, step: 940, loss: 6.065609931945801, acc: 0.9616045951843262, f1: 0.7138, precision: 0.7537, recall: 0.6779\n",
      "2019-01-24T10:26:17.733729, step: 941, loss: 3.180025577545166, acc: 0.968886137008667, f1: 0.7136, precision: 0.7451, recall: 0.6847\n",
      "2019-01-24T10:26:20.130117, step: 942, loss: 4.228752613067627, acc: 0.9685448408126831, f1: 0.6804, precision: 0.6781, recall: 0.6828\n",
      "2019-01-24T10:26:21.117904, step: 943, loss: 3.36350679397583, acc: 0.9690789580345154, f1: 0.6235, precision: 0.6974, recall: 0.5638\n",
      "2019-01-24T10:26:22.185815, step: 944, loss: 4.093262195587158, acc: 0.9675132632255554, f1: 0.6243, precision: 0.6344, recall: 0.6146\n",
      "2019-01-24T10:26:23.357995, step: 945, loss: 3.9623095989227295, acc: 0.9727626442909241, f1: 0.6878, precision: 0.7143, recall: 0.6633\n",
      "2019-01-24T10:26:23.881873, step: 946, loss: 3.4897637367248535, acc: 0.9667483568191528, f1: 0.6585, precision: 0.6667, recall: 0.6506\n",
      "2019-01-24T10:26:24.808772, step: 947, loss: 4.961555480957031, acc: 0.9646302461624146, f1: 0.6566, precision: 0.6444, recall: 0.6692\n",
      "2019-01-24T10:26:25.463053, step: 948, loss: 4.04437255859375, acc: 0.963839590549469, f1: 0.6633, precision: 0.6875, recall: 0.6408\n",
      "2019-01-24T10:26:25.995456, step: 949, loss: 5.686409950256348, acc: 0.9518154263496399, f1: 0.65, precision: 0.6842, recall: 0.619\n",
      "2019-01-24T10:26:26.535692, step: 950, loss: 5.337554931640625, acc: 0.9568276405334473, f1: 0.6186, precision: 0.6522, recall: 0.5882\n",
      "2019-01-24T10:26:27.401787, step: 951, loss: 4.469877243041992, acc: 0.9623960256576538, f1: 0.5733, precision: 0.5733, recall: 0.5733\n",
      "2019-01-24T10:26:28.090273, step: 952, loss: 4.962836742401123, acc: 0.9538826942443848, f1: 0.6862, precision: 0.6777, recall: 0.6949\n",
      "2019-01-24T10:26:28.681634, step: 953, loss: 3.409240245819092, acc: 0.9701541662216187, f1: 0.6914, precision: 0.7089, recall: 0.6747\n",
      "2019-01-24T10:26:29.305920, step: 954, loss: 4.0747761726379395, acc: 0.9606151580810547, f1: 0.641, precision: 0.6494, recall: 0.6329\n",
      "2019-01-24T10:26:30.445663, step: 955, loss: 5.74606466293335, acc: 0.9580819010734558, f1: 0.6878, precision: 0.7379, recall: 0.6441\n",
      "2019-01-24T10:26:31.312850, step: 956, loss: 4.0571184158325195, acc: 0.9585422873497009, f1: 0.6738, precision: 0.6848, recall: 0.6632\n",
      "2019-01-24T10:26:32.004572, step: 957, loss: 5.2315216064453125, acc: 0.9614234566688538, f1: 0.6103, precision: 0.6436, recall: 0.5804\n",
      "2019-01-24T10:26:32.647332, step: 958, loss: 4.122552871704102, acc: 0.9602912068367004, f1: 0.6364, precision: 0.6311, recall: 0.6417\n",
      "2019-01-24T10:26:33.483544, step: 959, loss: 4.263980865478516, acc: 0.9612272381782532, f1: 0.6279, precision: 0.6667, recall: 0.5934\n",
      "2019-01-24T10:26:33.992541, step: 960, loss: 4.827913284301758, acc: 0.9499664306640625, f1: 0.6186, precision: 0.6404, recall: 0.5984\n",
      "2019-01-24T10:26:34.552036, step: 961, loss: 3.7451839447021484, acc: 0.968616247177124, f1: 0.663, precision: 0.6289, recall: 0.7011\n",
      "2019-01-24T10:26:35.288012, step: 962, loss: 4.9027791023254395, acc: 0.9599999785423279, f1: 0.596, precision: 0.5842, recall: 0.6082\n",
      "2019-01-24T10:26:35.881951, step: 963, loss: 3.249520778656006, acc: 0.971794843673706, f1: 0.703, precision: 0.6988, recall: 0.7073\n",
      "2019-01-24T10:26:36.613163, step: 964, loss: 4.145513534545898, acc: 0.9664191007614136, f1: 0.6154, precision: 0.6437, recall: 0.5895\n",
      "2019-01-24T10:26:37.217714, step: 965, loss: 3.8182215690612793, acc: 0.9690956473350525, f1: 0.6697, precision: 0.6789, recall: 0.6607\n",
      "2019-01-24T10:26:37.952983, step: 966, loss: 3.691174030303955, acc: 0.9626467227935791, f1: 0.67, precision: 0.6979, recall: 0.6442\n",
      "2019-01-24T10:26:40.510159, step: 967, loss: 4.901397228240967, acc: 0.9641268849372864, f1: 0.7653, precision: 0.797, recall: 0.7361\n",
      "2019-01-24T10:26:41.173061, step: 968, loss: 6.624637603759766, acc: 0.9414141178131104, f1: 0.524, precision: 0.5405, recall: 0.5085\n",
      "2019-01-24T10:26:42.246942, step: 969, loss: 3.9394993782043457, acc: 0.9632107019424438, f1: 0.663, precision: 0.6593, recall: 0.6667\n",
      "2019-01-24T10:26:43.024200, step: 970, loss: 3.3635756969451904, acc: 0.9781269431114197, f1: 0.7902, precision: 0.7941, recall: 0.7864\n",
      "2019-01-24T10:26:43.742312, step: 971, loss: 4.419210433959961, acc: 0.9591412544250488, f1: 0.6226, precision: 0.6111, recall: 0.6346\n",
      "2019-01-24T10:26:44.256854, step: 972, loss: 3.297544002532959, acc: 0.9739493131637573, f1: 0.7093, precision: 0.6932, recall: 0.7262\n",
      "2019-01-24T10:26:44.985064, step: 973, loss: 3.5864715576171875, acc: 0.9727398157119751, f1: 0.7122, precision: 0.7228, recall: 0.7019\n",
      "2019-01-24T10:26:45.512604, step: 974, loss: 4.019350528717041, acc: 0.9615097045898438, f1: 0.7119, precision: 0.7119, recall: 0.7119\n",
      "2019-01-24T10:26:47.861648, step: 975, loss: 4.115012168884277, acc: 0.9725947380065918, f1: 0.7647, precision: 0.7879, recall: 0.7429\n",
      "2019-01-24T10:26:48.676942, step: 976, loss: 4.382665634155273, acc: 0.9624456763267517, f1: 0.6695, precision: 0.6724, recall: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:26:49.282722, step: 977, loss: 3.025611400604248, acc: 0.969342052936554, f1: 0.7097, precision: 0.713, recall: 0.7064\n",
      "2019-01-24T10:26:50.133782, step: 978, loss: 4.120806694030762, acc: 0.9677306413650513, f1: 0.5952, precision: 0.5952, recall: 0.5952\n",
      "start training model\n",
      "2019-01-24T10:26:50.723058, step: 979, loss: 3.399163246154785, acc: 0.9649059176445007, f1: 0.6743, precision: 0.6629, recall: 0.686\n",
      "2019-01-24T10:26:51.395696, step: 980, loss: 2.903690814971924, acc: 0.972991943359375, f1: 0.7225, precision: 0.7188, recall: 0.7263\n",
      "2019-01-24T10:26:52.091284, step: 981, loss: 3.070199489593506, acc: 0.9723404049873352, f1: 0.672, precision: 0.6774, recall: 0.6667\n",
      "2019-01-24T10:26:54.708380, step: 982, loss: 3.047454833984375, acc: 0.9791010618209839, f1: 0.7857, precision: 0.7971, recall: 0.7746\n",
      "2019-01-24T10:26:55.515453, step: 983, loss: 3.1366114616394043, acc: 0.9734429121017456, f1: 0.75, precision: 0.7841, recall: 0.7188\n",
      "2019-01-24T10:26:56.056037, step: 984, loss: 4.381615161895752, acc: 0.9604352116584778, f1: 0.6502, precision: 0.6695, recall: 0.632\n",
      "2019-01-24T10:26:56.902451, step: 985, loss: 3.4311814308166504, acc: 0.9676781296730042, f1: 0.6809, precision: 0.6882, recall: 0.6737\n",
      "2019-01-24T10:26:57.516545, step: 986, loss: 2.70570707321167, acc: 0.9787778854370117, f1: 0.6795, precision: 0.7067, recall: 0.6543\n",
      "2019-01-24T10:26:58.287796, step: 987, loss: 3.2638912200927734, acc: 0.9796366095542908, f1: 0.8041, precision: 0.8125, recall: 0.7959\n",
      "2019-01-24T10:26:58.822436, step: 988, loss: 4.781392574310303, acc: 0.9605348706245422, f1: 0.6512, precision: 0.6667, recall: 0.6364\n",
      "2019-01-24T10:26:59.386972, step: 989, loss: 3.5393271446228027, acc: 0.9680111408233643, f1: 0.7565, precision: 0.7838, recall: 0.7311\n",
      "2019-01-24T10:27:00.444191, step: 990, loss: 4.53209114074707, acc: 0.9608115553855896, f1: 0.6489, precision: 0.6822, recall: 0.6186\n",
      "2019-01-24T10:27:01.339274, step: 991, loss: 2.825212001800537, acc: 0.9785162806510925, f1: 0.75, precision: 0.7416, recall: 0.7586\n",
      "2019-01-24T10:27:02.243581, step: 992, loss: 3.7905192375183105, acc: 0.9729577302932739, f1: 0.7154, precision: 0.7458, recall: 0.6875\n",
      "2019-01-24T10:27:02.945710, step: 993, loss: 3.408370018005371, acc: 0.9704684615135193, f1: 0.6763, precision: 0.6796, recall: 0.6731\n",
      "2019-01-24T10:27:03.959338, step: 994, loss: 4.173161029815674, acc: 0.9654516577720642, f1: 0.6729, precision: 0.6792, recall: 0.6667\n",
      "2019-01-24T10:27:04.722369, step: 995, loss: 4.245493412017822, acc: 0.9613152742385864, f1: 0.6327, precision: 0.6392, recall: 0.6263\n",
      "2019-01-24T10:27:07.120510, step: 996, loss: 2.996338129043579, acc: 0.9723131656646729, f1: 0.7167, precision: 0.7227, recall: 0.7107\n",
      "2019-01-24T10:27:07.684369, step: 997, loss: 2.658682107925415, acc: 0.972733736038208, f1: 0.6909, precision: 0.6867, recall: 0.6951\n",
      "2019-01-24T10:27:08.153858, step: 998, loss: 2.708631992340088, acc: 0.9744567275047302, f1: 0.6739, precision: 0.7126, recall: 0.6392\n",
      "2019-01-24T10:27:08.617804, step: 999, loss: 2.946267604827881, acc: 0.9740785956382751, f1: 0.7571, precision: 0.7882, recall: 0.7283\n",
      "2019-01-24T10:27:09.126916, step: 1000, loss: 2.434464931488037, acc: 0.9710916876792908, f1: 0.6265, precision: 0.6047, recall: 0.65\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:27:34.438563, step: 1000, loss: 4.013615601592594, acc:0.9663868231905831, f1: 0.7020944444444441, precision: 0.7294277777777779, recall: 0.6772444444444443\n",
      "\n",
      "\n",
      "2019-01-24T10:27:35.296572, step: 1001, loss: 3.2105801105499268, acc: 0.971238911151886, f1: 0.6623, precision: 0.6538, recall: 0.6711\n",
      "2019-01-24T10:27:36.016913, step: 1002, loss: 3.4436519145965576, acc: 0.9728792309761047, f1: 0.7215, precision: 0.693, recall: 0.7524\n",
      "2019-01-24T10:27:36.754467, step: 1003, loss: 3.798717498779297, acc: 0.9700034856796265, f1: 0.7123, precision: 0.7647, recall: 0.6667\n",
      "2019-01-24T10:27:37.599980, step: 1004, loss: 3.5924863815307617, acc: 0.9749582409858704, f1: 0.699, precision: 0.7059, recall: 0.6923\n",
      "2019-01-24T10:27:38.028013, step: 1005, loss: 3.5367445945739746, acc: 0.9702048301696777, f1: 0.7164, precision: 0.7129, recall: 0.72\n",
      "2019-01-24T10:27:38.754205, step: 1006, loss: 2.8331480026245117, acc: 0.9781805872917175, f1: 0.7215, precision: 0.75, recall: 0.6951\n",
      "2019-01-24T10:27:39.401250, step: 1007, loss: 4.48749303817749, acc: 0.9525211453437805, f1: 0.6231, precision: 0.6263, recall: 0.62\n",
      "2019-01-24T10:27:40.056139, step: 1008, loss: 3.000713348388672, acc: 0.9756483435630798, f1: 0.7184, precision: 0.7255, recall: 0.7115\n",
      "2019-01-24T10:27:40.827643, step: 1009, loss: 3.8658599853515625, acc: 0.9690293669700623, f1: 0.6933, precision: 0.7091, recall: 0.6783\n",
      "2019-01-24T10:27:41.408535, step: 1010, loss: 3.2571306228637695, acc: 0.9714693427085876, f1: 0.6154, precision: 0.6076, recall: 0.6234\n",
      "2019-01-24T10:27:42.055476, step: 1011, loss: 2.732339382171631, acc: 0.9759759902954102, f1: 0.6736, precision: 0.6701, recall: 0.6771\n",
      "2019-01-24T10:27:42.584428, step: 1012, loss: 4.104068756103516, acc: 0.9612992405891418, f1: 0.6564, precision: 0.6957, recall: 0.6214\n",
      "2019-01-24T10:27:43.285786, step: 1013, loss: 2.8869919776916504, acc: 0.9777934551239014, f1: 0.7528, precision: 0.7791, recall: 0.7283\n",
      "2019-01-24T10:27:43.776349, step: 1014, loss: 2.924461841583252, acc: 0.9789285659790039, f1: 0.7534, precision: 0.7857, recall: 0.7237\n",
      "2019-01-24T10:27:44.252426, step: 1015, loss: 2.831477403640747, acc: 0.972834050655365, f1: 0.6093, precision: 0.6216, recall: 0.5974\n",
      "2019-01-24T10:27:44.935830, step: 1016, loss: 4.3835554122924805, acc: 0.9633440375328064, f1: 0.6216, precision: 0.6449, recall: 0.6\n",
      "2019-01-24T10:27:45.656723, step: 1017, loss: 2.7798376083374023, acc: 0.9768134951591492, f1: 0.7262, precision: 0.7439, recall: 0.7093\n",
      "2019-01-24T10:27:46.176571, step: 1018, loss: 6.477613925933838, acc: 0.9565359354019165, f1: 0.6429, precision: 0.6702, recall: 0.6176\n",
      "2019-01-24T10:27:47.003136, step: 1019, loss: 3.851116180419922, acc: 0.9748506546020508, f1: 0.7742, precision: 0.7925, recall: 0.7568\n",
      "2019-01-24T10:27:47.535226, step: 1020, loss: 3.7993452548980713, acc: 0.9653179049491882, f1: 0.6667, precision: 0.6813, recall: 0.6526\n",
      "2019-01-24T10:27:48.534569, step: 1021, loss: 3.487527370452881, acc: 0.9730501174926758, f1: 0.7477, precision: 0.7143, recall: 0.7843\n",
      "2019-01-24T10:27:49.058872, step: 1022, loss: 3.696998357772827, acc: 0.9597934484481812, f1: 0.6335, precision: 0.6456, recall: 0.622\n",
      "2019-01-24T10:27:50.463288, step: 1023, loss: 3.5744800567626953, acc: 0.9740051627159119, f1: 0.496, precision: 0.5, recall: 0.4921\n",
      "2019-01-24T10:27:50.982678, step: 1024, loss: 3.3646481037139893, acc: 0.9706745147705078, f1: 0.732, precision: 0.732, recall: 0.732\n",
      "2019-01-24T10:27:53.384383, step: 1025, loss: 3.5461764335632324, acc: 0.9761766195297241, f1: 0.7732, precision: 0.7591, recall: 0.7879\n",
      "2019-01-24T10:27:54.135051, step: 1026, loss: 4.019964218139648, acc: 0.9646302461624146, f1: 0.6374, precision: 0.6374, recall: 0.6374\n",
      "2019-01-24T10:27:55.138787, step: 1027, loss: 3.6804466247558594, acc: 0.9650092124938965, f1: 0.6338, precision: 0.6923, recall: 0.5844\n",
      "2019-01-24T10:27:55.860032, step: 1028, loss: 4.025228977203369, acc: 0.9678061604499817, f1: 0.6631, precision: 0.6813, recall: 0.6458\n",
      "2019-01-24T10:27:56.587798, step: 1029, loss: 3.825529098510742, acc: 0.9678675532341003, f1: 0.7047, precision: 0.7473, recall: 0.6667\n",
      "2019-01-24T10:27:57.063246, step: 1030, loss: 3.539846420288086, acc: 0.9672350287437439, f1: 0.5909, precision: 0.5591, recall: 0.6265\n",
      "2019-01-24T10:27:57.790347, step: 1031, loss: 3.535346746444702, acc: 0.9639355540275574, f1: 0.6635, precision: 0.6542, recall: 0.6731\n",
      "2019-01-24T10:27:58.290344, step: 1032, loss: 3.6887004375457764, acc: 0.9665728211402893, f1: 0.6842, precision: 0.6915, recall: 0.6771\n",
      "2019-01-24T10:27:59.056957, step: 1033, loss: 3.5121583938598633, acc: 0.9720670580863953, f1: 0.74, precision: 0.7475, recall: 0.7327\n",
      "2019-01-24T10:27:59.768748, step: 1034, loss: 3.7356362342834473, acc: 0.9663865566253662, f1: 0.6554, precision: 0.6824, recall: 0.6304\n",
      "2019-01-24T10:28:00.233308, step: 1035, loss: 4.221293926239014, acc: 0.9555160403251648, f1: 0.6328, precision: 0.6747, recall: 0.5957\n",
      "2019-01-24T10:28:00.527824, step: 1036, loss: 3.056367874145508, acc: 0.9753173589706421, f1: 0.6927, precision: 0.7294, recall: 0.6596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:28:01.114079, step: 1037, loss: 3.354461193084717, acc: 0.9718446731567383, f1: 0.7053, precision: 0.7128, recall: 0.6979\n",
      "2019-01-24T10:28:01.961543, step: 1038, loss: 4.040732383728027, acc: 0.9633874297142029, f1: 0.681, precision: 0.6583, recall: 0.7054\n",
      "2019-01-24T10:28:02.523532, step: 1039, loss: 3.6766819953918457, acc: 0.9614512324333191, f1: 0.67, precision: 0.701, recall: 0.6415\n",
      "2019-01-24T10:28:03.058931, step: 1040, loss: 4.103634834289551, acc: 0.9640902876853943, f1: 0.6635, precision: 0.6509, recall: 0.6765\n",
      "2019-01-24T10:28:04.273548, step: 1041, loss: 4.766238689422607, acc: 0.9675262570381165, f1: 0.7259, precision: 0.746, recall: 0.7068\n",
      "2019-01-24T10:28:05.001981, step: 1042, loss: 3.3314170837402344, acc: 0.9725984334945679, f1: 0.7424, precision: 0.7522, recall: 0.7328\n",
      "2019-01-24T10:28:05.966198, step: 1043, loss: 2.8502697944641113, acc: 0.9695518016815186, f1: 0.6145, precision: 0.6145, recall: 0.6145\n",
      "2019-01-24T10:28:06.501047, step: 1044, loss: 3.468904495239258, acc: 0.9670406579971313, f1: 0.5, precision: 0.4923, recall: 0.5079\n",
      "2019-01-24T10:28:07.209782, step: 1045, loss: 3.3440282344818115, acc: 0.9736930727958679, f1: 0.7052, precision: 0.7439, recall: 0.6703\n",
      "2019-01-24T10:28:07.849934, step: 1046, loss: 3.762697696685791, acc: 0.967775821685791, f1: 0.6933, precision: 0.729, recall: 0.661\n",
      "2019-01-24T10:28:08.661749, step: 1047, loss: 2.930907726287842, acc: 0.976047933101654, f1: 0.76, precision: 0.7661, recall: 0.754\n",
      "2019-01-24T10:28:10.983291, step: 1048, loss: 3.9900436401367188, acc: 0.9713187217712402, f1: 0.7191, precision: 0.75, recall: 0.6906\n",
      "2019-01-24T10:28:11.583815, step: 1049, loss: 2.937467098236084, acc: 0.9680116176605225, f1: 0.6522, precision: 0.6667, recall: 0.6383\n",
      "2019-01-24T10:28:12.215332, step: 1050, loss: 4.281208038330078, acc: 0.9577889442443848, f1: 0.6417, precision: 0.6525, recall: 0.6311\n",
      "2019-01-24T10:28:12.724113, step: 1051, loss: 2.616215467453003, acc: 0.9735649824142456, f1: 0.7826, precision: 0.8, recall: 0.766\n",
      "2019-01-24T10:28:13.483682, step: 1052, loss: 3.7620792388916016, acc: 0.9699097275733948, f1: 0.6885, precision: 0.7, recall: 0.6774\n",
      "2019-01-24T10:28:14.499035, step: 1053, loss: 2.4190526008605957, acc: 0.976372480392456, f1: 0.6708, precision: 0.7013, recall: 0.6429\n",
      "2019-01-24T10:28:14.834128, step: 1054, loss: 5.238746166229248, acc: 0.9473869800567627, f1: 0.6311, precision: 0.6283, recall: 0.6339\n",
      "2019-01-24T10:28:15.556703, step: 1055, loss: 3.110750675201416, acc: 0.9767202734947205, f1: 0.7215, precision: 0.7308, recall: 0.7125\n",
      "2019-01-24T10:28:16.235769, step: 1056, loss: 4.2111358642578125, acc: 0.9590566754341125, f1: 0.6186, precision: 0.5984, recall: 0.6404\n",
      "2019-01-24T10:28:16.752834, step: 1057, loss: 3.4036240577697754, acc: 0.9616397023200989, f1: 0.6832, precision: 0.6627, recall: 0.7051\n",
      "2019-01-24T10:28:17.374804, step: 1058, loss: 2.859830379486084, acc: 0.9772727489471436, f1: 0.731, precision: 0.75, recall: 0.7129\n",
      "2019-01-24T10:28:18.513661, step: 1059, loss: 4.735201358795166, acc: 0.9549638628959656, f1: 0.5907, precision: 0.6404, recall: 0.5481\n",
      "2019-01-24T10:28:19.204328, step: 1060, loss: 4.704521179199219, acc: 0.9557764530181885, f1: 0.593, precision: 0.6556, recall: 0.5413\n",
      "2019-01-24T10:28:19.961354, step: 1061, loss: 3.0174436569213867, acc: 0.9732909202575684, f1: 0.703, precision: 0.6893, recall: 0.7172\n",
      "2019-01-24T10:28:21.179261, step: 1062, loss: 3.021881580352783, acc: 0.9728643298149109, f1: 0.7701, precision: 0.7614, recall: 0.7791\n",
      "2019-01-24T10:28:22.747749, step: 1063, loss: 3.351889133453369, acc: 0.9685670137405396, f1: 0.6049, precision: 0.6447, recall: 0.5698\n",
      "2019-01-24T10:28:23.244086, step: 1064, loss: 5.831018924713135, acc: 0.9429206848144531, f1: 0.5274, precision: 0.5408, recall: 0.5146\n",
      "2019-01-24T10:28:23.741563, step: 1065, loss: 2.462754726409912, acc: 0.9726516008377075, f1: 0.75, precision: 0.7317, recall: 0.7692\n",
      "2019-01-24T10:28:24.291106, step: 1066, loss: 4.419655799865723, acc: 0.9671826362609863, f1: 0.6986, precision: 0.7087, recall: 0.6887\n",
      "2019-01-24T10:28:24.785958, step: 1067, loss: 3.9418294429779053, acc: 0.9584579467773438, f1: 0.6948, precision: 0.6981, recall: 0.6916\n",
      "2019-01-24T10:28:25.391419, step: 1068, loss: 1.6747355461120605, acc: 0.9857802987098694, f1: 0.7571, precision: 0.7465, recall: 0.7681\n",
      "2019-01-24T10:28:26.217819, step: 1069, loss: 3.1851963996887207, acc: 0.9782747626304626, f1: 0.7204, precision: 0.6907, recall: 0.7528\n",
      "2019-01-24T10:28:27.056791, step: 1070, loss: 2.912707805633545, acc: 0.9727177619934082, f1: 0.6267, precision: 0.6104, recall: 0.6438\n",
      "2019-01-24T10:28:27.697818, step: 1071, loss: 3.852376699447632, acc: 0.9629761576652527, f1: 0.6244, precision: 0.6154, recall: 0.6337\n",
      "2019-01-24T10:28:28.777143, step: 1072, loss: 2.7315545082092285, acc: 0.9716120362281799, f1: 0.6027, precision: 0.6197, recall: 0.5867\n",
      "2019-01-24T10:28:29.306209, step: 1073, loss: 3.853078842163086, acc: 0.9641434550285339, f1: 0.6442, precision: 0.6569, recall: 0.6321\n",
      "2019-01-24T10:28:29.918646, step: 1074, loss: 3.2024030685424805, acc: 0.9693058729171753, f1: 0.6467, precision: 0.6835, recall: 0.6136\n",
      "2019-01-24T10:28:31.163514, step: 1075, loss: 4.342325210571289, acc: 0.9669266939163208, f1: 0.6667, precision: 0.6893, recall: 0.6455\n",
      "2019-01-24T10:28:31.604869, step: 1076, loss: 5.04106330871582, acc: 0.9596412777900696, f1: 0.6667, precision: 0.6847, recall: 0.6496\n",
      "2019-01-24T10:28:32.526844, step: 1077, loss: 5.288305282592773, acc: 0.9488739967346191, f1: 0.6449, precision: 0.6642, recall: 0.6268\n",
      "2019-01-24T10:28:33.279290, step: 1078, loss: 4.085415363311768, acc: 0.9658725261688232, f1: 0.6905, precision: 0.696, recall: 0.685\n",
      "2019-01-24T10:28:33.871548, step: 1079, loss: 2.5431251525878906, acc: 0.9725073575973511, f1: 0.7353, precision: 0.7426, recall: 0.7282\n",
      "2019-01-24T10:28:34.531205, step: 1080, loss: 4.070681571960449, acc: 0.9665540456771851, f1: 0.6703, precision: 0.6526, recall: 0.6889\n",
      "2019-01-24T10:28:35.538113, step: 1081, loss: 3.3008077144622803, acc: 0.9703243374824524, f1: 0.663, precision: 0.6383, recall: 0.6897\n",
      "2019-01-24T10:28:36.118029, step: 1082, loss: 3.207976818084717, acc: 0.970695972442627, f1: 0.68, precision: 0.6623, recall: 0.6986\n",
      "2019-01-24T10:28:36.858191, step: 1083, loss: 4.094511032104492, acc: 0.9604203104972839, f1: 0.6597, precision: 0.6632, recall: 0.6562\n",
      "2019-01-24T10:28:37.353113, step: 1084, loss: 3.352886438369751, acc: 0.9679487347602844, f1: 0.6228, precision: 0.6341, recall: 0.6118\n",
      "2019-01-24T10:28:38.463329, step: 1085, loss: 4.2667622566223145, acc: 0.9564375877380371, f1: 0.5604, precision: 0.5743, recall: 0.5472\n",
      "2019-01-24T10:28:39.041792, step: 1086, loss: 2.7956924438476562, acc: 0.9640092849731445, f1: 0.7126, precision: 0.7654, recall: 0.6667\n",
      "2019-01-24T10:28:39.571516, step: 1087, loss: 4.9142231941223145, acc: 0.9572890996932983, f1: 0.6449, precision: 0.6583, recall: 0.632\n",
      "2019-01-24T10:28:40.078357, step: 1088, loss: 2.4912657737731934, acc: 0.9799270033836365, f1: 0.6777, precision: 0.7069, recall: 0.6508\n",
      "2019-01-24T10:28:40.892192, step: 1089, loss: 2.5325751304626465, acc: 0.9797340035438538, f1: 0.7516, precision: 0.7867, recall: 0.7195\n",
      "2019-01-24T10:28:41.312080, step: 1090, loss: 4.011541366577148, acc: 0.9652174115180969, f1: 0.6971, precision: 0.7119, recall: 0.6829\n",
      "2019-01-24T10:28:41.768675, step: 1091, loss: 2.96144962310791, acc: 0.9740308523178101, f1: 0.6829, precision: 0.7089, recall: 0.6588\n",
      "2019-01-24T10:28:42.573427, step: 1092, loss: 2.99995756149292, acc: 0.977250874042511, f1: 0.7086, precision: 0.6889, recall: 0.7294\n",
      "2019-01-24T10:28:43.525162, step: 1093, loss: 4.160887718200684, acc: 0.9608497619628906, f1: 0.6146, precision: 0.6082, recall: 0.6211\n",
      "2019-01-24T10:28:44.771945, step: 1094, loss: 4.656918525695801, acc: 0.9623968005180359, f1: 0.661, precision: 0.6842, recall: 0.6393\n",
      "2019-01-24T10:28:45.301745, step: 1095, loss: 2.6726579666137695, acc: 0.9763663411140442, f1: 0.7134, precision: 0.7089, recall: 0.7179\n",
      "2019-01-24T10:28:46.163355, step: 1096, loss: 2.977219581604004, acc: 0.9751665592193604, f1: 0.7389, precision: 0.7732, recall: 0.7075\n",
      "2019-01-24T10:28:48.714338, step: 1097, loss: 3.5515449047088623, acc: 0.9775086641311646, f1: 0.7871, precision: 0.8033, recall: 0.7717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:28:49.606379, step: 1098, loss: 2.4726409912109375, acc: 0.9756540656089783, f1: 0.6316, precision: 0.6667, recall: 0.6\n",
      "2019-01-24T10:28:50.192210, step: 1099, loss: 3.1077098846435547, acc: 0.9718586206436157, f1: 0.7107, precision: 0.7, recall: 0.7216\n",
      "2019-01-24T10:28:50.702961, step: 1100, loss: 2.3111371994018555, acc: 0.970325231552124, f1: 0.6364, precision: 0.6269, recall: 0.6462\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:29:16.395006, step: 1100, loss: 3.805624763170878, acc:0.9672094964318805, f1: 0.7175916666666668, precision: 0.7525305555555555, recall: 0.686413888888889\n",
      "\n",
      "\n",
      "2019-01-24T10:29:17.115253, step: 1101, loss: 3.40057110786438, acc: 0.9703543186187744, f1: 0.7126, precision: 0.7654, recall: 0.6667\n",
      "2019-01-24T10:29:17.794971, step: 1102, loss: 4.019933223724365, acc: 0.9627922177314758, f1: 0.5882, precision: 0.625, recall: 0.5556\n",
      "2019-01-24T10:29:18.576782, step: 1103, loss: 2.6291024684906006, acc: 0.9738517999649048, f1: 0.7303, precision: 0.7738, recall: 0.6915\n",
      "2019-01-24T10:29:19.313882, step: 1104, loss: 3.1396093368530273, acc: 0.9735355377197266, f1: 0.6875, precision: 0.7097, recall: 0.6667\n",
      "2019-01-24T10:29:20.100704, step: 1105, loss: 5.241531848907471, acc: 0.9559729099273682, f1: 0.6, precision: 0.6148, recall: 0.5859\n",
      "2019-01-24T10:29:20.552532, step: 1106, loss: 2.8701467514038086, acc: 0.9697999358177185, f1: 0.7324, precision: 0.7222, recall: 0.7429\n",
      "2019-01-24T10:29:21.262374, step: 1107, loss: 4.381354331970215, acc: 0.9643211364746094, f1: 0.6667, precision: 0.6476, recall: 0.6869\n",
      "2019-01-24T10:29:23.855793, step: 1108, loss: 3.7530503273010254, acc: 0.9731468558311462, f1: 0.8232, precision: 0.8438, recall: 0.8036\n",
      "2019-01-24T10:29:24.534790, step: 1109, loss: 4.541440963745117, acc: 0.9647519588470459, f1: 0.7126, precision: 0.7154, recall: 0.7097\n",
      "2019-01-24T10:29:25.216603, step: 1110, loss: 3.205679416656494, acc: 0.9674418568611145, f1: 0.6316, precision: 0.6067, recall: 0.6585\n",
      "2019-01-24T10:29:26.174136, step: 1111, loss: 4.7578959465026855, acc: 0.9474708437919617, f1: 0.6278, precision: 0.6364, recall: 0.6195\n",
      "2019-01-24T10:29:26.892361, step: 1112, loss: 3.2658510208129883, acc: 0.9631789922714233, f1: 0.6634, precision: 0.6837, recall: 0.6442\n",
      "2019-01-24T10:29:27.664619, step: 1113, loss: 3.353853702545166, acc: 0.9636697173118591, f1: 0.6364, precision: 0.6512, recall: 0.6222\n",
      "2019-01-24T10:29:30.103494, step: 1114, loss: 3.287243366241455, acc: 0.9758087992668152, f1: 0.7887, precision: 0.8058, recall: 0.7724\n",
      "2019-01-24T10:29:30.630050, step: 1115, loss: 2.9950499534606934, acc: 0.9704711437225342, f1: 0.6667, precision: 0.6897, recall: 0.6452\n",
      "2019-01-24T10:29:31.353723, step: 1116, loss: 2.7598695755004883, acc: 0.9653222560882568, f1: 0.7006, precision: 0.7143, recall: 0.6875\n",
      "2019-01-24T10:29:32.002170, step: 1117, loss: 3.953369140625, acc: 0.9693548679351807, f1: 0.7241, precision: 0.7925, recall: 0.6667\n",
      "2019-01-24T10:29:32.810987, step: 1118, loss: 3.7834341526031494, acc: 0.969072163105011, f1: 0.6887, precision: 0.7157, recall: 0.6636\n",
      "2019-01-24T10:29:33.264142, step: 1119, loss: 3.740100860595703, acc: 0.9639639854431152, f1: 0.6173, precision: 0.6579, recall: 0.5814\n",
      "2019-01-24T10:29:34.041513, step: 1120, loss: 4.2091779708862305, acc: 0.9587020874023438, f1: 0.675, precision: 0.6639, recall: 0.6864\n",
      "2019-01-24T10:29:36.606617, step: 1121, loss: 2.9395523071289062, acc: 0.9816591143608093, f1: 0.8352, precision: 0.845, recall: 0.8258\n",
      "2019-01-24T10:29:37.522233, step: 1122, loss: 2.125631332397461, acc: 0.9785932898521423, f1: 0.7407, precision: 0.7353, recall: 0.7463\n",
      "2019-01-24T10:29:38.363096, step: 1123, loss: 2.8382930755615234, acc: 0.9740980863571167, f1: 0.7683, precision: 0.7683, recall: 0.7683\n",
      "2019-01-24T10:29:39.288579, step: 1124, loss: 5.567427635192871, acc: 0.9511256217956543, f1: 0.5306, precision: 0.5328, recall: 0.5285\n",
      "2019-01-24T10:29:41.027302, step: 1125, loss: 4.212829113006592, acc: 0.9606127142906189, f1: 0.6632, precision: 0.6632, recall: 0.6632\n",
      "2019-01-24T10:29:41.632173, step: 1126, loss: 3.817711114883423, acc: 0.9704833030700684, f1: 0.7319, precision: 0.735, recall: 0.7288\n",
      "2019-01-24T10:29:42.374978, step: 1127, loss: 3.7151682376861572, acc: 0.9666098952293396, f1: 0.6667, precision: 0.6545, recall: 0.6792\n",
      "2019-01-24T10:29:42.992341, step: 1128, loss: 3.9175429344177246, acc: 0.9603567719459534, f1: 0.6667, precision: 0.6471, recall: 0.6875\n",
      "2019-01-24T10:29:43.598294, step: 1129, loss: 2.4911067485809326, acc: 0.9753040671348572, f1: 0.7007, precision: 0.7385, recall: 0.6667\n",
      "2019-01-24T10:29:44.540928, step: 1130, loss: 3.385308265686035, acc: 0.9716206192970276, f1: 0.6946, precision: 0.716, recall: 0.6744\n",
      "2019-01-24T10:29:45.128966, step: 1131, loss: 3.327589511871338, acc: 0.9704081416130066, f1: 0.75, precision: 0.7753, recall: 0.7263\n",
      "2019-01-24T10:29:45.652651, step: 1132, loss: 3.1920480728149414, acc: 0.9692359566688538, f1: 0.6725, precision: 0.6311, recall: 0.7196\n",
      "2019-01-24T10:29:46.239346, step: 1133, loss: 4.3565497398376465, acc: 0.962217390537262, f1: 0.6695, precision: 0.6903, recall: 0.65\n",
      "2019-01-24T10:29:46.962779, step: 1134, loss: 2.46144962310791, acc: 0.9719298481941223, f1: 0.7119, precision: 0.7159, recall: 0.7079\n",
      "2019-01-24T10:29:47.750223, step: 1135, loss: 3.500697612762451, acc: 0.9674906730651855, f1: 0.6923, precision: 0.75, recall: 0.6429\n",
      "2019-01-24T10:29:48.590080, step: 1136, loss: 3.3229923248291016, acc: 0.9728373289108276, f1: 0.7257, precision: 0.7257, recall: 0.7257\n",
      "2019-01-24T10:29:49.301907, step: 1137, loss: 2.2578771114349365, acc: 0.9742790460586548, f1: 0.687, precision: 0.6818, recall: 0.6923\n",
      "2019-01-24T10:29:49.784727, step: 1138, loss: 2.88838529586792, acc: 0.9758009314537048, f1: 0.7208, precision: 0.7245, recall: 0.7172\n",
      "2019-01-24T10:29:50.830769, step: 1139, loss: 4.253619194030762, acc: 0.9602735042572021, f1: 0.6538, precision: 0.6733, recall: 0.6355\n",
      "2019-01-24T10:29:51.558584, step: 1140, loss: 3.381168842315674, acc: 0.9728646278381348, f1: 0.7079, precision: 0.7241, recall: 0.6923\n",
      "2019-01-24T10:29:52.084755, step: 1141, loss: 4.033401966094971, acc: 0.9517264366149902, f1: 0.5787, precision: 0.5876, recall: 0.57\n",
      "2019-01-24T10:29:52.713535, step: 1142, loss: 3.4393768310546875, acc: 0.976219117641449, f1: 0.8018, precision: 0.8667, recall: 0.7459\n",
      "2019-01-24T10:29:53.097695, step: 1143, loss: 3.726212501525879, acc: 0.9673522710800171, f1: 0.72, precision: 0.7431, recall: 0.6983\n",
      "2019-01-24T10:29:53.474468, step: 1144, loss: 4.5309648513793945, acc: 0.961034893989563, f1: 0.656, precision: 0.6777, recall: 0.6357\n",
      "2019-01-24T10:29:53.795687, step: 1145, loss: 4.781490325927734, acc: 0.9570326805114746, f1: 0.6996, precision: 0.7589, recall: 0.6489\n",
      "2019-01-24T10:29:54.111476, step: 1146, loss: 3.8286163806915283, acc: 0.9663588404655457, f1: 0.6818, precision: 0.6881, recall: 0.6757\n",
      "2019-01-24T10:29:54.660611, step: 1147, loss: 3.6481313705444336, acc: 0.9657177925109863, f1: 0.7249, precision: 0.7281, recall: 0.7217\n",
      "2019-01-24T10:29:55.097415, step: 1148, loss: 2.845749855041504, acc: 0.9730700254440308, f1: 0.7303, precision: 0.7303, recall: 0.7303\n",
      "2019-01-24T10:29:55.682789, step: 1149, loss: 3.6160364151000977, acc: 0.9652002453804016, f1: 0.6981, precision: 0.6981, recall: 0.6981\n",
      "2019-01-24T10:29:56.385076, step: 1150, loss: 3.3090248107910156, acc: 0.9714385271072388, f1: 0.6762, precision: 0.6636, recall: 0.6893\n",
      "2019-01-24T10:29:57.108980, step: 1151, loss: 3.029797077178955, acc: 0.9757221937179565, f1: 0.765, precision: 0.7545, recall: 0.7757\n",
      "2019-01-24T10:29:57.908508, step: 1152, loss: 4.1379241943359375, acc: 0.9664452075958252, f1: 0.6392, precision: 0.6458, recall: 0.6327\n",
      "2019-01-24T10:29:58.654654, step: 1153, loss: 2.4150028228759766, acc: 0.9779314994812012, f1: 0.7383, precision: 0.7857, recall: 0.6962\n",
      "2019-01-24T10:29:59.937179, step: 1154, loss: 4.490800857543945, acc: 0.9659353494644165, f1: 0.6174, precision: 0.6574, recall: 0.582\n",
      "2019-01-24T10:30:00.545393, step: 1155, loss: 3.536367416381836, acc: 0.9675052165985107, f1: 0.6904, precision: 0.6939, recall: 0.6869\n",
      "2019-01-24T10:30:01.144029, step: 1156, loss: 3.1194188594818115, acc: 0.9702783823013306, f1: 0.6119, precision: 0.6212, recall: 0.6029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:30:02.947351, step: 1157, loss: 9.87601375579834, acc: 0.9325973391532898, f1: 0.5673, precision: 0.4797, recall: 0.6941\n",
      "2019-01-24T10:30:03.443357, step: 1158, loss: 3.5182571411132812, acc: 0.9557886719703674, f1: 0.6024, precision: 0.6024, recall: 0.6024\n",
      "2019-01-24T10:30:04.086163, step: 1159, loss: 4.0788774490356445, acc: 0.9619548320770264, f1: 0.6516, precision: 0.6667, recall: 0.6372\n",
      "2019-01-24T10:30:04.649279, step: 1160, loss: 3.985069513320923, acc: 0.9735162854194641, f1: 0.6882, precision: 0.7442, recall: 0.64\n",
      "2019-01-24T10:30:05.234433, step: 1161, loss: 4.416316032409668, acc: 0.9638926982879639, f1: 0.6725, precision: 0.7196, recall: 0.6311\n",
      "2019-01-24T10:30:05.808305, step: 1162, loss: 4.364824295043945, acc: 0.9550449848175049, f1: 0.681, precision: 0.7453, recall: 0.627\n",
      "2019-01-24T10:30:07.076612, step: 1163, loss: 3.9796133041381836, acc: 0.9642276167869568, f1: 0.6952, precision: 0.7374, recall: 0.6577\n",
      "2019-01-24T10:30:08.255184, step: 1164, loss: 4.001657009124756, acc: 0.9716610312461853, f1: 0.7113, precision: 0.7203, recall: 0.7025\n",
      "2019-01-24T10:30:08.947840, step: 1165, loss: 3.636838912963867, acc: 0.9670836329460144, f1: 0.6772, precision: 0.6809, recall: 0.6737\n",
      "2019-01-24T10:30:09.854652, step: 1166, loss: 2.8110404014587402, acc: 0.9761499166488647, f1: 0.6923, precision: 0.7013, recall: 0.6835\n",
      "2019-01-24T10:30:10.439507, step: 1167, loss: 2.7108712196350098, acc: 0.9753661751747131, f1: 0.7006, precision: 0.6962, recall: 0.7051\n",
      "2019-01-24T10:30:12.975955, step: 1168, loss: 4.042787551879883, acc: 0.9691963195800781, f1: 0.7789, precision: 0.7815, recall: 0.7763\n",
      "2019-01-24T10:30:13.726298, step: 1169, loss: 1.8463566303253174, acc: 0.9832636117935181, f1: 0.7134, precision: 0.7089, recall: 0.7179\n",
      "2019-01-24T10:30:14.409986, step: 1170, loss: 6.672113418579102, acc: 0.9463170766830444, f1: 0.6367, precision: 0.6133, recall: 0.6619\n",
      "2019-01-24T10:30:15.211889, step: 1171, loss: 3.6167869567871094, acc: 0.9675279259681702, f1: 0.6757, precision: 0.6696, recall: 0.6818\n",
      "2019-01-24T10:30:16.367033, step: 1172, loss: 4.316385269165039, acc: 0.9573341608047485, f1: 0.6667, precision: 0.6476, recall: 0.6869\n",
      "2019-01-24T10:30:17.105155, step: 1173, loss: 3.609829902648926, acc: 0.9660452604293823, f1: 0.5542, precision: 0.575, recall: 0.5349\n",
      "2019-01-24T10:30:17.611631, step: 1174, loss: 3.6680264472961426, acc: 0.9664017558097839, f1: 0.6911, precision: 0.6947, recall: 0.6875\n",
      "2019-01-24T10:30:18.161032, step: 1175, loss: 2.7492222785949707, acc: 0.9739015698432922, f1: 0.7459, precision: 0.734, recall: 0.7582\n",
      "2019-01-24T10:30:18.883440, step: 1176, loss: 2.46559739112854, acc: 0.9758278131484985, f1: 0.6923, precision: 0.6835, recall: 0.7013\n",
      "2019-01-24T10:30:19.541009, step: 1177, loss: 2.912658214569092, acc: 0.976094126701355, f1: 0.7361, precision: 0.726, recall: 0.7465\n",
      "2019-01-24T10:30:20.290305, step: 1178, loss: 2.745893955230713, acc: 0.9739601016044617, f1: 0.759, precision: 0.7683, recall: 0.75\n",
      "2019-01-24T10:30:20.989233, step: 1179, loss: 2.856947422027588, acc: 0.9720234274864197, f1: 0.6626, precision: 0.6506, recall: 0.675\n",
      "2019-01-24T10:30:21.612073, step: 1180, loss: 2.6170973777770996, acc: 0.9767822027206421, f1: 0.702, precision: 0.7361, recall: 0.6709\n",
      "2019-01-24T10:30:23.252460, step: 1181, loss: 3.2856063842773438, acc: 0.9743101596832275, f1: 0.6792, precision: 0.7606, recall: 0.6136\n",
      "2019-01-24T10:30:24.735054, step: 1182, loss: 3.2204761505126953, acc: 0.9727218151092529, f1: 0.7204, precision: 0.7451, recall: 0.6972\n",
      "2019-01-24T10:30:25.577492, step: 1183, loss: 2.4064464569091797, acc: 0.9802796840667725, f1: 0.804, precision: 0.8081, recall: 0.8\n",
      "2019-01-24T10:30:26.148519, step: 1184, loss: 2.691230058670044, acc: 0.9737974405288696, f1: 0.7394, precision: 0.7531, recall: 0.7262\n",
      "2019-01-24T10:30:26.687223, step: 1185, loss: 3.6566619873046875, acc: 0.9655978679656982, f1: 0.6742, precision: 0.7059, recall: 0.6452\n",
      "2019-01-24T10:30:27.448984, step: 1186, loss: 3.7036514282226562, acc: 0.9604772329330444, f1: 0.6849, precision: 0.6637, recall: 0.7075\n",
      "2019-01-24T10:30:28.366351, step: 1187, loss: 3.8735923767089844, acc: 0.9690994620323181, f1: 0.7338, precision: 0.7391, recall: 0.7286\n",
      "2019-01-24T10:30:30.662985, step: 1188, loss: 6.988090515136719, acc: 0.9446072578430176, f1: 0.6018, precision: 0.6182, recall: 0.5862\n",
      "2019-01-24T10:30:31.563911, step: 1189, loss: 3.189317226409912, acc: 0.9734042286872864, f1: 0.6849, precision: 0.7042, recall: 0.6667\n",
      "2019-01-24T10:30:34.151462, step: 1190, loss: 4.326600551605225, acc: 0.9698762893676758, f1: 0.7287, precision: 0.7287, recall: 0.7287\n",
      "2019-01-24T10:30:34.728549, step: 1191, loss: 3.3201918601989746, acc: 0.9705255627632141, f1: 0.6734, precision: 0.6979, recall: 0.6505\n",
      "2019-01-24T10:30:35.324526, step: 1192, loss: 3.6832597255706787, acc: 0.9686562418937683, f1: 0.6851, precision: 0.6813, recall: 0.6889\n",
      "2019-01-24T10:30:35.939909, step: 1193, loss: 3.515528678894043, acc: 0.9747684597969055, f1: 0.6772, precision: 0.6882, recall: 0.6667\n",
      "2019-01-24T10:30:36.697508, step: 1194, loss: 2.9654319286346436, acc: 0.9762611389160156, f1: 0.7251, precision: 0.7209, recall: 0.7294\n",
      "2019-01-24T10:30:37.441541, step: 1195, loss: 3.2328953742980957, acc: 0.9757738709449768, f1: 0.7204, precision: 0.7363, recall: 0.7053\n",
      "2019-01-24T10:30:37.946778, step: 1196, loss: 3.565169334411621, acc: 0.9671480059623718, f1: 0.6879, precision: 0.7105, recall: 0.6667\n",
      "2019-01-24T10:30:38.702854, step: 1197, loss: 4.474917411804199, acc: 0.9618556499481201, f1: 0.6303, precision: 0.6842, recall: 0.5843\n",
      "2019-01-24T10:30:39.961863, step: 1198, loss: 4.525445461273193, acc: 0.9640371203422546, f1: 0.6832, precision: 0.7041, recall: 0.6635\n",
      "2019-01-24T10:30:40.945930, step: 1199, loss: 3.79788875579834, acc: 0.9652662873268127, f1: 0.7039, precision: 0.7241, recall: 0.6848\n",
      "2019-01-24T10:30:41.529817, step: 1200, loss: 2.216400146484375, acc: 0.9819639325141907, f1: 0.7724, precision: 0.7568, recall: 0.7887\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:31:06.667593, step: 1200, loss: 3.6198776033189564, acc:0.9678707420825958, f1: 0.7247222222222222, precision: 0.7555972222222221, recall: 0.6973777777777777\n",
      "\n",
      "\n",
      "2019-01-24T10:31:07.416908, step: 1201, loss: 3.495084285736084, acc: 0.9763134121894836, f1: 0.7892, precision: 0.8, recall: 0.7788\n",
      "2019-01-24T10:31:07.900171, step: 1202, loss: 4.5531110763549805, acc: 0.9535865187644958, f1: 0.6496, precision: 0.6496, recall: 0.6496\n",
      "2019-01-24T10:31:08.679880, step: 1203, loss: 3.6591267585754395, acc: 0.9690654277801514, f1: 0.5906, precision: 0.6027, recall: 0.5789\n",
      "2019-01-24T10:31:09.221918, step: 1204, loss: 3.994067668914795, acc: 0.9651724100112915, f1: 0.6569, precision: 0.6569, recall: 0.6569\n",
      "2019-01-24T10:31:10.117550, step: 1205, loss: 2.802448272705078, acc: 0.9736301302909851, f1: 0.7526, precision: 0.7604, recall: 0.7449\n",
      "2019-01-24T10:31:12.675942, step: 1206, loss: 2.420836925506592, acc: 0.980622410774231, f1: 0.8593, precision: 0.8657, recall: 0.8529\n",
      "2019-01-24T10:31:13.723504, step: 1207, loss: 5.184370994567871, acc: 0.9598085880279541, f1: 0.5951, precision: 0.6354, recall: 0.5596\n",
      "2019-01-24T10:31:14.256237, step: 1208, loss: 3.0654263496398926, acc: 0.9689028859138489, f1: 0.6957, precision: 0.7273, recall: 0.6667\n",
      "2019-01-24T10:31:14.781212, step: 1209, loss: 2.0766048431396484, acc: 0.9774052500724792, f1: 0.7073, precision: 0.716, recall: 0.6988\n",
      "2019-01-24T10:31:15.767432, step: 1210, loss: 3.3318169116973877, acc: 0.9796279668807983, f1: 0.7524, precision: 0.7822, recall: 0.7248\n",
      "2019-01-24T10:31:16.970719, step: 1211, loss: 3.439246654510498, acc: 0.9739798903465271, f1: 0.7729, precision: 0.8, recall: 0.7477\n",
      "2019-01-24T10:31:17.868962, step: 1212, loss: 3.1899354457855225, acc: 0.9742659330368042, f1: 0.6575, precision: 0.6667, recall: 0.6486\n",
      "2019-01-24T10:31:18.845471, step: 1213, loss: 3.471696138381958, acc: 0.9719499945640564, f1: 0.6772, precision: 0.6957, recall: 0.6598\n",
      "2019-01-24T10:31:19.992080, step: 1214, loss: 2.1757078170776367, acc: 0.9806020259857178, f1: 0.7753, precision: 0.7841, recall: 0.7667\n",
      "2019-01-24T10:31:20.660679, step: 1215, loss: 3.163722276687622, acc: 0.9629629850387573, f1: 0.6333, precision: 0.6867, recall: 0.5876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:31:21.356860, step: 1216, loss: 4.858572959899902, acc: 0.9535518884658813, f1: 0.6667, precision: 0.7529, recall: 0.5981\n",
      "2019-01-24T10:31:22.079248, step: 1217, loss: 4.5227885246276855, acc: 0.9485049843788147, f1: 0.5888, precision: 0.6042, recall: 0.5743\n",
      "2019-01-24T10:31:22.647566, step: 1218, loss: 3.1472339630126953, acc: 0.9743990302085876, f1: 0.7368, precision: 0.7368, recall: 0.7368\n",
      "2019-01-24T10:31:23.636890, step: 1219, loss: 3.198577404022217, acc: 0.9757884740829468, f1: 0.6702, precision: 0.6632, recall: 0.6774\n",
      "2019-01-24T10:31:24.273142, step: 1220, loss: 2.4243946075439453, acc: 0.977544903755188, f1: 0.7273, precision: 0.7273, recall: 0.7273\n",
      "2019-01-24T10:31:24.711752, step: 1221, loss: 3.4385344982147217, acc: 0.9674469828605652, f1: 0.6634, precision: 0.6768, recall: 0.6505\n",
      "2019-01-24T10:31:25.237404, step: 1222, loss: 3.386916160583496, acc: 0.9613375067710876, f1: 0.6977, precision: 0.6579, recall: 0.7426\n",
      "2019-01-24T10:31:25.654173, step: 1223, loss: 3.434009552001953, acc: 0.9600929617881775, f1: 0.6522, precision: 0.6742, recall: 0.6316\n",
      "2019-01-24T10:31:26.164733, step: 1224, loss: 2.5493555068969727, acc: 0.9741247892379761, f1: 0.7473, precision: 0.764, recall: 0.7312\n",
      "2019-01-24T10:31:26.855100, step: 1225, loss: 3.420785427093506, acc: 0.9714380502700806, f1: 0.6742, precision: 0.7143, recall: 0.6383\n",
      "2019-01-24T10:31:27.559536, step: 1226, loss: 3.4659342765808105, acc: 0.9682640433311462, f1: 0.6545, precision: 0.6835, recall: 0.6279\n",
      "2019-01-24T10:31:28.077506, step: 1227, loss: 3.6396074295043945, acc: 0.969990611076355, f1: 0.7431, precision: 0.7297, recall: 0.757\n",
      "2019-01-24T10:31:28.963816, step: 1228, loss: 3.8070783615112305, acc: 0.9615753889083862, f1: 0.6912, precision: 0.7009, recall: 0.6818\n",
      "2019-01-24T10:31:29.802402, step: 1229, loss: 3.3529467582702637, acc: 0.968095064163208, f1: 0.6264, precision: 0.6, recall: 0.6552\n",
      "2019-01-24T10:31:30.490917, step: 1230, loss: 3.571077346801758, acc: 0.9644181132316589, f1: 0.7333, precision: 0.75, recall: 0.7174\n",
      "2019-01-24T10:31:31.172516, step: 1231, loss: 2.763829231262207, acc: 0.9729999899864197, f1: 0.7021, precision: 0.7253, recall: 0.6804\n",
      "2019-01-24T10:31:32.061217, step: 1232, loss: 3.7509777545928955, acc: 0.9664406776428223, f1: 0.6514, precision: 0.6477, recall: 0.6552\n",
      "2019-01-24T10:31:32.731151, step: 1233, loss: 3.4620039463043213, acc: 0.9759280681610107, f1: 0.7568, precision: 0.7568, recall: 0.7568\n",
      "2019-01-24T10:31:33.373364, step: 1234, loss: 4.084272384643555, acc: 0.9576558470726013, f1: 0.6608, precision: 0.6818, recall: 0.641\n",
      "2019-01-24T10:31:34.326832, step: 1235, loss: 4.097577095031738, acc: 0.962228536605835, f1: 0.6286, precision: 0.66, recall: 0.6\n",
      "2019-01-24T10:31:35.273246, step: 1236, loss: 3.5027291774749756, acc: 0.9687703251838684, f1: 0.6943, precision: 0.7128, recall: 0.6768\n",
      "2019-01-24T10:31:36.211260, step: 1237, loss: 2.858403444290161, acc: 0.9745734333992004, f1: 0.6629, precision: 0.6824, recall: 0.6444\n",
      "2019-01-24T10:31:37.036900, step: 1238, loss: 3.3151865005493164, acc: 0.9708737730979919, f1: 0.703, precision: 0.7245, recall: 0.6827\n",
      "2019-01-24T10:31:38.238844, step: 1239, loss: 4.941913604736328, acc: 0.9641674160957336, f1: 0.7492, precision: 0.777, recall: 0.7233\n",
      "2019-01-24T10:31:38.762298, step: 1240, loss: 3.830899238586426, acc: 0.962043285369873, f1: 0.6602, precision: 0.6667, recall: 0.6538\n",
      "2019-01-24T10:31:39.267162, step: 1241, loss: 4.444415092468262, acc: 0.9606912732124329, f1: 0.5714, precision: 0.5536, recall: 0.5905\n",
      "2019-01-24T10:31:39.775804, step: 1242, loss: 3.851034641265869, acc: 0.9649298787117004, f1: 0.7552, precision: 0.7647, recall: 0.7459\n",
      "2019-01-24T10:31:40.404382, step: 1243, loss: 1.790562391281128, acc: 0.9848819971084595, f1: 0.7972, precision: 0.7917, recall: 0.8028\n",
      "2019-01-24T10:31:40.944620, step: 1244, loss: 2.9011332988739014, acc: 0.9646446108818054, f1: 0.6757, precision: 0.6849, recall: 0.6667\n",
      "2019-01-24T10:31:41.859507, step: 1245, loss: 3.535127878189087, acc: 0.9717290997505188, f1: 0.7638, precision: 0.776, recall: 0.7519\n",
      "2019-01-24T10:31:42.654319, step: 1246, loss: 3.1836485862731934, acc: 0.9736751914024353, f1: 0.7456, precision: 0.75, recall: 0.7412\n",
      "2019-01-24T10:31:43.359461, step: 1247, loss: 3.479192018508911, acc: 0.9650669097900391, f1: 0.5422, precision: 0.5625, recall: 0.5233\n",
      "2019-01-24T10:31:44.204385, step: 1248, loss: 3.7908763885498047, acc: 0.9689403176307678, f1: 0.6872, precision: 0.6907, recall: 0.6837\n",
      "2019-01-24T10:31:45.228211, step: 1249, loss: 3.714167833328247, acc: 0.9733412265777588, f1: 0.8013, precision: 0.8264, recall: 0.7778\n",
      "2019-01-24T10:31:45.677696, step: 1250, loss: 3.0707130432128906, acc: 0.9712722301483154, f1: 0.7292, precision: 0.7527, recall: 0.7071\n",
      "2019-01-24T10:31:46.083657, step: 1251, loss: 3.2354049682617188, acc: 0.969674825668335, f1: 0.7305, precision: 0.7439, recall: 0.7176\n",
      "2019-01-24T10:31:47.026113, step: 1252, loss: 3.785330057144165, acc: 0.9690095782279968, f1: 0.69, precision: 0.7041, recall: 0.6765\n",
      "2019-01-24T10:31:47.771028, step: 1253, loss: 3.925541400909424, acc: 0.97431880235672, f1: 0.7679, precision: 0.7778, recall: 0.7583\n",
      "2019-01-24T10:31:48.427351, step: 1254, loss: 2.539121627807617, acc: 0.9777556657791138, f1: 0.7119, precision: 0.6923, recall: 0.7326\n",
      "2019-01-24T10:31:48.913661, step: 1255, loss: 2.8230180740356445, acc: 0.9624903202056885, f1: 0.6711, precision: 0.7042, recall: 0.641\n",
      "2019-01-24T10:31:49.514395, step: 1256, loss: 3.2253737449645996, acc: 0.9733163714408875, f1: 0.7368, precision: 0.7368, recall: 0.7368\n",
      "2019-01-24T10:31:50.416922, step: 1257, loss: 4.072060585021973, acc: 0.9639852046966553, f1: 0.6839, precision: 0.7174, recall: 0.6535\n",
      "2019-01-24T10:31:51.057799, step: 1258, loss: 2.51786732673645, acc: 0.9806975722312927, f1: 0.7222, precision: 0.7222, recall: 0.7222\n",
      "2019-01-24T10:31:51.665224, step: 1259, loss: 3.0025267601013184, acc: 0.9664138555526733, f1: 0.6228, precision: 0.642, recall: 0.6047\n",
      "2019-01-24T10:31:52.076993, step: 1260, loss: 2.5428037643432617, acc: 0.9734767079353333, f1: 0.6557, precision: 0.6452, recall: 0.6667\n",
      "2019-01-24T10:31:52.737269, step: 1261, loss: 2.806946039199829, acc: 0.9695160388946533, f1: 0.7243, precision: 0.7283, recall: 0.7204\n",
      "2019-01-24T10:31:53.460758, step: 1262, loss: 2.655848979949951, acc: 0.9751511216163635, f1: 0.7821, precision: 0.7955, recall: 0.7692\n",
      "2019-01-24T10:31:53.956937, step: 1263, loss: 5.295413970947266, acc: 0.9494984149932861, f1: 0.6321, precision: 0.6837, recall: 0.5877\n",
      "2019-01-24T10:31:54.462684, step: 1264, loss: 3.0068259239196777, acc: 0.9691599011421204, f1: 0.7347, precision: 0.7579, recall: 0.7129\n",
      "2019-01-24T10:31:55.574230, step: 1265, loss: 3.034555673599243, acc: 0.9769319295883179, f1: 0.7625, precision: 0.7821, recall: 0.7439\n",
      "2019-01-24T10:31:56.288307, step: 1266, loss: 2.3755016326904297, acc: 0.9801012873649597, f1: 0.8287, precision: 0.8242, recall: 0.8333\n",
      "2019-01-24T10:31:57.082717, step: 1267, loss: 3.338578701019287, acc: 0.9717012047767639, f1: 0.7686, precision: 0.7967, recall: 0.7424\n",
      "2019-01-24T10:31:59.611568, step: 1268, loss: 3.036409854888916, acc: 0.9750859141349792, f1: 0.7699, precision: 0.7863, recall: 0.7541\n",
      "2019-01-24T10:32:00.214642, step: 1269, loss: 2.7903530597686768, acc: 0.9740006327629089, f1: 0.7097, precision: 0.7097, recall: 0.7097\n",
      "2019-01-24T10:32:00.770479, step: 1270, loss: 2.4737656116485596, acc: 0.9763374328613281, f1: 0.7717, precision: 0.7889, recall: 0.7553\n",
      "2019-01-24T10:32:01.440999, step: 1271, loss: 4.184516429901123, acc: 0.9620090126991272, f1: 0.7227, precision: 0.735, recall: 0.7107\n",
      "2019-01-24T10:32:02.097288, step: 1272, loss: 2.8143749237060547, acc: 0.9745639562606812, f1: 0.7219, precision: 0.7531, recall: 0.6932\n",
      "2019-01-24T10:32:02.763206, step: 1273, loss: 4.38641357421875, acc: 0.9559627175331116, f1: 0.6732, precision: 0.7041, recall: 0.6449\n",
      "2019-01-24T10:32:03.475134, step: 1274, loss: 3.2819137573242188, acc: 0.9713457822799683, f1: 0.7117, precision: 0.6988, recall: 0.725\n",
      "2019-01-24T10:32:04.261858, step: 1275, loss: 3.996161460876465, acc: 0.9728038907051086, f1: 0.6771, precision: 0.6771, recall: 0.6771\n",
      "2019-01-24T10:32:04.920460, step: 1276, loss: 4.125420093536377, acc: 0.966688871383667, f1: 0.736, precision: 0.7667, recall: 0.7077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:32:05.332208, step: 1277, loss: 3.2088868618011475, acc: 0.9673063158988953, f1: 0.6548, precision: 0.6548, recall: 0.6548\n",
      "2019-01-24T10:32:06.089475, step: 1278, loss: 4.816179275512695, acc: 0.9622867107391357, f1: 0.6667, precision: 0.678, recall: 0.6557\n",
      "2019-01-24T10:32:06.781113, step: 1279, loss: 3.476494789123535, acc: 0.9669030904769897, f1: 0.6763, precision: 0.6863, recall: 0.6667\n",
      "2019-01-24T10:32:07.386756, step: 1280, loss: 3.7907023429870605, acc: 0.9615980386734009, f1: 0.6885, precision: 0.7, recall: 0.6774\n",
      "2019-01-24T10:32:08.343736, step: 1281, loss: 3.16756010055542, acc: 0.9697368144989014, f1: 0.6889, precision: 0.6889, recall: 0.6889\n",
      "2019-01-24T10:32:09.298679, step: 1282, loss: 2.617338180541992, acc: 0.9795855283737183, f1: 0.7097, precision: 0.7333, recall: 0.6875\n",
      "2019-01-24T10:32:09.860300, step: 1283, loss: 3.806642532348633, acc: 0.9553666114807129, f1: 0.6077, precision: 0.6395, recall: 0.5789\n",
      "2019-01-24T10:32:10.512917, step: 1284, loss: 2.480893135070801, acc: 0.972502589225769, f1: 0.75, precision: 0.7952, recall: 0.7097\n",
      "2019-01-24T10:32:11.284665, step: 1285, loss: 4.103597640991211, acc: 0.9667487740516663, f1: 0.6667, precision: 0.699, recall: 0.6372\n",
      "2019-01-24T10:32:11.986113, step: 1286, loss: 2.600097179412842, acc: 0.9778016209602356, f1: 0.7562, precision: 0.8, recall: 0.717\n",
      "2019-01-24T10:32:12.623493, step: 1287, loss: 3.3961377143859863, acc: 0.9675197005271912, f1: 0.7, precision: 0.7216, recall: 0.6796\n",
      "2019-01-24T10:32:13.071987, step: 1288, loss: 2.382810115814209, acc: 0.9736553430557251, f1: 0.6918, precision: 0.6707, recall: 0.7143\n",
      "2019-01-24T10:32:13.653401, step: 1289, loss: 3.349874973297119, acc: 0.9727272987365723, f1: 0.6628, precision: 0.6628, recall: 0.6628\n",
      "2019-01-24T10:32:14.394721, step: 1290, loss: 1.9440406560897827, acc: 0.9787387251853943, f1: 0.6891, precision: 0.6721, recall: 0.7069\n",
      "2019-01-24T10:32:14.940860, step: 1291, loss: 3.126636505126953, acc: 0.9735503792762756, f1: 0.7442, precision: 0.7619, recall: 0.7273\n",
      "2019-01-24T10:32:15.432369, step: 1292, loss: 2.1781702041625977, acc: 0.9803600907325745, f1: 0.7892, precision: 0.7935, recall: 0.7849\n",
      "2019-01-24T10:32:15.988791, step: 1293, loss: 2.5714690685272217, acc: 0.976190447807312, f1: 0.6993, precision: 0.6667, recall: 0.7353\n",
      "2019-01-24T10:32:16.523162, step: 1294, loss: 3.27925443649292, acc: 0.9628993272781372, f1: 0.6736, precision: 0.6701, recall: 0.6771\n",
      "2019-01-24T10:32:17.515001, step: 1295, loss: 3.5906119346618652, acc: 0.9660441279411316, f1: 0.5532, precision: 0.5571, recall: 0.5493\n",
      "2019-01-24T10:32:18.542110, step: 1296, loss: 3.9774694442749023, acc: 0.9714879989624023, f1: 0.6972, precision: 0.7308, recall: 0.6667\n",
      "2019-01-24T10:32:19.065546, step: 1297, loss: 2.368687868118286, acc: 0.9831578731536865, f1: 0.736, precision: 0.7797, recall: 0.697\n",
      "2019-01-24T10:32:19.999348, step: 1298, loss: 5.102748870849609, acc: 0.9542505145072937, f1: 0.601, precision: 0.6591, recall: 0.5524\n",
      "2019-01-24T10:32:20.919577, step: 1299, loss: 4.164516448974609, acc: 0.9659616947174072, f1: 0.6188, precision: 0.6364, recall: 0.6022\n",
      "2019-01-24T10:32:21.609506, step: 1300, loss: 2.961454153060913, acc: 0.9679650664329529, f1: 0.7255, precision: 0.7551, recall: 0.6981\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:32:46.596061, step: 1300, loss: 3.301865597565969, acc:0.9695159792900085, f1: 0.7372861111111109, precision: 0.7670166666666667, recall: 0.7105305555555556\n",
      "\n",
      "\n",
      "2019-01-24T10:32:47.205997, step: 1301, loss: 3.3711915016174316, acc: 0.9713786840438843, f1: 0.6821, precision: 0.7108, recall: 0.6556\n",
      "2019-01-24T10:32:48.183043, step: 1302, loss: 3.5102243423461914, acc: 0.9715179800987244, f1: 0.663, precision: 0.6742, recall: 0.6522\n",
      "2019-01-24T10:32:49.057280, step: 1303, loss: 5.292208194732666, acc: 0.9447092413902283, f1: 0.6608, precision: 0.6944, recall: 0.6303\n",
      "2019-01-24T10:32:49.807952, step: 1304, loss: 2.7400407791137695, acc: 0.9778246879577637, f1: 0.7296, precision: 0.7342, recall: 0.725\n",
      "start training model\n",
      "2019-01-24T10:32:50.553224, step: 1305, loss: 3.6699397563934326, acc: 0.9703574776649475, f1: 0.7014, precision: 0.7048, recall: 0.6981\n",
      "2019-01-24T10:32:51.262606, step: 1306, loss: 2.821413516998291, acc: 0.9655784368515015, f1: 0.65, precision: 0.642, recall: 0.6582\n",
      "2019-01-24T10:32:52.005837, step: 1307, loss: 2.78024959564209, acc: 0.970557689666748, f1: 0.6667, precision: 0.6795, recall: 0.6543\n",
      "2019-01-24T10:32:52.757863, step: 1308, loss: 2.823190212249756, acc: 0.9718453288078308, f1: 0.7319, precision: 0.7227, recall: 0.7414\n",
      "2019-01-24T10:32:53.759008, step: 1309, loss: 2.3793601989746094, acc: 0.9783179759979248, f1: 0.7514, precision: 0.7831, recall: 0.7222\n",
      "2019-01-24T10:32:54.624568, step: 1310, loss: 4.016871452331543, acc: 0.9662576913833618, f1: 0.7, precision: 0.6942, recall: 0.7059\n",
      "2019-01-24T10:32:55.084811, step: 1311, loss: 2.4901530742645264, acc: 0.967049241065979, f1: 0.7093, precision: 0.7262, recall: 0.6932\n",
      "2019-01-24T10:32:55.625728, step: 1312, loss: 3.5404560565948486, acc: 0.9683859944343567, f1: 0.7308, precision: 0.76, recall: 0.7037\n",
      "2019-01-24T10:32:56.485269, step: 1313, loss: 2.4878320693969727, acc: 0.9748110771179199, f1: 0.7701, precision: 0.7976, recall: 0.7444\n",
      "2019-01-24T10:32:56.998863, step: 1314, loss: 3.325660228729248, acc: 0.9693413972854614, f1: 0.6842, precision: 0.7429, recall: 0.6341\n",
      "2019-01-24T10:32:57.679360, step: 1315, loss: 3.6053290367126465, acc: 0.9642043709754944, f1: 0.6887, precision: 0.6952, recall: 0.6822\n",
      "2019-01-24T10:32:58.317573, step: 1316, loss: 2.7963833808898926, acc: 0.9736472368240356, f1: 0.7347, precision: 0.7742, recall: 0.699\n",
      "2019-01-24T10:32:58.944153, step: 1317, loss: 2.0932695865631104, acc: 0.9854040741920471, f1: 0.7977, precision: 0.8023, recall: 0.7931\n",
      "2019-01-24T10:32:59.636953, step: 1318, loss: 3.1182427406311035, acc: 0.9674922823905945, f1: 0.655, precision: 0.6667, recall: 0.6437\n",
      "2019-01-24T10:33:00.320695, step: 1319, loss: 3.0334665775299072, acc: 0.9736661911010742, f1: 0.7157, precision: 0.7019, recall: 0.73\n",
      "2019-01-24T10:33:01.086669, step: 1320, loss: 2.5613837242126465, acc: 0.9696760773658752, f1: 0.7429, precision: 0.7303, recall: 0.7558\n",
      "2019-01-24T10:33:01.840916, step: 1321, loss: 2.9611759185791016, acc: 0.9731258749961853, f1: 0.7152, precision: 0.75, recall: 0.6835\n",
      "2019-01-24T10:33:02.807606, step: 1322, loss: 3.038102626800537, acc: 0.9798578023910522, f1: 0.7943, precision: 0.8058, recall: 0.783\n",
      "2019-01-24T10:33:03.375247, step: 1323, loss: 2.617640972137451, acc: 0.9799630045890808, f1: 0.7174, precision: 0.7253, recall: 0.7097\n",
      "2019-01-24T10:33:04.046451, step: 1324, loss: 2.5057015419006348, acc: 0.9762060046195984, f1: 0.7228, precision: 0.7449, recall: 0.7019\n",
      "2019-01-24T10:33:04.494414, step: 1325, loss: 2.0378308296203613, acc: 0.976190447807312, f1: 0.7547, precision: 0.8219, recall: 0.6977\n",
      "2019-01-24T10:33:05.826005, step: 1326, loss: 2.5574722290039062, acc: 0.9737279415130615, f1: 0.5972, precision: 0.6056, recall: 0.589\n",
      "2019-01-24T10:33:08.708247, step: 1327, loss: 2.7106473445892334, acc: 0.9752066135406494, f1: 0.8191, precision: 0.8392, recall: 0.8\n",
      "2019-01-24T10:33:09.323667, step: 1328, loss: 2.314850091934204, acc: 0.9775784611701965, f1: 0.7564, precision: 0.7973, recall: 0.7195\n",
      "2019-01-24T10:33:11.039171, step: 1329, loss: 6.420351028442383, acc: 0.9482758641242981, f1: 0.6875, precision: 0.6471, recall: 0.7333\n",
      "2019-01-24T10:33:11.603237, step: 1330, loss: 3.283381700515747, acc: 0.9745117425918579, f1: 0.7753, precision: 0.8118, recall: 0.7419\n",
      "2019-01-24T10:33:12.386401, step: 1331, loss: 2.427590847015381, acc: 0.9735073447227478, f1: 0.6667, precision: 0.6622, recall: 0.6712\n",
      "2019-01-24T10:33:13.108584, step: 1332, loss: 2.3953371047973633, acc: 0.9794819951057434, f1: 0.798, precision: 0.7864, recall: 0.81\n",
      "2019-01-24T10:33:13.661375, step: 1333, loss: 3.8201966285705566, acc: 0.9644491672515869, f1: 0.7455, precision: 0.7523, recall: 0.7387\n",
      "2019-01-24T10:33:16.381323, step: 1334, loss: 2.6385321617126465, acc: 0.9797804951667786, f1: 0.8296, precision: 0.8358, recall: 0.8235\n",
      "2019-01-24T10:33:17.119435, step: 1335, loss: 1.9753371477127075, acc: 0.9838827848434448, f1: 0.8125, precision: 0.8025, recall: 0.8228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:33:17.653648, step: 1336, loss: 4.042395114898682, acc: 0.9667562246322632, f1: 0.7273, precision: 0.7477, recall: 0.708\n",
      "2019-01-24T10:33:18.403321, step: 1337, loss: 2.756913661956787, acc: 0.9721277952194214, f1: 0.5793, precision: 0.5915, recall: 0.5676\n",
      "2019-01-24T10:33:19.553286, step: 1338, loss: 3.1545183658599854, acc: 0.9770992398262024, f1: 0.7397, precision: 0.75, recall: 0.7297\n",
      "2019-01-24T10:33:20.201847, step: 1339, loss: 3.0326077938079834, acc: 0.9729359149932861, f1: 0.7572, precision: 0.7863, recall: 0.7302\n",
      "2019-01-24T10:33:20.941506, step: 1340, loss: 2.3768508434295654, acc: 0.9749013781547546, f1: 0.7458, precision: 0.7719, recall: 0.7213\n",
      "2019-01-24T10:33:21.883187, step: 1341, loss: 4.20090389251709, acc: 0.9607975482940674, f1: 0.6636, precision: 0.6893, recall: 0.6396\n",
      "2019-01-24T10:33:22.518650, step: 1342, loss: 3.1604020595550537, acc: 0.9715254306793213, f1: 0.6834, precision: 0.6939, recall: 0.6733\n",
      "2019-01-24T10:33:23.192542, step: 1343, loss: 3.0261783599853516, acc: 0.9725040793418884, f1: 0.6995, precision: 0.7245, recall: 0.6762\n",
      "2019-01-24T10:33:24.155173, step: 1344, loss: 2.282620429992676, acc: 0.9749593734741211, f1: 0.7822, precision: 0.79, recall: 0.7745\n",
      "2019-01-24T10:33:25.001965, step: 1345, loss: 2.6589784622192383, acc: 0.9780657887458801, f1: 0.7914, precision: 0.8132, recall: 0.7708\n",
      "2019-01-24T10:33:25.661873, step: 1346, loss: 2.5812020301818848, acc: 0.9774305820465088, f1: 0.7394, precision: 0.7625, recall: 0.7176\n",
      "2019-01-24T10:33:26.511276, step: 1347, loss: 3.260706663131714, acc: 0.9691588878631592, f1: 0.7097, precision: 0.7253, recall: 0.6947\n",
      "2019-01-24T10:33:26.824954, step: 1348, loss: 1.480690360069275, acc: 0.9871552586555481, f1: 0.8188, precision: 0.8356, recall: 0.8026\n",
      "2019-01-24T10:33:27.564925, step: 1349, loss: 3.1073410511016846, acc: 0.9710271954536438, f1: 0.7467, precision: 0.7706, recall: 0.7241\n",
      "2019-01-24T10:33:27.937604, step: 1350, loss: 3.8680779933929443, acc: 0.9533308148384094, f1: 0.673, precision: 0.6762, recall: 0.6698\n",
      "2019-01-24T10:33:28.663445, step: 1351, loss: 2.9149858951568604, acc: 0.9807503819465637, f1: 0.7753, precision: 0.7788, recall: 0.7719\n",
      "2019-01-24T10:33:29.194719, step: 1352, loss: 2.789309501647949, acc: 0.9688555598258972, f1: 0.7414, precision: 0.7414, recall: 0.7414\n",
      "2019-01-24T10:33:31.739565, step: 1353, loss: 2.5989437103271484, acc: 0.9833524823188782, f1: 0.8632, precision: 0.8601, recall: 0.8662\n",
      "2019-01-24T10:33:32.606094, step: 1354, loss: 2.9739999771118164, acc: 0.9709949493408203, f1: 0.7075, precision: 0.6944, recall: 0.7212\n",
      "2019-01-24T10:33:33.073724, step: 1355, loss: 2.3123960494995117, acc: 0.9817936420440674, f1: 0.7547, precision: 0.7792, recall: 0.7317\n",
      "2019-01-24T10:33:34.338380, step: 1356, loss: 2.8009157180786133, acc: 0.9793124198913574, f1: 0.8561, precision: 0.8692, recall: 0.8433\n",
      "2019-01-24T10:33:34.896520, step: 1357, loss: 3.4649014472961426, acc: 0.9666104316711426, f1: 0.6901, precision: 0.7284, recall: 0.6556\n",
      "2019-01-24T10:33:35.783489, step: 1358, loss: 2.8862757682800293, acc: 0.9678658843040466, f1: 0.6512, precision: 0.6747, recall: 0.6292\n",
      "2019-01-24T10:33:36.564929, step: 1359, loss: 2.374019145965576, acc: 0.9742318987846375, f1: 0.7209, precision: 0.7561, recall: 0.6889\n",
      "2019-01-24T10:33:37.226918, step: 1360, loss: 2.217369556427002, acc: 0.9767884612083435, f1: 0.7738, precision: 0.8025, recall: 0.7471\n",
      "2019-01-24T10:33:37.804415, step: 1361, loss: 2.4972386360168457, acc: 0.9756341576576233, f1: 0.7081, precision: 0.7037, recall: 0.7125\n",
      "2019-01-24T10:33:38.345091, step: 1362, loss: 2.273540496826172, acc: 0.9793776869773865, f1: 0.7562, precision: 0.7677, recall: 0.7451\n",
      "2019-01-24T10:33:38.916444, step: 1363, loss: 2.8523597717285156, acc: 0.9760207533836365, f1: 0.7535, precision: 0.7642, recall: 0.7431\n",
      "2019-01-24T10:33:39.461192, step: 1364, loss: 1.8935002088546753, acc: 0.9791350364685059, f1: 0.8293, precision: 0.8395, recall: 0.8193\n",
      "2019-01-24T10:33:40.001216, step: 1365, loss: 2.9065821170806885, acc: 0.9737462401390076, f1: 0.6788, precision: 0.6747, recall: 0.6829\n",
      "2019-01-24T10:33:41.119074, step: 1366, loss: 3.4327383041381836, acc: 0.9721402525901794, f1: 0.7577, precision: 0.7748, recall: 0.7414\n",
      "2019-01-24T10:33:43.441236, step: 1367, loss: 2.619344711303711, acc: 0.9828823804855347, f1: 0.8235, precision: 0.8485, recall: 0.8\n",
      "2019-01-24T10:33:43.945870, step: 1368, loss: 1.9857141971588135, acc: 0.9748031497001648, f1: 0.7784, precision: 0.8025, recall: 0.7558\n",
      "2019-01-24T10:33:44.798218, step: 1369, loss: 3.34108304977417, acc: 0.9767806529998779, f1: 0.7296, precision: 0.7391, recall: 0.7203\n",
      "2019-01-24T10:33:45.355951, step: 1370, loss: 2.640669584274292, acc: 0.9698384404182434, f1: 0.7204, precision: 0.7053, recall: 0.7363\n",
      "2019-01-24T10:33:46.065293, step: 1371, loss: 2.2586774826049805, acc: 0.9833123683929443, f1: 0.7528, precision: 0.7614, recall: 0.7444\n",
      "2019-01-24T10:33:46.907785, step: 1372, loss: 3.1915369033813477, acc: 0.9656445980072021, f1: 0.6769, precision: 0.6875, recall: 0.6667\n",
      "2019-01-24T10:33:47.454795, step: 1373, loss: 3.4210710525512695, acc: 0.9700182676315308, f1: 0.7196, precision: 0.7391, recall: 0.701\n",
      "2019-01-24T10:33:48.039725, step: 1374, loss: 3.246123790740967, acc: 0.9711876511573792, f1: 0.6456, precision: 0.68, recall: 0.6145\n",
      "2019-01-24T10:33:48.702574, step: 1375, loss: 2.158867359161377, acc: 0.9815950989723206, f1: 0.7742, precision: 0.7912, recall: 0.7579\n",
      "2019-01-24T10:33:49.659852, step: 1376, loss: 2.6875288486480713, acc: 0.9767680168151855, f1: 0.7164, precision: 0.7273, recall: 0.7059\n",
      "2019-01-24T10:33:50.380651, step: 1377, loss: 2.084611177444458, acc: 0.979115903377533, f1: 0.7234, precision: 0.7083, recall: 0.7391\n",
      "2019-01-24T10:33:50.912298, step: 1378, loss: 2.9882168769836426, acc: 0.9698227047920227, f1: 0.7107, precision: 0.7368, recall: 0.6863\n",
      "2019-01-24T10:33:51.452973, step: 1379, loss: 2.725407838821411, acc: 0.9755632877349854, f1: 0.6349, precision: 0.6667, recall: 0.6061\n",
      "2019-01-24T10:33:52.017758, step: 1380, loss: 1.4009788036346436, acc: 0.9890780448913574, f1: 0.8676, precision: 0.8559, recall: 0.8796\n",
      "2019-01-24T10:33:52.862640, step: 1381, loss: 2.2757911682128906, acc: 0.9805111885070801, f1: 0.7795, precision: 0.7755, recall: 0.7835\n",
      "2019-01-24T10:33:53.231907, step: 1382, loss: 3.2856838703155518, acc: 0.9690003395080566, f1: 0.6533, precision: 0.6436, recall: 0.6633\n",
      "2019-01-24T10:33:55.621303, step: 1383, loss: 1.973825216293335, acc: 0.9859194755554199, f1: 0.8803, precision: 0.8571, recall: 0.9048\n",
      "2019-01-24T10:33:56.123669, step: 1384, loss: 1.9123258590698242, acc: 0.9813821315765381, f1: 0.7853, precision: 0.7711, recall: 0.8\n",
      "2019-01-24T10:33:56.748788, step: 1385, loss: 2.3228554725646973, acc: 0.9781714677810669, f1: 0.7262, precision: 0.7093, recall: 0.7439\n",
      "2019-01-24T10:33:57.650498, step: 1386, loss: 2.33461332321167, acc: 0.9756860136985779, f1: 0.6759, precision: 0.7, recall: 0.6533\n",
      "2019-01-24T10:33:58.314943, step: 1387, loss: 3.5158939361572266, acc: 0.9570724964141846, f1: 0.7192, precision: 0.7526, recall: 0.6887\n",
      "2019-01-24T10:33:58.986291, step: 1388, loss: 4.044672012329102, acc: 0.9611998796463013, f1: 0.7256, precision: 0.7879, recall: 0.6724\n",
      "2019-01-24T10:33:59.365801, step: 1389, loss: 3.0026259422302246, acc: 0.9738770127296448, f1: 0.732, precision: 0.732, recall: 0.732\n",
      "2019-01-24T10:34:00.057642, step: 1390, loss: 2.813349723815918, acc: 0.9782450795173645, f1: 0.7586, precision: 0.7778, recall: 0.7404\n",
      "2019-01-24T10:34:00.777139, step: 1391, loss: 3.0189566612243652, acc: 0.9763455390930176, f1: 0.7128, precision: 0.7128, recall: 0.7128\n",
      "2019-01-24T10:34:01.534281, step: 1392, loss: 3.366076946258545, acc: 0.9699372053146362, f1: 0.7583, precision: 0.7407, recall: 0.7767\n",
      "2019-01-24T10:34:02.221747, step: 1393, loss: 2.5890371799468994, acc: 0.9689416885375977, f1: 0.7262, precision: 0.7262, recall: 0.7262\n",
      "2019-01-24T10:34:02.608548, step: 1394, loss: 3.1063785552978516, acc: 0.969111979007721, f1: 0.7721, precision: 0.7757, recall: 0.7685\n",
      "2019-01-24T10:34:03.346185, step: 1395, loss: 2.9157567024230957, acc: 0.9733491539955139, f1: 0.6851, precision: 0.6739, recall: 0.6966\n",
      "2019-01-24T10:34:03.861200, step: 1396, loss: 3.837181329727173, acc: 0.9601341485977173, f1: 0.587, precision: 0.6353, recall: 0.5455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:34:04.496697, step: 1397, loss: 2.4736790657043457, acc: 0.9741911888122559, f1: 0.7397, precision: 0.75, recall: 0.7297\n",
      "2019-01-24T10:34:05.090775, step: 1398, loss: 2.106193780899048, acc: 0.9797760844230652, f1: 0.7927, precision: 0.8228, recall: 0.7647\n",
      "2019-01-24T10:34:07.798337, step: 1399, loss: 3.6811635494232178, acc: 0.977325439453125, f1: 0.8278, precision: 0.8503, recall: 0.8065\n",
      "2019-01-24T10:34:08.643619, step: 1400, loss: 2.7831315994262695, acc: 0.9788079261779785, f1: 0.7746, precision: 0.8375, recall: 0.7204\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:34:33.122188, step: 1400, loss: 3.3293072746859655, acc:0.9701090173588859, f1: 0.7472027777777778, precision: 0.7853694444444442, recall: 0.713161111111111\n",
      "\n",
      "\n",
      "2019-01-24T10:34:33.806506, step: 1401, loss: 2.4767684936523438, acc: 0.9736340641975403, f1: 0.75, precision: 0.7636, recall: 0.7368\n",
      "2019-01-24T10:34:34.392063, step: 1402, loss: 3.1301350593566895, acc: 0.9728041291236877, f1: 0.7677, precision: 0.7525, recall: 0.7835\n",
      "2019-01-24T10:34:35.103740, step: 1403, loss: 2.7973852157592773, acc: 0.9741610884666443, f1: 0.7363, precision: 0.7872, recall: 0.6916\n",
      "2019-01-24T10:34:35.644301, step: 1404, loss: 4.194456577301025, acc: 0.9564459919929504, f1: 0.6145, precision: 0.6375, recall: 0.593\n",
      "2019-01-24T10:34:36.168562, step: 1405, loss: 2.848980665206909, acc: 0.9748666882514954, f1: 0.7391, precision: 0.7846, recall: 0.6986\n",
      "2019-01-24T10:34:37.061953, step: 1406, loss: 3.0235037803649902, acc: 0.9714847803115845, f1: 0.6961, precision: 0.732, recall: 0.6636\n",
      "2019-01-24T10:34:37.620831, step: 1407, loss: 2.8366787433624268, acc: 0.9755464792251587, f1: 0.7273, precision: 0.7647, recall: 0.6933\n",
      "2019-01-24T10:34:38.272338, step: 1408, loss: 2.2277302742004395, acc: 0.9757246375083923, f1: 0.7444, precision: 0.7283, recall: 0.7614\n",
      "2019-01-24T10:34:38.890307, step: 1409, loss: 1.8548479080200195, acc: 0.983038604259491, f1: 0.8333, precision: 0.8152, recall: 0.8523\n",
      "2019-01-24T10:34:39.403965, step: 1410, loss: 3.4660162925720215, acc: 0.9648103713989258, f1: 0.6977, precision: 0.6881, recall: 0.7075\n",
      "2019-01-24T10:34:40.062173, step: 1411, loss: 3.3622500896453857, acc: 0.9734799265861511, f1: 0.7706, precision: 0.7876, recall: 0.7542\n",
      "2019-01-24T10:34:40.489540, step: 1412, loss: 1.8229180574417114, acc: 0.9825732111930847, f1: 0.8308, precision: 0.8265, recall: 0.8351\n",
      "2019-01-24T10:34:41.417934, step: 1413, loss: 3.5541388988494873, acc: 0.9680114984512329, f1: 0.7534, precision: 0.7636, recall: 0.7434\n",
      "2019-01-24T10:34:42.380766, step: 1414, loss: 3.7537589073181152, acc: 0.9633343815803528, f1: 0.6732, precision: 0.7041, recall: 0.6449\n",
      "2019-01-24T10:34:43.302157, step: 1415, loss: 2.6026511192321777, acc: 0.9683566093444824, f1: 0.7679, precision: 0.7818, recall: 0.7544\n",
      "2019-01-24T10:34:43.946801, step: 1416, loss: 2.6207339763641357, acc: 0.9715850949287415, f1: 0.7293, precision: 0.75, recall: 0.7097\n",
      "2019-01-24T10:34:44.761574, step: 1417, loss: 3.8023123741149902, acc: 0.9668325185775757, f1: 0.7966, precision: 0.8319, recall: 0.7642\n",
      "2019-01-24T10:34:45.296008, step: 1418, loss: 3.3519606590270996, acc: 0.9709103107452393, f1: 0.7541, precision: 0.7841, recall: 0.7263\n",
      "2019-01-24T10:34:45.731674, step: 1419, loss: 1.6553767919540405, acc: 0.9849849939346313, f1: 0.7234, precision: 0.7391, recall: 0.7083\n",
      "2019-01-24T10:34:46.410592, step: 1420, loss: 2.5188241004943848, acc: 0.9745075106620789, f1: 0.8023, precision: 0.8118, recall: 0.7931\n",
      "2019-01-24T10:34:47.108973, step: 1421, loss: 3.8715195655822754, acc: 0.9629993438720703, f1: 0.7192, precision: 0.7228, recall: 0.7157\n",
      "2019-01-24T10:34:47.795320, step: 1422, loss: 2.5796821117401123, acc: 0.9684134721755981, f1: 0.6747, precision: 0.6512, recall: 0.7\n",
      "2019-01-24T10:34:48.386043, step: 1423, loss: 3.8129258155822754, acc: 0.9689058661460876, f1: 0.7653, precision: 0.7681, recall: 0.7626\n",
      "2019-01-24T10:34:49.105578, step: 1424, loss: 3.264122486114502, acc: 0.9725327491760254, f1: 0.7525, precision: 0.7525, recall: 0.7525\n",
      "2019-01-24T10:34:49.823389, step: 1425, loss: 2.829739809036255, acc: 0.9709643721580505, f1: 0.7282, precision: 0.7474, recall: 0.71\n",
      "2019-01-24T10:34:52.350371, step: 1426, loss: 3.1715171337127686, acc: 0.9811076521873474, f1: 0.8387, precision: 0.8563, recall: 0.8218\n",
      "2019-01-24T10:34:53.303872, step: 1427, loss: 3.5892772674560547, acc: 0.9640198349952698, f1: 0.6637, precision: 0.6916, recall: 0.6379\n",
      "2019-01-24T10:34:53.980931, step: 1428, loss: 2.2450599670410156, acc: 0.9762845635414124, f1: 0.7086, precision: 0.7045, recall: 0.7126\n",
      "2019-01-24T10:34:54.810153, step: 1429, loss: 1.9277125597000122, acc: 0.9857705235481262, f1: 0.8152, precision: 0.7979, recall: 0.8333\n",
      "2019-01-24T10:34:55.479915, step: 1430, loss: 2.6363449096679688, acc: 0.972547709941864, f1: 0.68, precision: 0.68, recall: 0.68\n",
      "2019-01-24T10:34:56.513242, step: 1431, loss: 2.9091920852661133, acc: 0.9729817509651184, f1: 0.712, precision: 0.7158, recall: 0.7083\n",
      "2019-01-24T10:34:57.003447, step: 1432, loss: 2.3895602226257324, acc: 0.9789885878562927, f1: 0.776, precision: 0.7978, recall: 0.7553\n",
      "2019-01-24T10:34:57.972342, step: 1433, loss: 4.214832305908203, acc: 0.96484375, f1: 0.7349, precision: 0.7822, recall: 0.693\n",
      "2019-01-24T10:34:58.903011, step: 1434, loss: 2.470587730407715, acc: 0.9774281978607178, f1: 0.7835, precision: 0.8085, recall: 0.76\n",
      "2019-01-24T10:34:59.698976, step: 1435, loss: 3.3529052734375, acc: 0.9736666679382324, f1: 0.7092, precision: 0.7692, recall: 0.6579\n",
      "2019-01-24T10:35:00.671184, step: 1436, loss: 3.2417378425598145, acc: 0.9723066687583923, f1: 0.6979, precision: 0.7053, recall: 0.6907\n",
      "2019-01-24T10:35:01.561574, step: 1437, loss: 1.9459495544433594, acc: 0.9813151359558105, f1: 0.8282, precision: 0.8174, recall: 0.8393\n",
      "2019-01-24T10:35:02.535909, step: 1438, loss: 2.148118019104004, acc: 0.9777625799179077, f1: 0.7294, precision: 0.7294, recall: 0.7294\n",
      "2019-01-24T10:35:03.174172, step: 1439, loss: 1.9401071071624756, acc: 0.9840663075447083, f1: 0.7407, precision: 0.7353, recall: 0.7463\n",
      "2019-01-24T10:35:03.902392, step: 1440, loss: 3.106132984161377, acc: 0.9756323099136353, f1: 0.7671, precision: 0.785, recall: 0.75\n",
      "2019-01-24T10:35:04.542535, step: 1441, loss: 2.3804407119750977, acc: 0.9738636612892151, f1: 0.75, precision: 0.7692, recall: 0.7317\n",
      "2019-01-24T10:35:05.424362, step: 1442, loss: 2.4461755752563477, acc: 0.9824129343032837, f1: 0.8406, precision: 0.8365, recall: 0.8447\n",
      "2019-01-24T10:35:06.359690, step: 1443, loss: 2.3215584754943848, acc: 0.9748833775520325, f1: 0.7284, precision: 0.7375, recall: 0.7195\n",
      "2019-01-24T10:35:08.018579, step: 1444, loss: 2.42720103263855, acc: 0.9789754152297974, f1: 0.7399, precision: 0.7711, recall: 0.7111\n",
      "2019-01-24T10:35:08.605528, step: 1445, loss: 2.197906494140625, acc: 0.9825728535652161, f1: 0.8098, precision: 0.8354, recall: 0.7857\n",
      "2019-01-24T10:35:09.099017, step: 1446, loss: 2.4982781410217285, acc: 0.9757281541824341, f1: 0.759, precision: 0.7975, recall: 0.7241\n",
      "2019-01-24T10:35:10.475704, step: 1447, loss: 3.869269847869873, acc: 0.9732288718223572, f1: 0.7623, precision: 0.775, recall: 0.75\n",
      "2019-01-24T10:35:11.097767, step: 1448, loss: 3.925759792327881, acc: 0.9684600830078125, f1: 0.7179, precision: 0.7434, recall: 0.6942\n",
      "2019-01-24T10:35:11.708406, step: 1449, loss: 1.956418752670288, acc: 0.9792633652687073, f1: 0.7105, precision: 0.7105, recall: 0.7105\n",
      "2019-01-24T10:35:12.707511, step: 1450, loss: 3.7367305755615234, acc: 0.9631367325782776, f1: 0.7692, precision: 0.7438, recall: 0.7965\n",
      "2019-01-24T10:35:13.294504, step: 1451, loss: 3.037111282348633, acc: 0.974715530872345, f1: 0.7845, precision: 0.8053, recall: 0.7647\n",
      "2019-01-24T10:35:14.020194, step: 1452, loss: 4.981496810913086, acc: 0.9644293189048767, f1: 0.664, precision: 0.6667, recall: 0.6613\n",
      "2019-01-24T10:35:14.489043, step: 1453, loss: 2.691524028778076, acc: 0.9739764332771301, f1: 0.7629, precision: 0.7629, recall: 0.7629\n",
      "2019-01-24T10:35:15.574503, step: 1454, loss: 2.815237522125244, acc: 0.9781491160392761, f1: 0.7399, precision: 0.7442, recall: 0.7356\n",
      "2019-01-24T10:35:16.121416, step: 1455, loss: 2.49176025390625, acc: 0.9757853150367737, f1: 0.7104, precision: 0.7143, recall: 0.7065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:35:16.841969, step: 1456, loss: 3.2422289848327637, acc: 0.9572212100028992, f1: 0.686, precision: 0.6975, recall: 0.6748\n",
      "2019-01-24T10:35:17.518297, step: 1457, loss: 2.228743553161621, acc: 0.970588207244873, f1: 0.7125, precision: 0.7308, recall: 0.6951\n",
      "2019-01-24T10:35:18.656537, step: 1458, loss: 2.884758472442627, acc: 0.9730221033096313, f1: 0.7045, precision: 0.7294, recall: 0.6813\n",
      "2019-01-24T10:35:18.972091, step: 1459, loss: 2.053750514984131, acc: 0.9721820950508118, f1: 0.7904, precision: 0.8148, recall: 0.7674\n",
      "2019-01-24T10:35:19.691185, step: 1460, loss: 3.87595796585083, acc: 0.9646914601325989, f1: 0.7089, precision: 0.7059, recall: 0.7119\n",
      "2019-01-24T10:35:20.200260, step: 1461, loss: 3.3613648414611816, acc: 0.9652551412582397, f1: 0.6263, precision: 0.6596, recall: 0.5962\n",
      "2019-01-24T10:35:20.790236, step: 1462, loss: 3.9136834144592285, acc: 0.9734144806861877, f1: 0.7126, precision: 0.7273, recall: 0.6984\n",
      "2019-01-24T10:35:21.400170, step: 1463, loss: 2.6399073600769043, acc: 0.9722133278846741, f1: 0.7447, precision: 0.7865, recall: 0.7071\n",
      "2019-01-24T10:35:22.025947, step: 1464, loss: 2.8648324012756348, acc: 0.9722131490707397, f1: 0.7021, precision: 0.7021, recall: 0.7021\n",
      "2019-01-24T10:35:23.732758, step: 1465, loss: 3.483290195465088, acc: 0.975348949432373, f1: 0.7229, precision: 0.7143, recall: 0.7317\n",
      "2019-01-24T10:35:24.723763, step: 1466, loss: 2.8965930938720703, acc: 0.9782196879386902, f1: 0.7293, precision: 0.7333, recall: 0.7253\n",
      "2019-01-24T10:35:25.381846, step: 1467, loss: 3.4530797004699707, acc: 0.9676584601402283, f1: 0.6298, precision: 0.6333, recall: 0.6264\n",
      "2019-01-24T10:35:26.363413, step: 1468, loss: 3.254437208175659, acc: 0.9745330810546875, f1: 0.7742, precision: 0.7559, recall: 0.7934\n",
      "2019-01-24T10:35:26.881359, step: 1469, loss: 3.1450045108795166, acc: 0.9693251252174377, f1: 0.7358, precision: 0.732, recall: 0.7396\n",
      "2019-01-24T10:35:27.365356, step: 1470, loss: 2.4969682693481445, acc: 0.9672619104385376, f1: 0.6829, precision: 0.7, recall: 0.6667\n",
      "2019-01-24T10:35:27.843964, step: 1471, loss: 3.1828556060791016, acc: 0.9684246778488159, f1: 0.7149, precision: 0.7182, recall: 0.7117\n",
      "2019-01-24T10:35:28.402839, step: 1472, loss: 2.65390682220459, acc: 0.9732294082641602, f1: 0.7363, precision: 0.7363, recall: 0.7363\n",
      "2019-01-24T10:35:29.067762, step: 1473, loss: 4.420454025268555, acc: 0.9569694995880127, f1: 0.6186, precision: 0.625, recall: 0.6122\n",
      "2019-01-24T10:35:29.748993, step: 1474, loss: 2.8261046409606934, acc: 0.9739633798599243, f1: 0.767, precision: 0.798, recall: 0.7383\n",
      "2019-01-24T10:35:32.380520, step: 1475, loss: 2.21530818939209, acc: 0.9851640462875366, f1: 0.8487, precision: 0.8647, recall: 0.8333\n",
      "2019-01-24T10:35:33.051907, step: 1476, loss: 3.4871935844421387, acc: 0.9673724174499512, f1: 0.7676, precision: 0.8068, recall: 0.732\n",
      "2019-01-24T10:35:33.612560, step: 1477, loss: 1.8041988611221313, acc: 0.985309898853302, f1: 0.803, precision: 0.803, recall: 0.803\n",
      "2019-01-24T10:35:34.916208, step: 1478, loss: 3.8247976303100586, acc: 0.9503916501998901, f1: 0.6275, precision: 0.6667, recall: 0.5926\n",
      "2019-01-24T10:35:35.547502, step: 1479, loss: 2.3483331203460693, acc: 0.9799299240112305, f1: 0.7355, precision: 0.7917, recall: 0.6867\n",
      "2019-01-24T10:35:36.450327, step: 1480, loss: 2.2974185943603516, acc: 0.9774678349494934, f1: 0.8085, precision: 0.8172, recall: 0.8\n",
      "2019-01-24T10:35:37.092564, step: 1481, loss: 3.427605152130127, acc: 0.9686154127120972, f1: 0.7435, precision: 0.7717, recall: 0.7172\n",
      "2019-01-24T10:35:37.672834, step: 1482, loss: 3.5395805835723877, acc: 0.9680325388908386, f1: 0.7226, precision: 0.7333, recall: 0.7122\n",
      "2019-01-24T10:35:38.200734, step: 1483, loss: 2.11433744430542, acc: 0.9814814925193787, f1: 0.7595, precision: 0.75, recall: 0.7692\n",
      "2019-01-24T10:35:40.830464, step: 1484, loss: 2.2814290523529053, acc: 0.981856644153595, f1: 0.8562, precision: 0.8477, recall: 0.8649\n",
      "2019-01-24T10:35:42.129373, step: 1485, loss: 3.626640796661377, acc: 0.970346987247467, f1: 0.7273, precision: 0.7636, recall: 0.6942\n",
      "2019-01-24T10:35:42.646259, step: 1486, loss: 2.883808135986328, acc: 0.9659714698791504, f1: 0.6771, precision: 0.7143, recall: 0.6436\n",
      "2019-01-24T10:35:43.483143, step: 1487, loss: 1.7094775438308716, acc: 0.9780380725860596, f1: 0.7458, precision: 0.7097, recall: 0.7857\n",
      "2019-01-24T10:35:44.249759, step: 1488, loss: 2.718040943145752, acc: 0.9762504696846008, f1: 0.7701, precision: 0.7742, recall: 0.766\n",
      "2019-01-24T10:35:44.861651, step: 1489, loss: 2.5775394439697266, acc: 0.9785985350608826, f1: 0.7817, precision: 0.7778, recall: 0.7857\n",
      "2019-01-24T10:35:45.706004, step: 1490, loss: 3.028554677963257, acc: 0.9709102511405945, f1: 0.7191, precision: 0.7619, recall: 0.6809\n",
      "2019-01-24T10:35:46.263568, step: 1491, loss: 2.587277889251709, acc: 0.9645621180534363, f1: 0.7191, precision: 0.7273, recall: 0.7111\n",
      "2019-01-24T10:35:46.767406, step: 1492, loss: 1.807432770729065, acc: 0.987500011920929, f1: 0.8485, precision: 0.8615, recall: 0.8358\n",
      "2019-01-24T10:35:47.966887, step: 1493, loss: 2.7381818294525146, acc: 0.9696016907691956, f1: 0.6, precision: 0.6164, recall: 0.5844\n",
      "2019-01-24T10:35:49.025491, step: 1494, loss: 3.2544894218444824, acc: 0.9720343947410583, f1: 0.7373, precision: 0.7477, recall: 0.7273\n",
      "2019-01-24T10:35:49.682965, step: 1495, loss: 2.4326486587524414, acc: 0.9744861721992493, f1: 0.7037, precision: 0.75, recall: 0.6628\n",
      "2019-01-24T10:35:50.741782, step: 1496, loss: 2.4285173416137695, acc: 0.9815115332603455, f1: 0.7702, precision: 0.8052, recall: 0.7381\n",
      "2019-01-24T10:35:51.531128, step: 1497, loss: 2.4869065284729004, acc: 0.9791737794876099, f1: 0.8427, precision: 0.8333, recall: 0.8523\n",
      "2019-01-24T10:35:52.715502, step: 1498, loss: 3.4676103591918945, acc: 0.959373950958252, f1: 0.5427, precision: 0.54, recall: 0.5455\n",
      "2019-01-24T10:35:53.776668, step: 1499, loss: 2.7488760948181152, acc: 0.9731299877166748, f1: 0.7772, precision: 0.7576, recall: 0.7979\n",
      "2019-01-24T10:35:54.292918, step: 1500, loss: 2.4815359115600586, acc: 0.9735999703407288, f1: 0.6714, precision: 0.6912, recall: 0.6528\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:36:17.625558, step: 1500, loss: 3.1395949257744684, acc:0.9701764252450731, f1: 0.7492972222222223, precision: 0.7783361111111113, recall: 0.7230944444444444\n",
      "\n",
      "\n",
      "2019-01-24T10:36:18.355976, step: 1501, loss: 3.4187376499176025, acc: 0.9678188562393188, f1: 0.6825, precision: 0.688, recall: 0.6772\n",
      "2019-01-24T10:36:19.148371, step: 1502, loss: 2.412307024002075, acc: 0.9771217703819275, f1: 0.7195, precision: 0.7564, recall: 0.686\n",
      "2019-01-24T10:36:20.010159, step: 1503, loss: 1.7604575157165527, acc: 0.9825912117958069, f1: 0.8199, precision: 0.8148, recall: 0.825\n",
      "2019-01-24T10:36:20.749927, step: 1504, loss: 3.0009236335754395, acc: 0.9739214181900024, f1: 0.7736, precision: 0.82, recall: 0.7321\n",
      "2019-01-24T10:36:21.571193, step: 1505, loss: 2.7796506881713867, acc: 0.9732313752174377, f1: 0.7302, precision: 0.7667, recall: 0.697\n",
      "2019-01-24T10:36:22.442667, step: 1506, loss: 2.7168478965759277, acc: 0.9761281609535217, f1: 0.7464, precision: 0.7573, recall: 0.7358\n",
      "2019-01-24T10:36:23.036600, step: 1507, loss: 3.1630706787109375, acc: 0.9710924625396729, f1: 0.7308, precision: 0.76, recall: 0.7037\n",
      "2019-01-24T10:36:23.779852, step: 1508, loss: 1.9471409320831299, acc: 0.9798657894134521, f1: 0.7786, precision: 0.8226, recall: 0.7391\n",
      "2019-01-24T10:36:24.485329, step: 1509, loss: 3.233258008956909, acc: 0.9708189368247986, f1: 0.6957, precision: 0.7129, recall: 0.6792\n",
      "2019-01-24T10:36:24.996366, step: 1510, loss: 2.5753321647644043, acc: 0.9747768640518188, f1: 0.8136, precision: 0.8, recall: 0.8276\n",
      "2019-01-24T10:36:25.770415, step: 1511, loss: 2.1653051376342773, acc: 0.984299898147583, f1: 0.8072, precision: 0.8171, recall: 0.7976\n",
      "2019-01-24T10:36:26.564696, step: 1512, loss: 2.244133949279785, acc: 0.9811802506446838, f1: 0.7614, precision: 0.7653, recall: 0.7576\n",
      "2019-01-24T10:36:27.285989, step: 1513, loss: 2.468167781829834, acc: 0.9781780242919922, f1: 0.6536, precision: 0.6494, recall: 0.6579\n",
      "2019-01-24T10:36:27.835675, step: 1514, loss: 2.417724609375, acc: 0.9734082221984863, f1: 0.7284, precision: 0.7468, recall: 0.7108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:36:28.396628, step: 1515, loss: 3.1871893405914307, acc: 0.9682160019874573, f1: 0.7136, precision: 0.7396, recall: 0.6893\n",
      "2019-01-24T10:36:29.327008, step: 1516, loss: 2.9938302040100098, acc: 0.9805564880371094, f1: 0.8426, precision: 0.8609, recall: 0.825\n",
      "2019-01-24T10:36:30.125004, step: 1517, loss: 3.0364885330200195, acc: 0.9693537354469299, f1: 0.6471, precision: 0.6707, recall: 0.625\n",
      "2019-01-24T10:36:31.294418, step: 1518, loss: 3.155545949935913, acc: 0.97825026512146, f1: 0.754, precision: 0.7724, recall: 0.7364\n",
      "2019-01-24T10:36:32.173858, step: 1519, loss: 2.016097068786621, acc: 0.9822052717208862, f1: 0.8148, precision: 0.825, recall: 0.8049\n",
      "2019-01-24T10:36:32.789658, step: 1520, loss: 2.6068525314331055, acc: 0.9726659655570984, f1: 0.7368, precision: 0.7404, recall: 0.7333\n",
      "2019-01-24T10:36:33.173152, step: 1521, loss: 2.914299964904785, acc: 0.9775784611701965, f1: 0.7158, precision: 0.7234, recall: 0.7083\n",
      "2019-01-24T10:36:33.906610, step: 1522, loss: 3.616969108581543, acc: 0.9684523940086365, f1: 0.6611, precision: 0.6475, recall: 0.6752\n",
      "2019-01-24T10:36:34.420582, step: 1523, loss: 3.0712742805480957, acc: 0.9715961813926697, f1: 0.7048, precision: 0.7339, recall: 0.678\n",
      "2019-01-24T10:36:35.060178, step: 1524, loss: 3.553518295288086, acc: 0.9655067920684814, f1: 0.7296, precision: 0.7456, recall: 0.7143\n",
      "2019-01-24T10:36:36.088609, step: 1525, loss: 3.370901346206665, acc: 0.9652312397956848, f1: 0.6667, precision: 0.6893, recall: 0.6455\n",
      "2019-01-24T10:36:36.593452, step: 1526, loss: 2.7929139137268066, acc: 0.9757158160209656, f1: 0.7839, precision: 0.7959, recall: 0.7723\n",
      "2019-01-24T10:36:37.141022, step: 1527, loss: 2.5525612831115723, acc: 0.9801587462425232, f1: 0.8018, precision: 0.8318, recall: 0.7739\n",
      "2019-01-24T10:36:37.937767, step: 1528, loss: 2.830899715423584, acc: 0.9677197933197021, f1: 0.6961, precision: 0.6923, recall: 0.7\n",
      "2019-01-24T10:36:38.473252, step: 1529, loss: 2.1381261348724365, acc: 0.9771673083305359, f1: 0.6306, precision: 0.6481, recall: 0.614\n",
      "2019-01-24T10:36:39.161235, step: 1530, loss: 3.064476728439331, acc: 0.9705690145492554, f1: 0.7985, precision: 0.8359, recall: 0.7643\n",
      "2019-01-24T10:36:39.729299, step: 1531, loss: 2.4790661334991455, acc: 0.9795446991920471, f1: 0.7273, precision: 0.7089, recall: 0.7467\n",
      "2019-01-24T10:36:40.194553, step: 1532, loss: 3.1116013526916504, acc: 0.9684802889823914, f1: 0.6854, precision: 0.6489, recall: 0.7262\n",
      "2019-01-24T10:36:40.916982, step: 1533, loss: 3.030066728591919, acc: 0.9733333587646484, f1: 0.6927, precision: 0.6813, recall: 0.7045\n",
      "2019-01-24T10:36:41.669731, step: 1534, loss: 2.1340932846069336, acc: 0.9792919158935547, f1: 0.7841, precision: 0.8023, recall: 0.7667\n",
      "2019-01-24T10:36:42.470552, step: 1535, loss: 3.1131019592285156, acc: 0.9715890288352966, f1: 0.7135, precision: 0.7262, recall: 0.7011\n",
      "2019-01-24T10:36:43.036618, step: 1536, loss: 2.570948600769043, acc: 0.9751895070075989, f1: 0.7528, precision: 0.7363, recall: 0.7701\n",
      "2019-01-24T10:36:45.594874, step: 1537, loss: 2.7510600090026855, acc: 0.979040265083313, f1: 0.7761, precision: 0.8062, recall: 0.7482\n",
      "2019-01-24T10:36:46.423797, step: 1538, loss: 2.510059356689453, acc: 0.9788122177124023, f1: 0.7595, precision: 0.7692, recall: 0.75\n",
      "2019-01-24T10:36:47.075704, step: 1539, loss: 2.9778800010681152, acc: 0.97318434715271, f1: 0.7607, precision: 0.8378, recall: 0.6966\n",
      "2019-01-24T10:36:48.565473, step: 1540, loss: 4.241612911224365, acc: 0.9577555656433105, f1: 0.7339, precision: 0.7742, recall: 0.6977\n",
      "2019-01-24T10:36:49.372713, step: 1541, loss: 1.9902703762054443, acc: 0.9865642786026001, f1: 0.7898, precision: 0.8158, recall: 0.7654\n",
      "2019-01-24T10:36:49.828820, step: 1542, loss: 3.466217517852783, acc: 0.9617677927017212, f1: 0.682, precision: 0.6899, recall: 0.6742\n",
      "2019-01-24T10:36:50.270198, step: 1543, loss: 3.04486083984375, acc: 0.9738868474960327, f1: 0.7322, precision: 0.7283, recall: 0.7363\n",
      "2019-01-24T10:36:50.849984, step: 1544, loss: 3.2956480979919434, acc: 0.970708429813385, f1: 0.6537, precision: 0.6381, recall: 0.67\n",
      "2019-01-24T10:36:51.809485, step: 1545, loss: 2.5082178115844727, acc: 0.9795055389404297, f1: 0.7349, precision: 0.6932, recall: 0.7821\n",
      "2019-01-24T10:36:52.763353, step: 1546, loss: 3.1434695720672607, acc: 0.969344973564148, f1: 0.7487, precision: 0.7216, recall: 0.7778\n",
      "2019-01-24T10:36:53.362717, step: 1547, loss: 2.6389639377593994, acc: 0.9694545269012451, f1: 0.7667, precision: 0.7667, recall: 0.7667\n",
      "2019-01-24T10:36:54.108551, step: 1548, loss: 3.509254217147827, acc: 0.9702746272087097, f1: 0.7378, precision: 0.7281, recall: 0.7477\n",
      "2019-01-24T10:36:54.789896, step: 1549, loss: 2.2685494422912598, acc: 0.9746790528297424, f1: 0.7458, precision: 0.75, recall: 0.7416\n",
      "2019-01-24T10:36:55.303466, step: 1550, loss: 2.727583408355713, acc: 0.9670767188072205, f1: 0.7115, precision: 0.74, recall: 0.6852\n",
      "2019-01-24T10:36:56.236399, step: 1551, loss: 2.1594719886779785, acc: 0.9807930588722229, f1: 0.7807, precision: 0.8022, recall: 0.7604\n",
      "2019-01-24T10:36:56.812644, step: 1552, loss: 2.859099864959717, acc: 0.9646234512329102, f1: 0.6742, precision: 0.7143, recall: 0.6383\n",
      "2019-01-24T10:36:57.487524, step: 1553, loss: 4.576441287994385, acc: 0.9615644216537476, f1: 0.7024, precision: 0.7375, recall: 0.6705\n",
      "2019-01-24T10:36:58.417652, step: 1554, loss: 3.135610580444336, acc: 0.9725860357284546, f1: 0.8154, precision: 0.848, recall: 0.7852\n",
      "2019-01-24T10:36:59.049210, step: 1555, loss: 2.2710766792297363, acc: 0.977096676826477, f1: 0.7643, precision: 0.8, recall: 0.7317\n",
      "2019-01-24T10:36:59.511596, step: 1556, loss: 2.0320985317230225, acc: 0.982450008392334, f1: 0.7816, precision: 0.7816, recall: 0.7816\n",
      "2019-01-24T10:37:00.473127, step: 1557, loss: 2.911713123321533, acc: 0.9740344285964966, f1: 0.7707, precision: 0.7524, recall: 0.79\n",
      "2019-01-24T10:37:01.221836, step: 1558, loss: 2.313596725463867, acc: 0.9808939099311829, f1: 0.7677, precision: 0.76, recall: 0.7755\n",
      "2019-01-24T10:37:02.253294, step: 1559, loss: 3.112431526184082, acc: 0.9708070755004883, f1: 0.7845, precision: 0.7986, recall: 0.7708\n",
      "2019-01-24T10:37:02.982005, step: 1560, loss: 2.5168838500976562, acc: 0.9823856353759766, f1: 0.7981, precision: 0.7905, recall: 0.8058\n",
      "2019-01-24T10:37:03.532803, step: 1561, loss: 3.6603031158447266, acc: 0.9659643173217773, f1: 0.75, precision: 0.7864, recall: 0.7168\n",
      "2019-01-24T10:37:04.840389, step: 1562, loss: 2.6220993995666504, acc: 0.9715948104858398, f1: 0.7343, precision: 0.7451, recall: 0.7238\n",
      "2019-01-24T10:37:05.448677, step: 1563, loss: 2.677060127258301, acc: 0.9729815721511841, f1: 0.7772, precision: 0.7979, recall: 0.7576\n",
      "2019-01-24T10:37:05.986215, step: 1564, loss: 2.9408371448516846, acc: 0.9752259850502014, f1: 0.7805, precision: 0.8081, recall: 0.7547\n",
      "2019-01-24T10:37:06.542407, step: 1565, loss: 2.6565608978271484, acc: 0.9761677980422974, f1: 0.7485, precision: 0.7442, recall: 0.7529\n",
      "2019-01-24T10:37:07.433079, step: 1566, loss: 3.7021799087524414, acc: 0.9635334610939026, f1: 0.7448, precision: 0.7739, recall: 0.7177\n",
      "2019-01-24T10:37:09.564528, step: 1567, loss: 3.6343913078308105, acc: 0.9695924520492554, f1: 0.69, precision: 0.6765, recall: 0.7041\n",
      "2019-01-24T10:37:10.803324, step: 1568, loss: 3.0860648155212402, acc: 0.9759112596511841, f1: 0.8057, precision: 0.8173, recall: 0.7944\n",
      "2019-01-24T10:37:11.538734, step: 1569, loss: 2.4066083431243896, acc: 0.9741735458374023, f1: 0.7826, precision: 0.7864, recall: 0.7788\n",
      "2019-01-24T10:37:12.065845, step: 1570, loss: 2.8947808742523193, acc: 0.9736935496330261, f1: 0.7486, precision: 0.7283, recall: 0.7701\n",
      "2019-01-24T10:37:12.904718, step: 1571, loss: 3.8027455806732178, acc: 0.9700618386268616, f1: 0.6907, precision: 0.6907, recall: 0.6907\n",
      "2019-01-24T10:37:13.616591, step: 1572, loss: 3.461698532104492, acc: 0.9699462056159973, f1: 0.7368, precision: 0.7434, recall: 0.7304\n",
      "2019-01-24T10:37:14.170278, step: 1573, loss: 2.057737350463867, acc: 0.9811579585075378, f1: 0.6777, precision: 0.6833, recall: 0.6721\n",
      "2019-01-24T10:37:14.930962, step: 1574, loss: 2.7364251613616943, acc: 0.9736429452896118, f1: 0.7322, precision: 0.7363, recall: 0.7283\n",
      "2019-01-24T10:37:15.503704, step: 1575, loss: 3.8748621940612793, acc: 0.9670395851135254, f1: 0.7424, precision: 0.7522, recall: 0.7328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:37:16.261826, step: 1576, loss: 3.1322991847991943, acc: 0.9705694317817688, f1: 0.7322, precision: 0.7701, recall: 0.6979\n",
      "2019-01-24T10:37:16.811510, step: 1577, loss: 3.2032456398010254, acc: 0.9685039520263672, f1: 0.7033, precision: 0.7111, recall: 0.6957\n",
      "2019-01-24T10:37:17.895059, step: 1578, loss: 3.00823974609375, acc: 0.9679803252220154, f1: 0.7623, precision: 0.7949, recall: 0.7323\n",
      "2019-01-24T10:37:18.597172, step: 1579, loss: 3.3498194217681885, acc: 0.9701952934265137, f1: 0.7807, precision: 0.7807, recall: 0.7807\n",
      "2019-01-24T10:37:19.069413, step: 1580, loss: 2.8651437759399414, acc: 0.9657720923423767, f1: 0.7447, precision: 0.7527, recall: 0.7368\n",
      "2019-01-24T10:37:20.116693, step: 1581, loss: 3.1974079608917236, acc: 0.9758937954902649, f1: 0.812, precision: 0.8051, recall: 0.819\n",
      "2019-01-24T10:37:20.680993, step: 1582, loss: 2.805400848388672, acc: 0.9758179187774658, f1: 0.7596, precision: 0.7822, recall: 0.7383\n",
      "2019-01-24T10:37:21.197756, step: 1583, loss: 2.66353702545166, acc: 0.9750367403030396, f1: 0.7183, precision: 0.7391, recall: 0.6986\n",
      "2019-01-24T10:37:23.808230, step: 1584, loss: 3.3801932334899902, acc: 0.9723567962646484, f1: 0.8, precision: 0.8254, recall: 0.7761\n",
      "2019-01-24T10:37:24.341618, step: 1585, loss: 3.1184158325195312, acc: 0.9637943506240845, f1: 0.7273, precision: 0.7579, recall: 0.699\n",
      "2019-01-24T10:37:24.875524, step: 1586, loss: 2.15482759475708, acc: 0.9824830889701843, f1: 0.6917, precision: 0.7188, recall: 0.6667\n",
      "2019-01-24T10:37:25.670597, step: 1587, loss: 3.6691174507141113, acc: 0.9669896960258484, f1: 0.6756, precision: 0.7037, recall: 0.6496\n",
      "2019-01-24T10:37:26.292489, step: 1588, loss: 2.2740750312805176, acc: 0.9808010458946228, f1: 0.7374, precision: 0.7253, recall: 0.75\n",
      "2019-01-24T10:37:26.717113, step: 1589, loss: 2.192969799041748, acc: 0.9738021492958069, f1: 0.759, precision: 0.7872, recall: 0.7327\n",
      "2019-01-24T10:37:27.376444, step: 1590, loss: 2.5365078449249268, acc: 0.9750458598136902, f1: 0.7549, precision: 0.7476, recall: 0.7624\n",
      "2019-01-24T10:37:28.634822, step: 1591, loss: 2.6714258193969727, acc: 0.9769060611724854, f1: 0.7766, precision: 0.7766, recall: 0.7766\n",
      "2019-01-24T10:37:29.419411, step: 1592, loss: 2.469027280807495, acc: 0.977863073348999, f1: 0.7636, precision: 0.7975, recall: 0.7326\n",
      "2019-01-24T10:37:30.349613, step: 1593, loss: 2.1636877059936523, acc: 0.9771150946617126, f1: 0.7425, precision: 0.775, recall: 0.7126\n",
      "2019-01-24T10:37:32.753568, step: 1594, loss: 2.8266870975494385, acc: 0.9788933396339417, f1: 0.83, precision: 0.8537, recall: 0.8077\n",
      "2019-01-24T10:37:33.396649, step: 1595, loss: 2.159970283508301, acc: 0.9745790958404541, f1: 0.7574, precision: 0.7711, recall: 0.7442\n",
      "2019-01-24T10:37:34.166923, step: 1596, loss: 2.1765170097351074, acc: 0.976466715335846, f1: 0.8304, precision: 0.8611, recall: 0.8017\n",
      "2019-01-24T10:37:35.868439, step: 1597, loss: 4.347711563110352, acc: 0.9630286693572998, f1: 0.7529, precision: 0.7857, recall: 0.7226\n",
      "2019-01-24T10:37:36.670432, step: 1598, loss: 2.247728109359741, acc: 0.9721545577049255, f1: 0.7135, precision: 0.7097, recall: 0.7174\n",
      "2019-01-24T10:37:37.173303, step: 1599, loss: 3.3272478580474854, acc: 0.9637168049812317, f1: 0.7592, precision: 0.7686, recall: 0.75\n",
      "2019-01-24T10:37:37.919596, step: 1600, loss: 2.502199172973633, acc: 0.9747623801231384, f1: 0.7579, precision: 0.7826, recall: 0.7347\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:38:01.809133, step: 1600, loss: 2.996989220380783, acc:0.9718350287940767, f1: 0.766238888888889, precision: 0.8025083333333332, recall: 0.7337583333333332\n",
      "\n",
      "\n",
      "2019-01-24T10:38:04.093738, step: 1601, loss: 4.545931816101074, acc: 0.9682145118713379, f1: 0.6502, precision: 0.6752, recall: 0.627\n",
      "2019-01-24T10:38:04.792183, step: 1602, loss: 3.140291690826416, acc: 0.97303706407547, f1: 0.7488, precision: 0.7835, recall: 0.717\n",
      "2019-01-24T10:38:05.658017, step: 1603, loss: 2.157968521118164, acc: 0.9800058603286743, f1: 0.7337, precision: 0.7561, recall: 0.7126\n",
      "2019-01-24T10:38:06.357846, step: 1604, loss: 2.8018898963928223, acc: 0.9653695225715637, f1: 0.7263, precision: 0.7263, recall: 0.7263\n",
      "2019-01-24T10:38:07.346742, step: 1605, loss: 2.4441583156585693, acc: 0.9757112860679626, f1: 0.7753, precision: 0.7841, recall: 0.7667\n",
      "2019-01-24T10:38:08.218393, step: 1606, loss: 3.1263771057128906, acc: 0.967961847782135, f1: 0.7232, precision: 0.7442, recall: 0.7033\n",
      "2019-01-24T10:38:08.683826, step: 1607, loss: 2.1242446899414062, acc: 0.9820422530174255, f1: 0.8333, precision: 0.8333, recall: 0.8333\n",
      "2019-01-24T10:38:09.388252, step: 1608, loss: 2.3911571502685547, acc: 0.9750901460647583, f1: 0.7525, precision: 0.7525, recall: 0.7525\n",
      "2019-01-24T10:38:09.874564, step: 1609, loss: 3.3872313499450684, acc: 0.9684028029441833, f1: 0.7374, precision: 0.7604, recall: 0.7157\n",
      "2019-01-24T10:38:10.845912, step: 1610, loss: 2.8409252166748047, acc: 0.9708348512649536, f1: 0.7285, precision: 0.7534, recall: 0.7051\n",
      "2019-01-24T10:38:11.440071, step: 1611, loss: 2.3689005374908447, acc: 0.977642297744751, f1: 0.7545, precision: 0.759, recall: 0.75\n",
      "2019-01-24T10:38:11.975603, step: 1612, loss: 1.3953896760940552, acc: 0.9835285544395447, f1: 0.8221, precision: 0.8375, recall: 0.8072\n",
      "2019-01-24T10:38:12.959927, step: 1613, loss: 2.013880729675293, acc: 0.9816905856132507, f1: 0.7595, precision: 0.7895, recall: 0.7317\n",
      "2019-01-24T10:38:13.663420, step: 1614, loss: 3.2246956825256348, acc: 0.9672897458076477, f1: 0.7208, precision: 0.7553, recall: 0.6893\n",
      "2019-01-24T10:38:14.556428, step: 1615, loss: 2.161687135696411, acc: 0.9803738594055176, f1: 0.8352, precision: 0.8941, recall: 0.7835\n",
      "2019-01-24T10:38:15.113820, step: 1616, loss: 2.9916062355041504, acc: 0.9702141880989075, f1: 0.6575, precision: 0.6857, recall: 0.6316\n",
      "2019-01-24T10:38:15.621108, step: 1617, loss: 2.7783915996551514, acc: 0.9718464612960815, f1: 0.7594, precision: 0.7889, recall: 0.732\n",
      "2019-01-24T10:38:16.277069, step: 1618, loss: 2.9773855209350586, acc: 0.9716053605079651, f1: 0.7945, precision: 0.8208, recall: 0.7699\n",
      "2019-01-24T10:38:16.843854, step: 1619, loss: 2.5173959732055664, acc: 0.9742594957351685, f1: 0.7713, precision: 0.8037, recall: 0.7414\n",
      "2019-01-24T10:38:17.253986, step: 1620, loss: 2.6726486682891846, acc: 0.9719657897949219, f1: 0.7889, precision: 0.8353, recall: 0.7474\n",
      "2019-01-24T10:38:17.853800, step: 1621, loss: 2.1080923080444336, acc: 0.985924482345581, f1: 0.814, precision: 0.8235, recall: 0.8046\n",
      "2019-01-24T10:38:18.622402, step: 1622, loss: 4.097002983093262, acc: 0.95835942029953, f1: 0.5905, precision: 0.6019, recall: 0.5794\n",
      "2019-01-24T10:38:19.523647, step: 1623, loss: 3.093338966369629, acc: 0.9682890772819519, f1: 0.7418, precision: 0.7524, recall: 0.7315\n",
      "2019-01-24T10:38:20.062281, step: 1624, loss: 2.3294968605041504, acc: 0.9796916246414185, f1: 0.7518, precision: 0.7571, recall: 0.7465\n",
      "2019-01-24T10:38:20.934980, step: 1625, loss: 2.1780014038085938, acc: 0.9843102693557739, f1: 0.8387, precision: 0.8764, recall: 0.8041\n",
      "2019-01-24T10:38:21.844434, step: 1626, loss: 1.3851391077041626, acc: 0.9907209277153015, f1: 0.8686, precision: 0.8736, recall: 0.8636\n",
      "2019-01-24T10:38:22.205261, step: 1627, loss: 3.1884193420410156, acc: 0.9697895646095276, f1: 0.7918, precision: 0.822, recall: 0.7638\n",
      "2019-01-24T10:38:22.596423, step: 1628, loss: 2.6495256423950195, acc: 0.9736379384994507, f1: 0.7845, precision: 0.7802, recall: 0.7889\n",
      "2019-01-24T10:38:22.982181, step: 1629, loss: 2.4836652278900146, acc: 0.9727585911750793, f1: 0.7308, precision: 0.7125, recall: 0.75\n",
      "2019-01-24T10:38:23.692120, step: 1630, loss: 2.676056385040283, acc: 0.9771900773048401, f1: 0.7121, precision: 0.7121, recall: 0.7121\n",
      "start training model\n",
      "2019-01-24T10:38:24.250611, step: 1631, loss: 3.187523365020752, acc: 0.9676717519760132, f1: 0.6632, precision: 0.6957, recall: 0.6337\n",
      "2019-01-24T10:38:25.130041, step: 1632, loss: 1.9620782136917114, acc: 0.9847182631492615, f1: 0.8083, precision: 0.8211, recall: 0.7959\n",
      "2019-01-24T10:38:25.760166, step: 1633, loss: 2.333552360534668, acc: 0.9720830321311951, f1: 0.7579, precision: 0.7826, recall: 0.7347\n",
      "2019-01-24T10:38:26.364639, step: 1634, loss: 2.126711845397949, acc: 0.9740128517150879, f1: 0.789, precision: 0.7679, recall: 0.8113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:38:26.860964, step: 1635, loss: 2.087620258331299, acc: 0.9749364256858826, f1: 0.656, precision: 0.6613, recall: 0.6508\n",
      "2019-01-24T10:38:27.343089, step: 1636, loss: 2.081826686859131, acc: 0.9829989671707153, f1: 0.8317, precision: 0.866, recall: 0.8\n",
      "2019-01-24T10:38:27.885329, step: 1637, loss: 2.042146682739258, acc: 0.984375, f1: 0.8, precision: 0.8046, recall: 0.7955\n",
      "2019-01-24T10:38:28.770813, step: 1638, loss: 2.2281126976013184, acc: 0.9786259531974792, f1: 0.6861, precision: 0.7344, recall: 0.6438\n",
      "2019-01-24T10:38:29.273219, step: 1639, loss: 1.9346365928649902, acc: 0.9824561476707458, f1: 0.8597, precision: 0.8407, recall: 0.8796\n",
      "2019-01-24T10:38:29.764026, step: 1640, loss: 1.4889405965805054, acc: 0.9891945123672485, f1: 0.8217, precision: 0.8413, recall: 0.803\n",
      "2019-01-24T10:38:30.746900, step: 1641, loss: 1.6003960371017456, acc: 0.9864524006843567, f1: 0.8402, precision: 0.8765, recall: 0.8068\n",
      "2019-01-24T10:38:31.325583, step: 1642, loss: 2.0237784385681152, acc: 0.9819962382316589, f1: 0.8517, precision: 0.875, recall: 0.8296\n",
      "2019-01-24T10:38:31.858359, step: 1643, loss: 2.1786603927612305, acc: 0.9773361086845398, f1: 0.7909, precision: 0.8131, recall: 0.7699\n",
      "2019-01-24T10:38:32.763194, step: 1644, loss: 2.9591805934906006, acc: 0.9707140326499939, f1: 0.7273, precision: 0.7347, recall: 0.72\n",
      "2019-01-24T10:38:33.160049, step: 1645, loss: 1.8458715677261353, acc: 0.9776785969734192, f1: 0.7778, precision: 0.8358, recall: 0.7273\n",
      "2019-01-24T10:38:34.138757, step: 1646, loss: 2.3535399436950684, acc: 0.9715909361839294, f1: 0.7512, precision: 0.7938, recall: 0.713\n",
      "2019-01-24T10:38:35.248138, step: 1647, loss: 1.829438328742981, acc: 0.9846009016036987, f1: 0.8387, precision: 0.8505, recall: 0.8273\n",
      "2019-01-24T10:38:35.739628, step: 1648, loss: 1.8704406023025513, acc: 0.9853256940841675, f1: 0.7848, precision: 0.775, recall: 0.7949\n",
      "2019-01-24T10:38:37.361126, step: 1649, loss: 6.0758466720581055, acc: 0.9558196067810059, f1: 0.7016, precision: 0.6569, recall: 0.7528\n",
      "2019-01-24T10:38:37.911741, step: 1650, loss: 2.013692855834961, acc: 0.9720084071159363, f1: 0.7101, precision: 0.6897, recall: 0.7317\n",
      "2019-01-24T10:38:38.555259, step: 1651, loss: 1.9241623878479004, acc: 0.9874039888381958, f1: 0.8144, precision: 0.8095, recall: 0.8193\n",
      "2019-01-24T10:38:39.358244, step: 1652, loss: 1.6933660507202148, acc: 0.9860116243362427, f1: 0.8263, precision: 0.8313, recall: 0.8214\n",
      "2019-01-24T10:38:40.190604, step: 1653, loss: 2.4012367725372314, acc: 0.9782390594482422, f1: 0.79, precision: 0.8229, recall: 0.7596\n",
      "2019-01-24T10:38:40.952998, step: 1654, loss: 2.064534902572632, acc: 0.9793078899383545, f1: 0.7705, precision: 0.7705, recall: 0.7705\n",
      "2019-01-24T10:38:41.439116, step: 1655, loss: 1.8863310813903809, acc: 0.9799150228500366, f1: 0.8276, precision: 0.8451, recall: 0.8108\n",
      "2019-01-24T10:38:42.223865, step: 1656, loss: 2.419839859008789, acc: 0.9780998229980469, f1: 0.7511, precision: 0.7672, recall: 0.7355\n",
      "2019-01-24T10:38:44.852551, step: 1657, loss: 2.37117862701416, acc: 0.9804184436798096, f1: 0.8478, precision: 0.8525, recall: 0.8432\n",
      "2019-01-24T10:38:45.413250, step: 1658, loss: 2.413327217102051, acc: 0.9714285731315613, f1: 0.7586, precision: 0.7778, recall: 0.7404\n",
      "2019-01-24T10:38:46.264761, step: 1659, loss: 2.2389564514160156, acc: 0.9791946411132812, f1: 0.7683, precision: 0.7778, recall: 0.759\n",
      "2019-01-24T10:38:46.921715, step: 1660, loss: 2.6232335567474365, acc: 0.9743124842643738, f1: 0.6667, precision: 0.6951, recall: 0.6404\n",
      "2019-01-24T10:38:47.455647, step: 1661, loss: 2.5489501953125, acc: 0.9701818227767944, f1: 0.7094, precision: 0.6923, recall: 0.7273\n",
      "2019-01-24T10:38:48.064235, step: 1662, loss: 2.3508524894714355, acc: 0.977412760257721, f1: 0.7674, precision: 0.7952, recall: 0.7416\n",
      "2019-01-24T10:38:48.900769, step: 1663, loss: 2.8247175216674805, acc: 0.9694041609764099, f1: 0.6972, precision: 0.7451, recall: 0.6552\n",
      "2019-01-24T10:38:49.498853, step: 1664, loss: 1.4700472354888916, acc: 0.9880815148353577, f1: 0.8649, precision: 0.8602, recall: 0.8696\n",
      "2019-01-24T10:38:49.975550, step: 1665, loss: 2.7272207736968994, acc: 0.9690759778022766, f1: 0.7538, precision: 0.7653, recall: 0.7426\n",
      "2019-01-24T10:38:50.886052, step: 1666, loss: 2.3966221809387207, acc: 0.9653767943382263, f1: 0.6885, precision: 0.7241, recall: 0.6562\n",
      "2019-01-24T10:38:51.514471, step: 1667, loss: 2.5289130210876465, acc: 0.9802259802818298, f1: 0.7848, precision: 0.7881, recall: 0.7815\n",
      "2019-01-24T10:38:54.020608, step: 1668, loss: 1.76252281665802, acc: 0.9879240989685059, f1: 0.8629, precision: 0.856, recall: 0.8699\n",
      "2019-01-24T10:38:54.551675, step: 1669, loss: 3.070746421813965, acc: 0.9696469902992249, f1: 0.7923, precision: 0.811, recall: 0.7744\n",
      "2019-01-24T10:38:55.146871, step: 1670, loss: 1.8665823936462402, acc: 0.9799859523773193, f1: 0.8072, precision: 0.8171, recall: 0.7976\n",
      "2019-01-24T10:38:55.974836, step: 1671, loss: 2.683413028717041, acc: 0.9708196520805359, f1: 0.7512, precision: 0.77, recall: 0.7333\n",
      "2019-01-24T10:38:58.590382, step: 1672, loss: 2.7519304752349854, acc: 0.9748942852020264, f1: 0.7771, precision: 0.7821, recall: 0.7722\n",
      "2019-01-24T10:38:59.111184, step: 1673, loss: 2.1037087440490723, acc: 0.9763001799583435, f1: 0.776, precision: 0.8256, recall: 0.732\n",
      "2019-01-24T10:38:59.965614, step: 1674, loss: 3.10239839553833, acc: 0.9689344763755798, f1: 0.7157, precision: 0.7019, recall: 0.73\n",
      "2019-01-24T10:39:00.722727, step: 1675, loss: 1.9084104299545288, acc: 0.9818621277809143, f1: 0.7879, precision: 0.7959, recall: 0.78\n",
      "2019-01-24T10:39:01.588979, step: 1676, loss: 2.914273262023926, acc: 0.9705492854118347, f1: 0.7024, precision: 0.7059, recall: 0.699\n",
      "2019-01-24T10:39:02.296737, step: 1677, loss: 2.412019968032837, acc: 0.9822021126747131, f1: 0.8037, precision: 0.8224, recall: 0.7857\n",
      "2019-01-24T10:39:03.053488, step: 1678, loss: 1.576733112335205, acc: 0.988231360912323, f1: 0.8523, precision: 0.8523, recall: 0.8523\n",
      "2019-01-24T10:39:03.876095, step: 1679, loss: 2.252272367477417, acc: 0.9806022047996521, f1: 0.7709, precision: 0.7667, recall: 0.7753\n",
      "2019-01-24T10:39:04.291261, step: 1680, loss: 2.771116256713867, acc: 0.9694384932518005, f1: 0.7159, precision: 0.7412, recall: 0.6923\n",
      "2019-01-24T10:39:05.099606, step: 1681, loss: 3.138444423675537, acc: 0.9723487496376038, f1: 0.7815, precision: 0.7815, recall: 0.7815\n",
      "2019-01-24T10:39:06.098450, step: 1682, loss: 1.7890448570251465, acc: 0.9841803908348083, f1: 0.8, precision: 0.8434, recall: 0.7609\n",
      "2019-01-24T10:39:07.084753, step: 1683, loss: 2.3747851848602295, acc: 0.9806427955627441, f1: 0.8583, precision: 0.8957, recall: 0.824\n",
      "2019-01-24T10:39:07.715999, step: 1684, loss: 3.848140239715576, acc: 0.9671052694320679, f1: 0.7889, precision: 0.7972, recall: 0.7808\n",
      "2019-01-24T10:39:08.212293, step: 1685, loss: 1.9845154285430908, acc: 0.9808095097541809, f1: 0.8229, precision: 0.8182, recall: 0.8276\n",
      "2019-01-24T10:39:08.842558, step: 1686, loss: 2.5077576637268066, acc: 0.9817131757736206, f1: 0.7521, precision: 0.7652, recall: 0.7395\n",
      "2019-01-24T10:39:09.657089, step: 1687, loss: 3.8751473426818848, acc: 0.9707543849945068, f1: 0.7893, precision: 0.8047, recall: 0.7744\n",
      "2019-01-24T10:39:10.658850, step: 1688, loss: 1.8871009349822998, acc: 0.9833388924598694, f1: 0.7586, precision: 0.75, recall: 0.7674\n",
      "2019-01-24T10:39:11.164044, step: 1689, loss: 1.9937338829040527, acc: 0.9859062433242798, f1: 0.8488, precision: 0.8447, recall: 0.8529\n",
      "2019-01-24T10:39:11.977908, step: 1690, loss: 2.7826461791992188, acc: 0.9818238615989685, f1: 0.786, precision: 0.7953, recall: 0.7769\n",
      "2019-01-24T10:39:12.564179, step: 1691, loss: 1.7604610919952393, acc: 0.9811074733734131, f1: 0.7582, precision: 0.7753, recall: 0.7419\n",
      "2019-01-24T10:39:13.056162, step: 1692, loss: 2.0931458473205566, acc: 0.9759217500686646, f1: 0.8398, precision: 0.8352, recall: 0.8444\n",
      "2019-01-24T10:39:13.519921, step: 1693, loss: 1.8023149967193604, acc: 0.9853691458702087, f1: 0.8601, precision: 0.883, recall: 0.8384\n",
      "2019-01-24T10:39:14.510075, step: 1694, loss: 2.7043089866638184, acc: 0.9738842844963074, f1: 0.7463, precision: 0.7576, recall: 0.7353\n",
      "2019-01-24T10:39:15.173831, step: 1695, loss: 2.0774683952331543, acc: 0.9794496893882751, f1: 0.7558, precision: 0.7647, recall: 0.7471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:39:15.869422, step: 1696, loss: 2.2335093021392822, acc: 0.9803125262260437, f1: 0.8098, precision: 0.8218, recall: 0.7981\n",
      "2019-01-24T10:39:16.489237, step: 1697, loss: 1.7879674434661865, acc: 0.9828340411186218, f1: 0.8024, precision: 0.8272, recall: 0.7791\n",
      "2019-01-24T10:39:17.189853, step: 1698, loss: 2.8321709632873535, acc: 0.9758298397064209, f1: 0.7222, precision: 0.7647, recall: 0.6842\n",
      "2019-01-24T10:39:18.413231, step: 1699, loss: 2.791677713394165, acc: 0.9818748235702515, f1: 0.7983, precision: 0.8304, recall: 0.7686\n",
      "2019-01-24T10:39:19.016261, step: 1700, loss: 2.6914093494415283, acc: 0.9627659320831299, f1: 0.77, precision: 0.781, recall: 0.7593\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:39:43.858752, step: 1700, loss: 2.83412793940968, acc:0.9724232041173511, f1: 0.7687749999999999, precision: 0.790661111111111, recall: 0.7486638888888889\n",
      "\n",
      "\n",
      "2019-01-24T10:39:44.374234, step: 1701, loss: 2.918635368347168, acc: 0.964091420173645, f1: 0.7297, precision: 0.7431, recall: 0.7168\n",
      "2019-01-24T10:39:47.134681, step: 1702, loss: 2.233095645904541, acc: 0.9833201169967651, f1: 0.8079, precision: 0.8079, recall: 0.8079\n",
      "2019-01-24T10:39:47.974462, step: 1703, loss: 1.5400828123092651, acc: 0.9847870469093323, f1: 0.7712, precision: 0.7662, recall: 0.7763\n",
      "2019-01-24T10:39:48.983735, step: 1704, loss: 2.76019549369812, acc: 0.9724233150482178, f1: 0.7264, precision: 0.6952, recall: 0.7604\n",
      "2019-01-24T10:39:50.372441, step: 1705, loss: 2.4778409004211426, acc: 0.9801653027534485, f1: 0.8093, precision: 0.8131, recall: 0.8056\n",
      "2019-01-24T10:39:50.962580, step: 1706, loss: 1.9648804664611816, acc: 0.9830208420753479, f1: 0.8182, precision: 0.8372, recall: 0.8\n",
      "2019-01-24T10:39:51.600487, step: 1707, loss: 1.6217650175094604, acc: 0.9845689535140991, f1: 0.8488, precision: 0.8391, recall: 0.8588\n",
      "2019-01-24T10:39:52.298946, step: 1708, loss: 2.3978145122528076, acc: 0.973648190498352, f1: 0.7767, precision: 0.7843, recall: 0.7692\n",
      "2019-01-24T10:39:53.068786, step: 1709, loss: 2.9234561920166016, acc: 0.9725590348243713, f1: 0.764, precision: 0.7556, recall: 0.7727\n",
      "2019-01-24T10:39:53.717359, step: 1710, loss: 2.697214126586914, acc: 0.972296416759491, f1: 0.7716, precision: 0.8172, recall: 0.7308\n",
      "2019-01-24T10:39:54.441645, step: 1711, loss: 2.7134482860565186, acc: 0.9742587208747864, f1: 0.715, precision: 0.7419, recall: 0.69\n",
      "2019-01-24T10:39:54.924605, step: 1712, loss: 1.9889055490493774, acc: 0.9737729430198669, f1: 0.7182, precision: 0.7303, recall: 0.7065\n",
      "2019-01-24T10:39:55.555971, step: 1713, loss: 1.9115463495254517, acc: 0.9799330830574036, f1: 0.8155, precision: 0.8317, recall: 0.8\n",
      "2019-01-24T10:39:56.438050, step: 1714, loss: 1.8869832754135132, acc: 0.98531574010849, f1: 0.8478, precision: 0.8298, recall: 0.8667\n",
      "2019-01-24T10:39:57.069612, step: 1715, loss: 2.151585578918457, acc: 0.9820119738578796, f1: 0.8491, precision: 0.8571, recall: 0.8411\n",
      "2019-01-24T10:39:57.869567, step: 1716, loss: 1.7359251976013184, acc: 0.9838761687278748, f1: 0.7933, precision: 0.7889, recall: 0.7978\n",
      "2019-01-24T10:39:59.527623, step: 1717, loss: 2.217311143875122, acc: 0.9854742288589478, f1: 0.8137, precision: 0.8058, recall: 0.8218\n",
      "2019-01-24T10:40:00.126530, step: 1718, loss: 2.8750035762786865, acc: 0.9723137617111206, f1: 0.7059, precision: 0.7333, recall: 0.6804\n",
      "2019-01-24T10:40:02.515366, step: 1719, loss: 2.2565383911132812, acc: 0.9847021102905273, f1: 0.8417, precision: 0.8478, recall: 0.8357\n",
      "2019-01-24T10:40:03.060571, step: 1720, loss: 2.0642919540405273, acc: 0.9759330153465271, f1: 0.7753, precision: 0.7719, recall: 0.7788\n",
      "2019-01-24T10:40:04.320689, step: 1721, loss: 2.1230149269104004, acc: 0.9797492623329163, f1: 0.8054, precision: 0.8333, recall: 0.7792\n",
      "2019-01-24T10:40:04.913739, step: 1722, loss: 2.1018905639648438, acc: 0.9778820276260376, f1: 0.7638, precision: 0.7677, recall: 0.76\n",
      "2019-01-24T10:40:05.843009, step: 1723, loss: 2.1846272945404053, acc: 0.9786024689674377, f1: 0.7979, precision: 0.7938, recall: 0.8021\n",
      "2019-01-24T10:40:07.194099, step: 1724, loss: 2.4461591243743896, acc: 0.980322003364563, f1: 0.7837, precision: 0.8, recall: 0.768\n",
      "2019-01-24T10:40:07.899445, step: 1725, loss: 2.550966262817383, acc: 0.9773597121238708, f1: 0.8348, precision: 0.8496, recall: 0.8205\n",
      "2019-01-24T10:40:08.555596, step: 1726, loss: 1.6883177757263184, acc: 0.9825124740600586, f1: 0.7919, precision: 0.8194, recall: 0.7662\n",
      "2019-01-24T10:40:09.581592, step: 1727, loss: 2.6749491691589355, acc: 0.9758620858192444, f1: 0.7326, precision: 0.75, recall: 0.7159\n",
      "2019-01-24T10:40:10.517515, step: 1728, loss: 2.2300987243652344, acc: 0.9791530966758728, f1: 0.7963, precision: 0.7963, recall: 0.7963\n",
      "2019-01-24T10:40:11.150201, step: 1729, loss: 1.9818060398101807, acc: 0.9813409447669983, f1: 0.8629, precision: 0.8763, recall: 0.85\n",
      "2019-01-24T10:40:11.549139, step: 1730, loss: 1.872711181640625, acc: 0.9796863794326782, f1: 0.7347, precision: 0.7397, recall: 0.7297\n",
      "2019-01-24T10:40:12.268341, step: 1731, loss: 2.8219642639160156, acc: 0.9747317433357239, f1: 0.6806, precision: 0.6842, recall: 0.6771\n",
      "2019-01-24T10:40:12.711413, step: 1732, loss: 2.3013083934783936, acc: 0.9791511297225952, f1: 0.8191, precision: 0.8462, recall: 0.7938\n",
      "2019-01-24T10:40:13.502676, step: 1733, loss: 2.55399227142334, acc: 0.9748543500900269, f1: 0.7432, precision: 0.7312, recall: 0.7556\n",
      "2019-01-24T10:40:14.507387, step: 1734, loss: 1.9567723274230957, acc: 0.9845407605171204, f1: 0.8544, precision: 0.8544, recall: 0.8544\n",
      "2019-01-24T10:40:14.734926, step: 1735, loss: 2.908082962036133, acc: 0.9637518525123596, f1: 0.7207, precision: 0.7273, recall: 0.7143\n",
      "2019-01-24T10:40:15.238069, step: 1736, loss: 2.7528581619262695, acc: 0.976401150226593, f1: 0.8072, precision: 0.8491, recall: 0.7692\n",
      "2019-01-24T10:40:15.859851, step: 1737, loss: 2.84238338470459, acc: 0.9762773513793945, f1: 0.7614, precision: 0.7979, recall: 0.7282\n",
      "2019-01-24T10:40:16.667568, step: 1738, loss: 2.1409244537353516, acc: 0.9811211228370667, f1: 0.8273, precision: 0.875, recall: 0.7845\n",
      "2019-01-24T10:40:17.465188, step: 1739, loss: 2.907074451446533, acc: 0.9691458344459534, f1: 0.6947, precision: 0.7174, recall: 0.6735\n",
      "2019-01-24T10:40:18.477729, step: 1740, loss: 2.598853349685669, acc: 0.9765488505363464, f1: 0.8058, precision: 0.8218, recall: 0.7905\n",
      "2019-01-24T10:40:19.724458, step: 1741, loss: 2.0365633964538574, acc: 0.985097348690033, f1: 0.8148, precision: 0.8354, recall: 0.7952\n",
      "2019-01-24T10:40:20.270265, step: 1742, loss: 1.5845129489898682, acc: 0.9841441512107849, f1: 0.8652, precision: 0.8851, recall: 0.8462\n",
      "2019-01-24T10:40:20.773360, step: 1743, loss: 2.0439224243164062, acc: 0.980400025844574, f1: 0.8024, precision: 0.8072, recall: 0.7976\n",
      "2019-01-24T10:40:21.349189, step: 1744, loss: 2.241272449493408, acc: 0.9743970036506653, f1: 0.7657, precision: 0.7701, recall: 0.7614\n",
      "2019-01-24T10:40:22.403551, step: 1745, loss: 2.297954797744751, acc: 0.9796454310417175, f1: 0.8282, precision: 0.8319, recall: 0.8246\n",
      "2019-01-24T10:40:23.343453, step: 1746, loss: 1.8350461721420288, acc: 0.9821244478225708, f1: 0.8343, precision: 0.8295, recall: 0.8391\n",
      "2019-01-24T10:40:23.924814, step: 1747, loss: 2.1552844047546387, acc: 0.9825616478919983, f1: 0.8396, precision: 0.8396, recall: 0.8396\n",
      "2019-01-24T10:40:24.639219, step: 1748, loss: 2.9280402660369873, acc: 0.9743760228157043, f1: 0.7006, precision: 0.7561, recall: 0.6526\n",
      "2019-01-24T10:40:25.170704, step: 1749, loss: 1.8779138326644897, acc: 0.9746468663215637, f1: 0.7853, precision: 0.8101, recall: 0.7619\n",
      "2019-01-24T10:40:25.802078, step: 1750, loss: 3.1811234951019287, acc: 0.9734513163566589, f1: 0.7917, precision: 0.812, recall: 0.7724\n",
      "2019-01-24T10:40:26.416545, step: 1751, loss: 2.5924224853515625, acc: 0.9683898687362671, f1: 0.7589, precision: 0.7522, recall: 0.7658\n",
      "2019-01-24T10:40:27.622569, step: 1752, loss: 2.2404439449310303, acc: 0.9769322872161865, f1: 0.806, precision: 0.8351, recall: 0.7788\n",
      "2019-01-24T10:40:28.363413, step: 1753, loss: 2.3917088508605957, acc: 0.9742090702056885, f1: 0.7393, precision: 0.7573, recall: 0.7222\n",
      "2019-01-24T10:40:29.004813, step: 1754, loss: 2.7867465019226074, acc: 0.9719842076301575, f1: 0.7006, precision: 0.7381, recall: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:40:29.635280, step: 1755, loss: 3.211209774017334, acc: 0.9737258553504944, f1: 0.7435, precision: 0.7717, recall: 0.7172\n",
      "2019-01-24T10:40:30.514790, step: 1756, loss: 2.9342668056488037, acc: 0.9726890921592712, f1: 0.757, precision: 0.757, recall: 0.757\n",
      "2019-01-24T10:40:31.423187, step: 1757, loss: 1.9588146209716797, acc: 0.979126513004303, f1: 0.8085, precision: 0.8261, recall: 0.7917\n",
      "2019-01-24T10:40:32.155821, step: 1758, loss: 1.4291329383850098, acc: 0.9886363744735718, f1: 0.8378, precision: 0.8378, recall: 0.8378\n",
      "2019-01-24T10:40:32.924451, step: 1759, loss: 2.5608949661254883, acc: 0.9741322994232178, f1: 0.79, precision: 0.79, recall: 0.79\n",
      "2019-01-24T10:40:33.805032, step: 1760, loss: 2.1234183311462402, acc: 0.9793035387992859, f1: 0.7707, precision: 0.7745, recall: 0.767\n",
      "2019-01-24T10:40:34.505070, step: 1761, loss: 2.376509666442871, acc: 0.9731733202934265, f1: 0.7273, precision: 0.7423, recall: 0.7129\n",
      "2019-01-24T10:40:35.024314, step: 1762, loss: 1.9896796941757202, acc: 0.9719311594963074, f1: 0.6623, precision: 0.6579, recall: 0.6667\n",
      "2019-01-24T10:40:35.599561, step: 1763, loss: 1.6772105693817139, acc: 0.9838337302207947, f1: 0.7451, precision: 0.75, recall: 0.7403\n",
      "2019-01-24T10:40:36.139358, step: 1764, loss: 3.5089988708496094, acc: 0.9679999947547913, f1: 0.7128, precision: 0.7283, recall: 0.6979\n",
      "2019-01-24T10:40:36.744119, step: 1765, loss: 2.540299892425537, acc: 0.9739999771118164, f1: 0.7151, precision: 0.7191, recall: 0.7111\n",
      "2019-01-24T10:40:37.512914, step: 1766, loss: 3.2793095111846924, acc: 0.9715200066566467, f1: 0.7103, precision: 0.7103, recall: 0.7103\n",
      "2019-01-24T10:40:37.955758, step: 1767, loss: 1.3892408609390259, acc: 0.9833997488021851, f1: 0.7901, precision: 0.8, recall: 0.7805\n",
      "2019-01-24T10:40:38.912375, step: 1768, loss: 2.34969162940979, acc: 0.9742075800895691, f1: 0.7745, precision: 0.7745, recall: 0.7745\n",
      "2019-01-24T10:40:39.596275, step: 1769, loss: 1.7406456470489502, acc: 0.9801946878433228, f1: 0.8195, precision: 0.8317, recall: 0.8077\n",
      "2019-01-24T10:40:40.243957, step: 1770, loss: 2.215039014816284, acc: 0.9761738181114197, f1: 0.7805, precision: 0.7767, recall: 0.7843\n",
      "2019-01-24T10:40:40.765368, step: 1771, loss: 2.9895710945129395, acc: 0.9698290228843689, f1: 0.6633, precision: 0.6735, recall: 0.6535\n",
      "2019-01-24T10:40:41.285024, step: 1772, loss: 2.663865566253662, acc: 0.9693500995635986, f1: 0.6769, precision: 0.6804, recall: 0.6735\n",
      "2019-01-24T10:40:41.805829, step: 1773, loss: 2.1337199211120605, acc: 0.9808379411697388, f1: 0.7821, precision: 0.7955, recall: 0.7692\n",
      "2019-01-24T10:40:42.803494, step: 1774, loss: 2.3081982135772705, acc: 0.9774696826934814, f1: 0.6667, precision: 0.6825, recall: 0.6515\n",
      "2019-01-24T10:40:43.725993, step: 1775, loss: 1.63814377784729, acc: 0.9902818202972412, f1: 0.8843, precision: 0.8992, recall: 0.8699\n",
      "2019-01-24T10:40:44.379564, step: 1776, loss: 1.560085415840149, acc: 0.9838607311248779, f1: 0.837, precision: 0.837, recall: 0.837\n",
      "2019-01-24T10:40:44.935975, step: 1777, loss: 2.109405755996704, acc: 0.9800608158111572, f1: 0.8168, precision: 0.8387, recall: 0.7959\n",
      "2019-01-24T10:40:45.869291, step: 1778, loss: 2.505406618118286, acc: 0.9765676856040955, f1: 0.7958, precision: 0.8172, recall: 0.7755\n",
      "2019-01-24T10:40:46.377659, step: 1779, loss: 2.4776792526245117, acc: 0.9758599996566772, f1: 0.7411, precision: 0.73, recall: 0.7526\n",
      "2019-01-24T10:40:46.988515, step: 1780, loss: 2.001091480255127, acc: 0.983668327331543, f1: 0.7865, precision: 0.8235, recall: 0.7527\n",
      "2019-01-24T10:40:47.693389, step: 1781, loss: 2.3654003143310547, acc: 0.9792836904525757, f1: 0.7685, precision: 0.7981, recall: 0.7411\n",
      "2019-01-24T10:40:48.496101, step: 1782, loss: 2.6088132858276367, acc: 0.9798746109008789, f1: 0.7536, precision: 0.78, recall: 0.729\n",
      "2019-01-24T10:40:48.985627, step: 1783, loss: 2.2277116775512695, acc: 0.9736841917037964, f1: 0.7578, precision: 0.7722, recall: 0.7439\n",
      "2019-01-24T10:40:49.612362, step: 1784, loss: 2.1268625259399414, acc: 0.981528639793396, f1: 0.8387, precision: 0.8667, recall: 0.8125\n",
      "2019-01-24T10:40:50.496444, step: 1785, loss: 2.613832712173462, acc: 0.9776380062103271, f1: 0.8368, precision: 0.8475, recall: 0.8264\n",
      "2019-01-24T10:40:52.553862, step: 1786, loss: 2.4263479709625244, acc: 0.9808459877967834, f1: 0.7948, precision: 0.8053, recall: 0.7845\n",
      "2019-01-24T10:40:53.344255, step: 1787, loss: 1.9559876918792725, acc: 0.9828326106071472, f1: 0.8075, precision: 0.8333, recall: 0.7831\n",
      "2019-01-24T10:40:54.618028, step: 1788, loss: 2.7851099967956543, acc: 0.9743138551712036, f1: 0.7965, precision: 0.8036, recall: 0.7895\n",
      "2019-01-24T10:40:55.188818, step: 1789, loss: 3.215055465698242, acc: 0.9707759618759155, f1: 0.7969, precision: 0.816, recall: 0.7786\n",
      "2019-01-24T10:40:55.740957, step: 1790, loss: 2.5550975799560547, acc: 0.9638386368751526, f1: 0.6171, precision: 0.6207, recall: 0.6136\n",
      "2019-01-24T10:40:56.535647, step: 1791, loss: 1.9447975158691406, acc: 0.9794908165931702, f1: 0.6047, precision: 0.5821, recall: 0.629\n",
      "2019-01-24T10:40:57.364833, step: 1792, loss: 2.021519660949707, acc: 0.9849079251289368, f1: 0.8482, precision: 0.8526, recall: 0.8438\n",
      "2019-01-24T10:40:58.030011, step: 1793, loss: 1.992324948310852, acc: 0.9813641309738159, f1: 0.8242, precision: 0.8395, recall: 0.8095\n",
      "2019-01-24T10:40:58.626699, step: 1794, loss: 1.776520013809204, acc: 0.9792592525482178, f1: 0.766, precision: 0.806, recall: 0.7297\n",
      "2019-01-24T10:40:59.276696, step: 1795, loss: 2.3126533031463623, acc: 0.9817054271697998, f1: 0.8769, precision: 0.8837, recall: 0.8702\n",
      "2019-01-24T10:41:00.212469, step: 1796, loss: 2.4498844146728516, acc: 0.9726848602294922, f1: 0.7077, precision: 0.734, recall: 0.6832\n",
      "2019-01-24T10:41:00.770473, step: 1797, loss: 2.3764424324035645, acc: 0.9695994257926941, f1: 0.7556, precision: 0.7522, recall: 0.7589\n",
      "2019-01-24T10:41:01.461317, step: 1798, loss: 2.829011917114258, acc: 0.9693160653114319, f1: 0.6585, precision: 0.6667, recall: 0.6506\n",
      "2019-01-24T10:41:02.257121, step: 1799, loss: 2.0125489234924316, acc: 0.974520206451416, f1: 0.7407, precision: 0.7407, recall: 0.7407\n",
      "2019-01-24T10:41:02.900549, step: 1800, loss: 2.2675459384918213, acc: 0.9793199896812439, f1: 0.7937, precision: 0.8427, recall: 0.75\n",
      "\n",
      "Evaluation:\n",
      "2019-01-24T10:41:27.908533, step: 1800, loss: 2.878213004933463, acc:0.9725347128179338, f1: 0.7724222222222223, precision: 0.8128777777777777, recall: 0.7368333333333335\n",
      "\n",
      "\n",
      "2019-01-24T10:41:28.635957, step: 1801, loss: 2.772397041320801, acc: 0.9733210802078247, f1: 0.7631, precision: 0.7787, recall: 0.748\n",
      "2019-01-24T10:41:29.222876, step: 1802, loss: 1.807077169418335, acc: 0.9806950092315674, f1: 0.8471, precision: 0.8471, recall: 0.8471\n",
      "2019-01-24T10:41:29.801611, step: 1803, loss: 2.966671943664551, acc: 0.975252091884613, f1: 0.71, precision: 0.732, recall: 0.6893\n",
      "2019-01-24T10:41:30.444372, step: 1804, loss: 1.814417839050293, acc: 0.9794835448265076, f1: 0.7439, precision: 0.7262, recall: 0.7625\n",
      "2019-01-24T10:41:31.136696, step: 1805, loss: 1.840145230293274, acc: 0.9838165640830994, f1: 0.7845, precision: 0.7889, recall: 0.7802\n",
      "2019-01-24T10:41:31.714848, step: 1806, loss: 2.8361001014709473, acc: 0.9640015363693237, f1: 0.7282, precision: 0.7396, recall: 0.7172\n",
      "2019-01-24T10:41:32.605970, step: 1807, loss: 2.014352798461914, acc: 0.9822361469268799, f1: 0.8165, precision: 0.8091, recall: 0.8241\n",
      "2019-01-24T10:41:33.365222, step: 1808, loss: 1.8300809860229492, acc: 0.9829999804496765, f1: 0.8293, precision: 0.8095, recall: 0.85\n",
      "2019-01-24T10:41:33.989558, step: 1809, loss: 3.121051788330078, acc: 0.9698776006698608, f1: 0.7193, precision: 0.7523, recall: 0.6891\n",
      "2019-01-24T10:41:34.492730, step: 1810, loss: 2.6091084480285645, acc: 0.9640029072761536, f1: 0.7685, precision: 0.7757, recall: 0.7615\n",
      "2019-01-24T10:41:35.047585, step: 1811, loss: 1.9428749084472656, acc: 0.9798722267150879, f1: 0.7753, precision: 0.8023, recall: 0.75\n",
      "2019-01-24T10:41:35.724766, step: 1812, loss: 1.2356109619140625, acc: 0.9866806864738464, f1: 0.8214, precision: 0.8118, recall: 0.8313\n",
      "2019-01-24T10:41:36.242047, step: 1813, loss: 1.5826239585876465, acc: 0.9803030490875244, f1: 0.7778, precision: 0.8235, recall: 0.7368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:41:37.120372, step: 1814, loss: 2.458761692047119, acc: 0.9763978719711304, f1: 0.784, precision: 0.8033, recall: 0.7656\n",
      "2019-01-24T10:41:37.653462, step: 1815, loss: 1.6837632656097412, acc: 0.984455943107605, f1: 0.8191, precision: 0.8105, recall: 0.828\n",
      "2019-01-24T10:41:38.467457, step: 1816, loss: 2.260334014892578, acc: 0.9768258333206177, f1: 0.7407, precision: 0.7609, recall: 0.7216\n",
      "2019-01-24T10:41:39.479182, step: 1817, loss: 1.6416460275650024, acc: 0.9810725450515747, f1: 0.8621, precision: 0.9036, recall: 0.8242\n",
      "2019-01-24T10:41:40.219437, step: 1818, loss: 1.5171003341674805, acc: 0.9886121153831482, f1: 0.8673, precision: 0.8854, recall: 0.85\n",
      "2019-01-24T10:41:40.736610, step: 1819, loss: 2.2456374168395996, acc: 0.97620689868927, f1: 0.7386, precision: 0.7558, recall: 0.7222\n",
      "2019-01-24T10:41:41.658902, step: 1820, loss: 1.728732705116272, acc: 0.9798431992530823, f1: 0.7784, precision: 0.7831, recall: 0.7738\n",
      "2019-01-24T10:41:44.175667, step: 1821, loss: 3.371572971343994, acc: 0.9736325144767761, f1: 0.8201, precision: 0.8129, recall: 0.8274\n",
      "2019-01-24T10:41:45.239098, step: 1822, loss: 2.9650583267211914, acc: 0.9664031863212585, f1: 0.7586, precision: 0.7458, recall: 0.7719\n",
      "2019-01-24T10:41:45.782155, step: 1823, loss: 2.4782042503356934, acc: 0.9768429398536682, f1: 0.77, precision: 0.77, recall: 0.77\n",
      "2019-01-24T10:41:48.248103, step: 1824, loss: 4.162880897521973, acc: 0.9695298671722412, f1: 0.6545, precision: 0.6923, recall: 0.6207\n",
      "2019-01-24T10:41:48.960328, step: 1825, loss: 1.8752895593643188, acc: 0.9794173240661621, f1: 0.7674, precision: 0.8049, recall: 0.7333\n",
      "2019-01-24T10:41:51.416422, step: 1826, loss: 1.8365360498428345, acc: 0.9832756519317627, f1: 0.8116, precision: 0.8421, recall: 0.7832\n",
      "2019-01-24T10:41:51.917735, step: 1827, loss: 1.6752618551254272, acc: 0.9788418412208557, f1: 0.7722, precision: 0.8026, recall: 0.7439\n",
      "2019-01-24T10:41:52.740008, step: 1828, loss: 1.9059174060821533, acc: 0.977958619594574, f1: 0.7917, precision: 0.8085, recall: 0.7755\n",
      "2019-01-24T10:41:53.301581, step: 1829, loss: 3.5500741004943848, acc: 0.9700374603271484, f1: 0.731, precision: 0.75, recall: 0.7129\n",
      "2019-01-24T10:41:54.366991, step: 1830, loss: 1.8114864826202393, acc: 0.9836897253990173, f1: 0.795, precision: 0.8, recall: 0.7901\n",
      "2019-01-24T10:41:55.343807, step: 1831, loss: 2.9794063568115234, acc: 0.9708770513534546, f1: 0.7639, precision: 0.7607, recall: 0.7672\n",
      "2019-01-24T10:41:56.050504, step: 1832, loss: 2.411125898361206, acc: 0.9770469069480896, f1: 0.7558, precision: 0.7471, recall: 0.7647\n",
      "2019-01-24T10:41:56.773497, step: 1833, loss: 2.493440628051758, acc: 0.9762418866157532, f1: 0.7083, precision: 0.7312, recall: 0.6869\n",
      "2019-01-24T10:41:57.383698, step: 1834, loss: 1.8001562356948853, acc: 0.9818046689033508, f1: 0.7791, precision: 0.7614, recall: 0.7976\n",
      "2019-01-24T10:41:57.751146, step: 1835, loss: 2.1022086143493652, acc: 0.9751391410827637, f1: 0.7244, precision: 0.7302, recall: 0.7188\n",
      "2019-01-24T10:41:58.289922, step: 1836, loss: 2.22902774810791, acc: 0.9810606241226196, f1: 0.7448, precision: 0.7397, recall: 0.75\n",
      "2019-01-24T10:41:59.053346, step: 1837, loss: 1.6945360898971558, acc: 0.9848910570144653, f1: 0.809, precision: 0.809, recall: 0.809\n",
      "2019-01-24T10:41:59.584049, step: 1838, loss: 1.9365589618682861, acc: 0.9813324809074402, f1: 0.78, precision: 0.7879, recall: 0.7723\n",
      "2019-01-24T10:42:00.091230, step: 1839, loss: 2.1079301834106445, acc: 0.9725660681724548, f1: 0.78, precision: 0.8125, recall: 0.75\n",
      "2019-01-24T10:42:00.998310, step: 1840, loss: 2.4633982181549072, acc: 0.9792658090591431, f1: 0.8537, precision: 0.84, recall: 0.8678\n",
      "2019-01-24T10:42:01.601711, step: 1841, loss: 2.5278358459472656, acc: 0.975640594959259, f1: 0.7, precision: 0.7179, recall: 0.6829\n",
      "2019-01-24T10:42:02.255319, step: 1842, loss: 2.1799707412719727, acc: 0.9754855036735535, f1: 0.7937, precision: 0.8152, recall: 0.7732\n",
      "2019-01-24T10:42:03.077946, step: 1843, loss: 2.7541747093200684, acc: 0.9683662056922913, f1: 0.6957, precision: 0.7356, recall: 0.6598\n",
      "2019-01-24T10:42:04.501103, step: 1844, loss: 2.280897617340088, acc: 0.9849031567573547, f1: 0.7033, precision: 0.7111, recall: 0.6957\n",
      "2019-01-24T10:42:05.516901, step: 1845, loss: 2.025266170501709, acc: 0.98128342628479, f1: 0.7882, precision: 0.8081, recall: 0.7692\n",
      "2019-01-24T10:42:06.105127, step: 1846, loss: 2.205000877380371, acc: 0.9824290871620178, f1: 0.7668, precision: 0.7629, recall: 0.7708\n",
      "2019-01-24T10:42:07.691681, step: 1847, loss: 3.235645294189453, acc: 0.9697309136390686, f1: 0.7708, precision: 0.7838, recall: 0.7582\n",
      "2019-01-24T10:42:08.254423, step: 1848, loss: 1.5391323566436768, acc: 0.9855072498321533, f1: 0.8235, precision: 0.8434, recall: 0.8046\n",
      "2019-01-24T10:42:08.788673, step: 1849, loss: 1.7241369485855103, acc: 0.983818769454956, f1: 0.8421, precision: 0.878, recall: 0.809\n",
      "2019-01-24T10:42:09.951020, step: 1850, loss: 1.2584887742996216, acc: 0.9860116839408875, f1: 0.8889, precision: 0.8947, recall: 0.8831\n",
      "2019-01-24T10:42:10.430120, step: 1851, loss: 2.4875340461730957, acc: 0.9745199680328369, f1: 0.75, precision: 0.75, recall: 0.75\n",
      "2019-01-24T10:42:11.182575, step: 1852, loss: 2.080310344696045, acc: 0.9785560965538025, f1: 0.7517, precision: 0.7568, recall: 0.7467\n",
      "2019-01-24T10:42:12.025040, step: 1853, loss: 2.025179862976074, acc: 0.9786545634269714, f1: 0.8326, precision: 0.8435, recall: 0.822\n",
      "2019-01-24T10:42:12.788558, step: 1854, loss: 2.4399681091308594, acc: 0.9774527549743652, f1: 0.7841, precision: 0.8214, recall: 0.75\n",
      "2019-01-24T10:42:13.674488, step: 1855, loss: 2.7852416038513184, acc: 0.9788199663162231, f1: 0.77, precision: 0.8119, recall: 0.7321\n",
      "2019-01-24T10:42:14.058790, step: 1856, loss: 1.7552732229232788, acc: 0.9821295738220215, f1: 0.8221, precision: 0.8375, recall: 0.8072\n",
      "2019-01-24T10:42:14.562168, step: 1857, loss: 2.3430533409118652, acc: 0.9780297875404358, f1: 0.7882, precision: 0.7921, recall: 0.7843\n",
      "2019-01-24T10:42:15.255754, step: 1858, loss: 3.2428817749023438, acc: 0.961417019367218, f1: 0.7437, precision: 0.74, recall: 0.7475\n",
      "2019-01-24T10:42:15.895555, step: 1859, loss: 2.997460126876831, acc: 0.9740012884140015, f1: 0.7426, precision: 0.7426, recall: 0.7426\n",
      "2019-01-24T10:42:16.453714, step: 1860, loss: 2.718703031539917, acc: 0.9777268171310425, f1: 0.77, precision: 0.7857, recall: 0.7549\n",
      "2019-01-24T10:42:17.000984, step: 1861, loss: 2.673007011413574, acc: 0.9764320850372314, f1: 0.7368, precision: 0.7404, recall: 0.7333\n",
      "2019-01-24T10:42:18.048387, step: 1862, loss: 2.1775190830230713, acc: 0.9809296727180481, f1: 0.8111, precision: 0.8073, recall: 0.8148\n",
      "2019-01-24T10:42:18.637274, step: 1863, loss: 1.6977005004882812, acc: 0.9875126481056213, f1: 0.8654, precision: 0.8654, recall: 0.8654\n",
      "2019-01-24T10:42:19.550515, step: 1864, loss: 2.236318349838257, acc: 0.9809746146202087, f1: 0.7561, precision: 0.8052, recall: 0.7126\n",
      "2019-01-24T10:42:20.112027, step: 1865, loss: 2.1317954063415527, acc: 0.9757639765739441, f1: 0.7456, precision: 0.7875, recall: 0.7079\n",
      "2019-01-24T10:42:20.851053, step: 1866, loss: 1.9410053491592407, acc: 0.9818049669265747, f1: 0.791, precision: 0.7778, recall: 0.8046\n",
      "2019-01-24T10:42:23.453682, step: 1867, loss: 2.4121084213256836, acc: 0.9820499420166016, f1: 0.8127, precision: 0.8156, recall: 0.8099\n",
      "2019-01-24T10:42:24.154041, step: 1868, loss: 2.09163236618042, acc: 0.9790928363800049, f1: 0.773, precision: 0.7975, recall: 0.75\n",
      "2019-01-24T10:42:24.530770, step: 1869, loss: 1.9724584817886353, acc: 0.974973201751709, f1: 0.7375, precision: 0.7867, recall: 0.6941\n",
      "2019-01-24T10:42:25.261800, step: 1870, loss: 2.032560348510742, acc: 0.9849094748497009, f1: 0.8317, precision: 0.8485, recall: 0.8155\n",
      "2019-01-24T10:42:27.538987, step: 1871, loss: 1.4531149864196777, acc: 0.9928593635559082, f1: 0.9179, precision: 0.9318, recall: 0.9044\n",
      "2019-01-24T10:42:28.006697, step: 1872, loss: 2.591276168823242, acc: 0.9713144302368164, f1: 0.7604, precision: 0.8022, recall: 0.7228\n",
      "2019-01-24T10:42:28.495478, step: 1873, loss: 3.352678060531616, acc: 0.9656084775924683, f1: 0.6814, precision: 0.6754, recall: 0.6875\n",
      "2019-01-24T10:42:29.057737, step: 1874, loss: 2.161076307296753, acc: 0.9751483798027039, f1: 0.7059, precision: 0.7059, recall: 0.7059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24T10:42:29.911053, step: 1875, loss: 2.5376791954040527, acc: 0.9737171530723572, f1: 0.7811, precision: 0.7982, recall: 0.7647\n",
      "2019-01-24T10:42:30.463679, step: 1876, loss: 2.370288372039795, acc: 0.9716412425041199, f1: 0.7729, precision: 0.8, recall: 0.7477\n",
      "2019-01-24T10:42:31.013829, step: 1877, loss: 2.310115337371826, acc: 0.98175048828125, f1: 0.7625, precision: 0.7922, recall: 0.7349\n",
      "2019-01-24T10:42:31.760605, step: 1878, loss: 1.7339906692504883, acc: 0.9837962985038757, f1: 0.8458, precision: 0.8649, recall: 0.8276\n",
      "2019-01-24T10:42:34.053014, step: 1879, loss: 2.875293016433716, acc: 0.9781083464622498, f1: 0.8114, precision: 0.8261, recall: 0.7972\n",
      "2019-01-24T10:42:34.924293, step: 1880, loss: 1.8289082050323486, acc: 0.9811260104179382, f1: 0.7959, precision: 0.8211, recall: 0.7723\n",
      "2019-01-24T10:42:35.946684, step: 1881, loss: 2.8372416496276855, acc: 0.9719222187995911, f1: 0.7373, precision: 0.7767, recall: 0.7018\n",
      "2019-01-24T10:42:36.707813, step: 1882, loss: 2.0665860176086426, acc: 0.9794801473617554, f1: 0.7937, precision: 0.7979, recall: 0.7895\n",
      "2019-01-24T10:42:37.706207, step: 1883, loss: 1.4660664796829224, acc: 0.9842271208763123, f1: 0.7826, precision: 0.806, recall: 0.7606\n",
      "2019-01-24T10:42:38.657345, step: 1884, loss: 2.0113675594329834, acc: 0.9798701405525208, f1: 0.7529, precision: 0.7711, recall: 0.7356\n",
      "2019-01-24T10:42:39.226748, step: 1885, loss: 2.6138699054718018, acc: 0.9668826460838318, f1: 0.7228, precision: 0.7374, recall: 0.7087\n",
      "2019-01-24T10:42:39.974309, step: 1886, loss: 2.4245667457580566, acc: 0.9763292074203491, f1: 0.775, precision: 0.775, recall: 0.775\n",
      "2019-01-24T10:42:41.562043, step: 1887, loss: 1.6028919219970703, acc: 0.9832743406295776, f1: 0.7571, precision: 0.7465, recall: 0.7681\n",
      "2019-01-24T10:42:42.177674, step: 1888, loss: 2.112626552581787, acc: 0.9806996583938599, f1: 0.8128, precision: 0.8165, recall: 0.8091\n",
      "2019-01-24T10:42:43.169729, step: 1889, loss: 1.7471342086791992, acc: 0.9853542447090149, f1: 0.8492, precision: 0.8444, recall: 0.8539\n",
      "2019-01-24T10:42:43.729159, step: 1890, loss: 3.1626105308532715, acc: 0.9655913710594177, f1: 0.7009, precision: 0.7576, recall: 0.6522\n",
      "2019-01-24T10:42:46.611278, step: 1891, loss: 3.1251895427703857, acc: 0.9760000109672546, f1: 0.8591, precision: 0.8767, recall: 0.8421\n",
      "2019-01-24T10:42:47.204014, step: 1892, loss: 3.155907154083252, acc: 0.971446692943573, f1: 0.7718, precision: 0.7881, recall: 0.7561\n",
      "2019-01-24T10:42:47.835986, step: 1893, loss: 2.522425413131714, acc: 0.9736679196357727, f1: 0.761, precision: 0.7647, recall: 0.7573\n",
      "2019-01-24T10:42:48.978215, step: 1894, loss: 2.301748037338257, acc: 0.9792882204055786, f1: 0.8327, precision: 0.8421, recall: 0.8235\n",
      "2019-01-24T10:42:50.055665, step: 1895, loss: 1.818480134010315, acc: 0.9801861047744751, f1: 0.8037, precision: 0.8113, recall: 0.7963\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "train_datas = dataGen.train_datas\n",
    "train_labels = dataGen.train_labels\n",
    "eval_datas = dataGen.eval_datas\n",
    "eval_labels = dataGen.eval_labels\n",
    "\n",
    "vocab_len = dataGen.vocab_len\n",
    "label_to_index = dataGen.label_to_index\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTMCrf(config, vocab_len, True)\n",
    "        \n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learning_rate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        grads_and_vars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "#         grad_summaries = []\n",
    "#         for g, v in grads_and_vars:\n",
    "#             if g is not None:\n",
    "#                 tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "#                 tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "#         out_dir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "#         print(\"Writing to {}\\n\".format(out_dir))\n",
    "        \n",
    "#         loss_summary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "#         summary_op = tf.summary.merge_all()\n",
    "        \n",
    "#         train_summary_dir = os.path.join(out_dir, \"train\")\n",
    "#         train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "        \n",
    "#         eval_summary_dir = os.path.join(out_dir, \"eval\")\n",
    "#         eval_summary_writer = tf.summary.FileWriter(eval_summary_dir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(batch_x, batch_y, sequence_len, max_len):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "                lstm.input_x: batch_x,\n",
    "                lstm.input_y: batch_y,\n",
    "                lstm.max_len: max_len,\n",
    "                lstm.sequence_len: sequence_len,\n",
    "                lstm.dropout_keep_prob: config.model.dropout_keep_prob\n",
    "            }\n",
    "        \n",
    "            _, step, loss, acc, true_y, pred_y = sess.run(\n",
    "                [train_op, global_step, lstm.loss, lstm.accuracy, lstm.true_y, lstm.pred_y], feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "\n",
    "            f1, precision, recall = gen_metrics(true_y, pred_y, label_to_index)\n",
    "            \n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, f1: {}, precision: {}, recall: {}\".format(time_str, step, loss, \n",
    "                                                                                              acc, f1, precision, recall))\n",
    "#             train_summary_writer.add_summary(summary, step)\n",
    "\n",
    "        def dev_step(batch_x, batch_y, sequence_len, max_len):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                lstm.input_x: batch_x,\n",
    "                lstm.input_y: batch_y,\n",
    "                lstm.max_len: max_len,\n",
    "                lstm.sequence_len: sequence_len,\n",
    "                lstm.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, loss, acc, true_y, pred_y = sess.run(\n",
    "                [global_step, lstm.loss, lstm.accuracy, lstm.true_y, lstm.pred_y], feed_dict)\n",
    "            \n",
    "            f1, precision, recall = gen_metrics(true_y, pred_y, label_to_index)\n",
    "            \n",
    "#             evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, f1, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batch_train in dataGen.next_batch(train_datas, train_labels):\n",
    "                train_step(batch_train[\"input_x\"], batch_train[\"input_y\"], batch_train[\"sequence_len\"], batch_train[\"max_len\"])\n",
    "\n",
    "                current_step = tf.train.global_step(sess, global_step) \n",
    "                if current_step % config.training.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accuracys = []\n",
    "                    f1s = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    for batch_eval in dataGen.next_batch(eval_datas, eval_labels):\n",
    "                        loss, accuracy, f1, precision, recall = dev_step(batch_eval[\"input_x\"], batch_eval[\"input_y\"], \n",
    "                                                                         batch_eval[\"sequence_len\"], batch_eval[\"max_len\"])\n",
    "                        losses.append(loss)\n",
    "                        accuracys.append(accuracy)\n",
    "                        f1s.append(f1)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc:{}, f1: {}, precision: {}, recall: {}\".format(time_str, current_step, mean(losses), \n",
    "                                                                                                     mean(accuracys), mean(f1s), \n",
    "                                                                                                     mean(precisions), mean(recalls)))\n",
    "                    print(\"\\n\")\n",
    "                    \n",
    "#                 if current_step % config.training.checkpoint_every == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=current_step)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
