{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from distutils.version import LooseVersion\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion(\"1.1\")\n",
    "print(\"Tensorflow Version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "class ModelConfig():\n",
    "    encoder_hidden_layers = [50, 50]\n",
    "    decoder_hidden_layers = [50, 50]\n",
    "    dropout_prob = 0.5\n",
    "    encoder_embedding_size = 15\n",
    "    decoder_embedding_size = 15\n",
    "    \n",
    "\n",
    "class TrainConfig():\n",
    "    epochs = 10\n",
    "    every_checkpoint = 100\n",
    "    learning_rate = 0.01\n",
    "    max_grad_norm = 3\n",
    "\n",
    "\n",
    "class Config():\n",
    "    batch_size = 128\n",
    "    infer_prob = 0.2\n",
    "    \n",
    "    source_path = \"data/letters_source.txt\"\n",
    "    target_path = \"data/letters_target.txt\"\n",
    "    \n",
    "    train = TrainConfig()\n",
    "    model = ModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "\n",
    "class DataGen():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.source_path = config.source_path\n",
    "        self.target_path = config.target_path\n",
    "        \n",
    "        self.source_char_to_int = {}\n",
    "        self.source_int_to_char = {}\n",
    "        self.target_char_to_int = {}\n",
    "        self.target_int_to_char = {}\n",
    "        \n",
    "        self.source_data = []\n",
    "        self.target_data = []\n",
    "        \n",
    "        \n",
    "    def read_data(self):\n",
    "        with open(self.source_path, \"r\") as f:\n",
    "            source_char_to_int, source_int_to_char, source_data = self.gen_vocab_dict(f.read())\n",
    "        self.source_char_to_int = source_char_to_int\n",
    "        self.source_int_to_char = source_int_to_char\n",
    "        self.source_data = source_data\n",
    "            \n",
    "        with open(self.target_path, \"r\") as f:\n",
    "            target_char_to_int, target_int_to_char, target_data = self.gen_vocab_dict(f.read(), True)\n",
    "        self.target_char_to_int = target_char_to_int\n",
    "        self.target_int_to_char = target_int_to_char\n",
    "        self.target_data = target_data\n",
    "            \n",
    "    def gen_vocab_dict(self, string, is_target=False):\n",
    "        special_words = ['<PAD>', '<UNK>', '<GO>',  '<EOS>']\n",
    "        vocab = list(set(string))\n",
    "        vocab.remove(\"\\n\")\n",
    "        vocab = special_words + vocab\n",
    "\n",
    "        \n",
    "        int_to_char = {index: char for index, char in enumerate(vocab)}\n",
    "        char_to_int = {char: index for index, char in int_to_char.items()}\n",
    "\n",
    "        word_list = string.strip().split(\"\\n\")\n",
    "        if is_target:\n",
    "            data = [[char_to_int.get(char, '<UNK>') for char in word] + [char_to_int['<EOS>']] for word in word_list]\n",
    "        else:\n",
    "            data = [[char_to_int.get(char, '<UNK>') for char in word] for word in word_list]\n",
    "        return char_to_int, int_to_char, data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data: [8, 26, 6, 10, 10]\n",
      "target data: [6, 8, 10, 10, 26, 3]\n",
      "source data length: 10000\n",
      "target data length: 10000\n"
     ]
    }
   ],
   "source": [
    "dataGen = DataGen(config)\n",
    "dataGen.read_data()\n",
    "print(\"source data: {}\".format(dataGen.source_data[0]))\n",
    "print(\"target data: {}\".format(dataGen.target_data[0]))\n",
    "print(\"source data length: {}\".format(len(dataGen.source_data)))\n",
    "print(\"target data length: {}\".format(len(dataGen.target_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class Seq2SeqModel():\n",
    "    \n",
    "    def __init__(self, config, encoder_vocab_size, target_char_to_int, is_infer=False):\n",
    "        self.inputs = tf.placeholder(tf.int32, [None, None], name=\"inputs\")\n",
    "        self.targets = tf.placeholder(tf.int32, [None, None], name=\"targets\")\n",
    "        self.dropout_prob = tf.placeholder(tf.float32, name=\"dropout_prob\")\n",
    "        self.source_sequence_length = tf.placeholder(tf.int32, [None], name=\"source_sequence_length\")\n",
    "        self.target_sequence_length = tf.placeholder(tf.int32, [None], name=\"target_sequence_length\")\n",
    "        self.target_max_length = tf.reduce_max(self.target_sequence_length, name='target_max_length')\n",
    "        \n",
    "        decoder_output = self.seq2seq(config, encoder_vocab_size, target_char_to_int, is_infer)\n",
    "        \n",
    "        if is_infer:\n",
    "            self.infer_logits = tf.identity(decoder_output.sample_id, \"infer_logits\")\n",
    "            \n",
    "        else:\n",
    "            self.logits = tf.identity(decoder_output.rnn_output, \"logits\")\n",
    "            \n",
    "            masks = tf.sequence_mask(self.target_sequence_length, self.target_max_length, dtype=tf.float32, name=\"mask\")\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.loss = tf.contrib.seq2seq.sequence_loss(self.logits, self.targets, masks)\n",
    "\n",
    "            with tf.name_scope(\"accuracy\"):\n",
    "                self.predictions = tf.argmax(self.logits, 2)\n",
    "                correctness = tf.equal(tf.cast(self.predictions, dtype=tf.int32), self.targets)\n",
    "                self.accu = tf.reduce_mean(tf.cast(correctness, \"float\"), name=\"accu\")\n",
    "        \n",
    "    def encoder(self, config, encoder_vocab_size):\n",
    "        encoder_embed_input = tf.contrib.layers.embed_sequence(self.inputs, encoder_vocab_size, config.model.encoder_embedding_size)\n",
    "        \n",
    "        def get_lstm_cell(hidden_size):\n",
    "            lstm_cell = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True, \n",
    "                                                initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            drop_cell = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_cell, output_keep_prob=self.dropout_prob)\n",
    "            \n",
    "            return drop_cell\n",
    "        \n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([get_lstm_cell(hidden_size) for hidden_size in config.model.encoder_hidden_layers])\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, encoder_embed_input, sequence_length=self.source_sequence_length, dtype=tf.float32)\n",
    "        \n",
    "        return outputs, final_state\n",
    "\n",
    "    def decoder(self, config, encoder_state, target_char_to_int, is_infer):\n",
    "        \n",
    "        decoder_vocab_size = len(target_char_to_int)\n",
    "        \n",
    "        embeddings = tf.Variable(tf.random_uniform([decoder_vocab_size, config.model.decoder_embedding_size]))\n",
    "        decoder_embed_input = tf.nn.embedding_lookup(embeddings, self.targets)\n",
    "        \n",
    "        def get_lstm_cell(hidden_size):\n",
    "            lstm_cell = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True, \n",
    "                                                initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            drop_cell = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_cell, output_keep_prob=self.dropout_prob)\n",
    "            \n",
    "            return drop_cell\n",
    "        \n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([get_lstm_cell(hidden_size) for hidden_size in config.model.decoder_hidden_layers])\n",
    "        \n",
    "        # 定义有Dense方法生成的全连接层\n",
    "        output_layer = Dense(decoder_vocab_size, kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "        \n",
    "        # 定义训练时的decode的代码\n",
    "        with tf.variable_scope(\"decode\"):\n",
    "            # 得到help对象，帮助读取数据\n",
    "            train_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input, sequence_length=self.target_sequence_length)\n",
    "            \n",
    "            # 构建decoder\n",
    "            train_decoder = tf.contrib.seq2seq.BasicDecoder(cell, train_helper, encoder_state, output_layer)\n",
    "            train_decoder_output, train_state, train_sequence_length = tf.contrib.seq2seq.dynamic_decode(train_decoder, impute_finished=True, \n",
    "                                                                                                         maximum_iterations=self.target_max_length)\n",
    "            \n",
    "        \n",
    "        # 定义预测时的decode代码\n",
    "        with tf.variable_scope(\"decode\", reuse=True):\n",
    "            # 解码时的第一个时间步上的输入，之后的时间步上的输入是上一时间步的输出\n",
    "            start_tokens = tf.tile(tf.constant([target_char_to_int[\"<GO>\"]], dtype=tf.int32), [config.batch_size], name=\"start_tokens\")\n",
    "            \n",
    "            # 解码时按贪心法解码，按照最大条件概率来预测输出值，该方法需要输入启动词和结束词，启动词是个一维tensor，结束词是标量\n",
    "            infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings, start_tokens, target_char_to_int[\"<EOS>\"])\n",
    "            infer_decoder = tf.contrib.seq2seq.BasicDecoder(cell, infer_helper, encoder_state, output_layer)\n",
    "            infer_decoder_output, infer_state, infer_sequence_length = tf.contrib.seq2seq.dynamic_decode(infer_decoder,\n",
    "                                                                                                           impute_finished=True,\n",
    "                                                                                                          maximum_iterations=self.target_max_length)\n",
    "            \n",
    "        if is_infer:\n",
    "            return infer_decoder_output\n",
    "        \n",
    "        return train_decoder_output\n",
    "    \n",
    "    def seq2seq(self, config, encoder_vocab_size, target_char_to_int, is_infer):\n",
    "        \"\"\"\n",
    "        将encoder和decoder合并输出\n",
    "        \"\"\"\n",
    "        encoder_output, encoder_state = self.encoder(config, encoder_vocab_size)\n",
    "        \n",
    "        decoder_output = self.decoder(config, encoder_state, target_char_to_int, is_infer)\n",
    "        \n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义其他的函数\n",
    "def pad_batch(batch, char_to_int):\n",
    "    sequence_length = [len(sequence) for sequence in batch]\n",
    "    max_length = max(sequence_length)\n",
    "    \n",
    "    new_batch = [sequence + [char_to_int[\"<PAD>\"]] * (max_length - len(sequence)) for sequence in batch]\n",
    "    \n",
    "    return sequence_length, max_length, new_batch    \n",
    "    \n",
    "def next_batch(source, target, batch_size, source_char_to_int, target_char_to_int):\n",
    "    num_batches = len(source) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        source_batch = source[i * batch_size: (i + 1) * batch_size]\n",
    "        target_batch = target[i * batch_size: (i + 1) * batch_size]\n",
    "        \n",
    "        source_sequence_length, source_max_length, new_source_batch = pad_batch(source_batch, source_char_to_int)\n",
    "        target_sequence_length, target_max_length, new_target_batch = pad_batch(target_batch, target_char_to_int)\n",
    "        \n",
    "        yield dict(source_batch=np.array(new_source_batch), target_batch=np.array(new_target_batch), \n",
    "                   source_sequence_length=np.array(source_sequence_length), target_sequence_length=np.array(target_sequence_length), \n",
    "                   target_max_length=target_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化完成，开始训练模型\n",
      "step: 1  loss: 3.3972744941711426  accu: 0.396484375\n",
      "step: 2  loss: 3.352038860321045  accu: 0.5068359375\n",
      "step: 3  loss: 3.2333476543426514  accu: 0.537109375\n",
      "step: 4  loss: 3.133819818496704  accu: 0.5107421875\n",
      "step: 5  loss: 3.1084017753601074  accu: 0.494140625\n",
      "step: 6  loss: 3.019141674041748  accu: 0.5361328125\n",
      "step: 7  loss: 3.019252061843872  accu: 0.5205078125\n",
      "step: 8  loss: 2.961327314376831  accu: 0.4794921875\n",
      "step: 9  loss: 2.9175174236297607  accu: 0.55078125\n",
      "step: 10  loss: 2.9328219890594482  accu: 0.515625\n",
      "step: 11  loss: 2.8361661434173584  accu: 0.55859375\n",
      "step: 12  loss: 2.825479507446289  accu: 0.490234375\n",
      "step: 13  loss: 2.8294155597686768  accu: 0.470703125\n",
      "step: 14  loss: 2.733366012573242  accu: 0.5234375\n",
      "step: 15  loss: 2.662925958633423  accu: 0.560546875\n",
      "step: 16  loss: 2.6590795516967773  accu: 0.5185546875\n",
      "step: 17  loss: 2.6025102138519287  accu: 0.541015625\n",
      "step: 18  loss: 2.5877397060394287  accu: 0.5234375\n",
      "step: 19  loss: 2.548031806945801  accu: 0.51953125\n",
      "step: 20  loss: 2.5171799659729004  accu: 0.5595703125\n",
      "step: 21  loss: 2.561244249343872  accu: 0.50390625\n",
      "step: 22  loss: 2.4349355697631836  accu: 0.5556640625\n",
      "step: 23  loss: 2.4562885761260986  accu: 0.5205078125\n",
      "step: 24  loss: 2.415034055709839  accu: 0.54296875\n",
      "step: 25  loss: 2.3782644271850586  accu: 0.509765625\n",
      "step: 26  loss: 2.3122398853302  accu: 0.5791015625\n",
      "step: 27  loss: 2.3514175415039062  accu: 0.5087890625\n",
      "step: 28  loss: 2.284989356994629  accu: 0.5625\n",
      "step: 29  loss: 2.292593240737915  accu: 0.533203125\n",
      "step: 30  loss: 2.2975783348083496  accu: 0.5205078125\n",
      "step: 31  loss: 2.270669460296631  accu: 0.5400390625\n",
      "step: 32  loss: 2.204996109008789  accu: 0.5771484375\n",
      "step: 33  loss: 2.2213358879089355  accu: 0.548828125\n",
      "step: 34  loss: 2.1655383110046387  accu: 0.5908203125\n",
      "step: 35  loss: 2.1609137058258057  accu: 0.5673828125\n",
      "step: 36  loss: 2.1604666709899902  accu: 0.521484375\n",
      "step: 37  loss: 2.0726118087768555  accu: 0.556640625\n",
      "step: 38  loss: 2.0944387912750244  accu: 0.583984375\n",
      "step: 39  loss: 2.0657124519348145  accu: 0.5849609375\n",
      "step: 40  loss: 2.018254518508911  accu: 0.5576171875\n",
      "step: 41  loss: 2.039016008377075  accu: 0.5537109375\n",
      "step: 42  loss: 1.9720503091812134  accu: 0.578125\n",
      "step: 43  loss: 2.0071964263916016  accu: 0.552734375\n",
      "step: 44  loss: 1.96622896194458  accu: 0.572265625\n",
      "step: 45  loss: 1.957801342010498  accu: 0.5634765625\n",
      "step: 46  loss: 1.9385437965393066  accu: 0.5947265625\n",
      "step: 47  loss: 1.9140905141830444  accu: 0.583984375\n",
      "step: 48  loss: 1.8792563676834106  accu: 0.6201171875\n",
      "step: 49  loss: 1.854565143585205  accu: 0.625\n",
      "step: 50  loss: 1.8657726049423218  accu: 0.5771484375\n",
      "step: 51  loss: 1.7980918884277344  accu: 0.64453125\n",
      "step: 52  loss: 1.8086849451065063  accu: 0.6376953125\n",
      "step: 53  loss: 1.778518557548523  accu: 0.6103515625\n",
      "step: 54  loss: 1.7460787296295166  accu: 0.6396484375\n",
      "step: 55  loss: 1.7688522338867188  accu: 0.623046875\n",
      "step: 56  loss: 1.682984471321106  accu: 0.6650390625\n",
      "step: 57  loss: 1.7244772911071777  accu: 0.62109375\n",
      "step: 58  loss: 1.7223844528198242  accu: 0.6162109375\n",
      "step: 59  loss: 1.7526977062225342  accu: 0.5849609375\n",
      "step: 60  loss: 1.6746463775634766  accu: 0.6416015625\n",
      "step: 61  loss: 1.6912881135940552  accu: 0.6181640625\n",
      "step: 62  loss: 1.6334211826324463  accu: 0.6396484375\n",
      "step: 63  loss: 1.6251977682113647  accu: 0.638671875\n",
      "step: 64  loss: 1.6329076290130615  accu: 0.6416015625\n",
      "step: 65  loss: 1.5936822891235352  accu: 0.6611328125\n",
      "step: 66  loss: 1.5845063924789429  accu: 0.6494140625\n",
      "step: 67  loss: 1.5751559734344482  accu: 0.66015625\n",
      "step: 68  loss: 1.5663707256317139  accu: 0.6611328125\n",
      "step: 69  loss: 1.5308970212936401  accu: 0.6591796875\n",
      "step: 70  loss: 1.5568259954452515  accu: 0.62890625\n",
      "step: 71  loss: 1.5214357376098633  accu: 0.68359375\n",
      "step: 72  loss: 1.5283501148223877  accu: 0.6708984375\n",
      "step: 73  loss: 1.4725830554962158  accu: 0.697265625\n",
      "step: 74  loss: 1.5617932081222534  accu: 0.6328125\n",
      "step: 75  loss: 1.513946294784546  accu: 0.626953125\n",
      "step: 76  loss: 1.5153186321258545  accu: 0.6630859375\n",
      "step: 77  loss: 1.445725917816162  accu: 0.685546875\n",
      "step: 78  loss: 1.4533205032348633  accu: 0.6640625\n",
      "step: 79  loss: 1.4363378286361694  accu: 0.693359375\n",
      "step: 80  loss: 1.4460842609405518  accu: 0.6767578125\n",
      "step: 81  loss: 1.4768576622009277  accu: 0.6630859375\n",
      "step: 82  loss: 1.384360909461975  accu: 0.7021484375\n",
      "step: 83  loss: 1.4384912252426147  accu: 0.654296875\n",
      "step: 84  loss: 1.3611115217208862  accu: 0.6982421875\n",
      "step: 85  loss: 1.400131344795227  accu: 0.6728515625\n",
      "step: 86  loss: 1.3794236183166504  accu: 0.69140625\n",
      "step: 87  loss: 1.3826202154159546  accu: 0.69140625\n",
      "step: 88  loss: 1.3077731132507324  accu: 0.7158203125\n",
      "step: 89  loss: 1.358813762664795  accu: 0.658203125\n",
      "step: 90  loss: 1.3143552541732788  accu: 0.71875\n",
      "step: 91  loss: 1.2884286642074585  accu: 0.7177734375\n",
      "step: 92  loss: 1.3446229696273804  accu: 0.6767578125\n",
      "step: 93  loss: 1.2576698064804077  accu: 0.7216796875\n",
      "step: 94  loss: 1.3125282526016235  accu: 0.7158203125\n",
      "step: 95  loss: 1.2987022399902344  accu: 0.7080078125\n",
      "step: 96  loss: 1.266187071800232  accu: 0.724609375\n",
      "step: 97  loss: 1.2474427223205566  accu: 0.71484375\n",
      "step: 98  loss: 1.2626317739486694  accu: 0.6865234375\n",
      "step: 99  loss: 1.239538311958313  accu: 0.7060546875\n",
      "step: 100  loss: 1.2009721994400024  accu: 0.7265625\n",
      "\n",
      "\n",
      "Evaluation accuracy: 0.49345944553410426\n",
      "\n",
      "\n",
      "step: 101  loss: 1.189357042312622  accu: 0.734375\n",
      "step: 102  loss: 1.1890769004821777  accu: 0.720703125\n",
      "step: 103  loss: 1.199211597442627  accu: 0.720703125\n",
      "step: 104  loss: 1.1308391094207764  accu: 0.73046875\n",
      "step: 105  loss: 1.1878005266189575  accu: 0.7158203125\n",
      "step: 106  loss: 1.1903305053710938  accu: 0.7177734375\n",
      "step: 107  loss: 1.134551763534546  accu: 0.732421875\n",
      "step: 108  loss: 1.1149747371673584  accu: 0.7666015625\n",
      "step: 109  loss: 1.1345666646957397  accu: 0.736328125\n",
      "step: 110  loss: 1.0857768058776855  accu: 0.767578125\n",
      "step: 111  loss: 1.0792278051376343  accu: 0.76953125\n",
      "step: 112  loss: 1.0804985761642456  accu: 0.751953125\n",
      "step: 113  loss: 1.0473171472549438  accu: 0.7724609375\n",
      "step: 114  loss: 1.0418726205825806  accu: 0.775390625\n",
      "step: 115  loss: 1.0288562774658203  accu: 0.775390625\n",
      "step: 116  loss: 0.9975759387016296  accu: 0.78515625\n",
      "step: 117  loss: 1.0073282718658447  accu: 0.77734375\n",
      "step: 118  loss: 0.9447953701019287  accu: 0.8125\n",
      "step: 119  loss: 0.9852312803268433  accu: 0.76953125\n",
      "step: 120  loss: 0.9595542550086975  accu: 0.7734375\n",
      "step: 121  loss: 0.9750018119812012  accu: 0.7587890625\n",
      "step: 122  loss: 0.9463610053062439  accu: 0.8046875\n",
      "step: 123  loss: 0.9727222323417664  accu: 0.779296875\n",
      "step: 124  loss: 0.9199241995811462  accu: 0.8017578125\n",
      "step: 125  loss: 0.8875441551208496  accu: 0.8203125\n",
      "step: 126  loss: 0.8726112246513367  accu: 0.81640625\n",
      "step: 127  loss: 0.892274796962738  accu: 0.810546875\n",
      "step: 128  loss: 0.8543358445167542  accu: 0.814453125\n",
      "step: 129  loss: 0.8544927835464478  accu: 0.8046875\n",
      "step: 130  loss: 0.8265075087547302  accu: 0.837890625\n",
      "step: 131  loss: 0.8281135559082031  accu: 0.8212890625\n",
      "step: 132  loss: 0.834295392036438  accu: 0.8193359375\n",
      "step: 133  loss: 0.7817465662956238  accu: 0.8525390625\n",
      "step: 134  loss: 0.8129774332046509  accu: 0.826171875\n",
      "step: 135  loss: 0.7966614365577698  accu: 0.8408203125\n",
      "step: 136  loss: 0.8261427879333496  accu: 0.8115234375\n",
      "step: 137  loss: 0.7992817759513855  accu: 0.810546875\n",
      "step: 138  loss: 0.7858878970146179  accu: 0.818359375\n",
      "step: 139  loss: 0.692070484161377  accu: 0.8623046875\n",
      "step: 140  loss: 0.7994542121887207  accu: 0.8134765625\n",
      "step: 141  loss: 0.7285968065261841  accu: 0.841796875\n",
      "step: 142  loss: 0.7217517495155334  accu: 0.845703125\n",
      "step: 143  loss: 0.7259346842765808  accu: 0.8447265625\n",
      "step: 144  loss: 0.7098904848098755  accu: 0.8447265625\n",
      "step: 145  loss: 0.6871033906936646  accu: 0.84375\n",
      "step: 146  loss: 0.6991338729858398  accu: 0.857421875\n",
      "step: 147  loss: 0.6475669145584106  accu: 0.8623046875\n",
      "step: 148  loss: 0.6512823104858398  accu: 0.8642578125\n",
      "step: 149  loss: 0.6786645650863647  accu: 0.849609375\n",
      "step: 150  loss: 0.6370152235031128  accu: 0.87109375\n",
      "step: 151  loss: 0.6329055428504944  accu: 0.8564453125\n",
      "step: 152  loss: 0.6267017126083374  accu: 0.873046875\n",
      "step: 153  loss: 0.5846686363220215  accu: 0.8740234375\n",
      "step: 154  loss: 0.650203287601471  accu: 0.85546875\n",
      "step: 155  loss: 0.6283345818519592  accu: 0.8623046875\n",
      "step: 156  loss: 0.6061846613883972  accu: 0.8955078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 157  loss: 0.5990403294563293  accu: 0.869140625\n",
      "step: 158  loss: 0.5435983538627625  accu: 0.90625\n",
      "step: 159  loss: 0.6049606800079346  accu: 0.8759765625\n",
      "step: 160  loss: 0.5833047032356262  accu: 0.8701171875\n",
      "step: 161  loss: 0.5891702175140381  accu: 0.873046875\n",
      "step: 162  loss: 0.5502305626869202  accu: 0.90234375\n",
      "step: 163  loss: 0.5205056667327881  accu: 0.8974609375\n",
      "step: 164  loss: 0.5394227504730225  accu: 0.87890625\n",
      "step: 165  loss: 0.5013298988342285  accu: 0.8857421875\n",
      "step: 166  loss: 0.5243359804153442  accu: 0.8935546875\n",
      "step: 167  loss: 0.5390182733535767  accu: 0.8798828125\n",
      "step: 168  loss: 0.4949582517147064  accu: 0.8896484375\n",
      "step: 169  loss: 0.46689775586128235  accu: 0.9033203125\n",
      "step: 170  loss: 0.5204634666442871  accu: 0.8994140625\n",
      "step: 171  loss: 0.5004429221153259  accu: 0.8994140625\n",
      "step: 172  loss: 0.47173967957496643  accu: 0.8994140625\n",
      "step: 173  loss: 0.4594154953956604  accu: 0.904296875\n",
      "step: 174  loss: 0.48389893770217896  accu: 0.904296875\n",
      "step: 175  loss: 0.45978423953056335  accu: 0.9130859375\n",
      "step: 176  loss: 0.4223805069923401  accu: 0.9267578125\n",
      "step: 177  loss: 0.45256516337394714  accu: 0.916015625\n",
      "step: 178  loss: 0.4393559694290161  accu: 0.9189453125\n",
      "step: 179  loss: 0.43124890327453613  accu: 0.921875\n",
      "step: 180  loss: 0.3906610906124115  accu: 0.9365234375\n",
      "step: 181  loss: 0.4521854519844055  accu: 0.916015625\n",
      "step: 182  loss: 0.40419405698776245  accu: 0.92578125\n",
      "step: 183  loss: 0.41481444239616394  accu: 0.919921875\n",
      "step: 184  loss: 0.41489726305007935  accu: 0.931640625\n",
      "step: 185  loss: 0.4019720256328583  accu: 0.9208984375\n",
      "step: 186  loss: 0.4019019603729248  accu: 0.9296875\n",
      "step: 187  loss: 0.3875104784965515  accu: 0.9267578125\n",
      "step: 188  loss: 0.3899766504764557  accu: 0.9326171875\n",
      "step: 189  loss: 0.362798273563385  accu: 0.9404296875\n",
      "step: 190  loss: 0.3467831015586853  accu: 0.94140625\n",
      "step: 191  loss: 0.3359607756137848  accu: 0.9462890625\n",
      "step: 192  loss: 0.33741846680641174  accu: 0.9453125\n",
      "step: 193  loss: 0.3567342162132263  accu: 0.94140625\n",
      "step: 194  loss: 0.3482016324996948  accu: 0.9384765625\n",
      "step: 195  loss: 0.32000216841697693  accu: 0.953125\n",
      "step: 196  loss: 0.3361264765262604  accu: 0.9482421875\n",
      "step: 197  loss: 0.3286144435405731  accu: 0.9482421875\n",
      "step: 198  loss: 0.3251529633998871  accu: 0.9365234375\n",
      "step: 199  loss: 0.3224601745605469  accu: 0.9462890625\n",
      "step: 200  loss: 0.30478495359420776  accu: 0.9501953125\n",
      "\n",
      "\n",
      "Evaluation accuracy: 0.40486050978740806\n",
      "\n",
      "\n",
      "step: 201  loss: 0.2979624271392822  accu: 0.9521484375\n",
      "step: 202  loss: 0.33082008361816406  accu: 0.93359375\n",
      "step: 203  loss: 0.2893669903278351  accu: 0.9462890625\n",
      "step: 204  loss: 0.29324886202812195  accu: 0.94140625\n",
      "step: 205  loss: 0.3018334209918976  accu: 0.9404296875\n",
      "step: 206  loss: 0.29111751914024353  accu: 0.9521484375\n",
      "step: 207  loss: 0.29057809710502625  accu: 0.947265625\n",
      "step: 208  loss: 0.27620700001716614  accu: 0.9560546875\n",
      "step: 209  loss: 0.2734891474246979  accu: 0.9541015625\n",
      "step: 210  loss: 0.2959739863872528  accu: 0.9501953125\n",
      "step: 211  loss: 0.26314470171928406  accu: 0.9443359375\n",
      "step: 212  loss: 0.22832100093364716  accu: 0.9677734375\n",
      "step: 213  loss: 0.25572749972343445  accu: 0.955078125\n",
      "step: 214  loss: 0.2562570571899414  accu: 0.955078125\n",
      "step: 215  loss: 0.2530571222305298  accu: 0.953125\n",
      "step: 216  loss: 0.2580425441265106  accu: 0.9462890625\n",
      "step: 217  loss: 0.24221621453762054  accu: 0.95703125\n",
      "step: 218  loss: 0.2509271502494812  accu: 0.9521484375\n",
      "step: 219  loss: 0.23931792378425598  accu: 0.958984375\n",
      "step: 220  loss: 0.22222912311553955  accu: 0.966796875\n",
      "step: 221  loss: 0.2818729281425476  accu: 0.951171875\n",
      "step: 222  loss: 0.2560797929763794  accu: 0.9501953125\n",
      "step: 223  loss: 0.22980433702468872  accu: 0.962890625\n",
      "step: 224  loss: 0.25980719923973083  accu: 0.9521484375\n",
      "step: 225  loss: 0.2147338092327118  accu: 0.9609375\n",
      "step: 226  loss: 0.2119119018316269  accu: 0.962890625\n",
      "step: 227  loss: 0.21562090516090393  accu: 0.970703125\n",
      "step: 228  loss: 0.2243121713399887  accu: 0.958984375\n",
      "step: 229  loss: 0.20591409504413605  accu: 0.9677734375\n",
      "step: 230  loss: 0.19356396794319153  accu: 0.96875\n",
      "step: 231  loss: 0.2194317728281021  accu: 0.95703125\n",
      "step: 232  loss: 0.18811249732971191  accu: 0.974609375\n",
      "step: 233  loss: 0.18934151530265808  accu: 0.9677734375\n",
      "step: 234  loss: 0.18572260439395905  accu: 0.9755859375\n",
      "step: 235  loss: 0.18575987219810486  accu: 0.9775390625\n",
      "step: 236  loss: 0.17115189135074615  accu: 0.9755859375\n",
      "step: 237  loss: 0.18810001015663147  accu: 0.9736328125\n",
      "step: 238  loss: 0.17097003757953644  accu: 0.9775390625\n",
      "step: 239  loss: 0.20759858191013336  accu: 0.9638671875\n",
      "step: 240  loss: 0.18845540285110474  accu: 0.9716796875\n",
      "step: 241  loss: 0.18629401922225952  accu: 0.9736328125\n",
      "step: 242  loss: 0.20396792888641357  accu: 0.9658203125\n",
      "step: 243  loss: 0.17171286046504974  accu: 0.97265625\n",
      "step: 244  loss: 0.16774289309978485  accu: 0.974609375\n",
      "step: 245  loss: 0.18001680076122284  accu: 0.970703125\n",
      "step: 246  loss: 0.17299319803714752  accu: 0.9716796875\n",
      "step: 247  loss: 0.1605101376771927  accu: 0.974609375\n",
      "step: 248  loss: 0.1731223464012146  accu: 0.97265625\n",
      "step: 249  loss: 0.14520449936389923  accu: 0.978515625\n",
      "step: 250  loss: 0.1945529580116272  accu: 0.962890625\n",
      "step: 251  loss: 0.17815762758255005  accu: 0.978515625\n",
      "step: 252  loss: 0.1955423802137375  accu: 0.966796875\n",
      "step: 253  loss: 0.17243549227714539  accu: 0.9736328125\n",
      "step: 254  loss: 0.14114272594451904  accu: 0.9814453125\n",
      "step: 255  loss: 0.17042601108551025  accu: 0.97265625\n",
      "step: 256  loss: 0.1652476042509079  accu: 0.97265625\n",
      "step: 257  loss: 0.15432938933372498  accu: 0.9736328125\n",
      "step: 258  loss: 0.16679470241069794  accu: 0.9736328125\n",
      "step: 259  loss: 0.1545337736606598  accu: 0.98046875\n",
      "step: 260  loss: 0.14806531369686127  accu: 0.9755859375\n",
      "step: 261  loss: 0.1479993611574173  accu: 0.9775390625\n",
      "step: 262  loss: 0.13440915942192078  accu: 0.9833984375\n",
      "step: 263  loss: 0.13505889475345612  accu: 0.982421875\n",
      "step: 264  loss: 0.12975430488586426  accu: 0.9814453125\n",
      "step: 265  loss: 0.14205709099769592  accu: 0.9765625\n",
      "step: 266  loss: 0.15523196756839752  accu: 0.97265625\n",
      "step: 267  loss: 0.1267184168100357  accu: 0.978515625\n",
      "step: 268  loss: 0.1329062581062317  accu: 0.9814453125\n",
      "step: 269  loss: 0.1139892116189003  accu: 0.9873046875\n",
      "step: 270  loss: 0.1398562788963318  accu: 0.9833984375\n",
      "step: 271  loss: 0.13246597349643707  accu: 0.98046875\n",
      "step: 272  loss: 0.15682333707809448  accu: 0.9765625\n",
      "step: 273  loss: 0.1257452815771103  accu: 0.98046875\n",
      "step: 274  loss: 0.1243767961859703  accu: 0.984375\n",
      "step: 275  loss: 0.12947812676429749  accu: 0.982421875\n",
      "step: 276  loss: 0.14255085587501526  accu: 0.970703125\n",
      "step: 277  loss: 0.1408907026052475  accu: 0.978515625\n",
      "step: 278  loss: 0.1317978948354721  accu: 0.974609375\n",
      "step: 279  loss: 0.1042817234992981  accu: 0.982421875\n",
      "step: 280  loss: 0.1358892023563385  accu: 0.98046875\n",
      "step: 281  loss: 0.11402412503957748  accu: 0.9853515625\n",
      "step: 282  loss: 0.1290345937013626  accu: 0.98046875\n",
      "step: 283  loss: 0.13568805158138275  accu: 0.978515625\n",
      "step: 284  loss: 0.13937710225582123  accu: 0.9755859375\n",
      "step: 285  loss: 0.10362749546766281  accu: 0.9853515625\n",
      "step: 286  loss: 0.11685159802436829  accu: 0.978515625\n",
      "step: 287  loss: 0.1121826320886612  accu: 0.9833984375\n",
      "step: 288  loss: 0.11540537327528  accu: 0.984375\n",
      "step: 289  loss: 0.11698023974895477  accu: 0.9853515625\n",
      "step: 290  loss: 0.1150854304432869  accu: 0.9814453125\n",
      "step: 291  loss: 0.098166324198246  accu: 0.986328125\n",
      "step: 292  loss: 0.11812557280063629  accu: 0.986328125\n",
      "step: 293  loss: 0.12932471930980682  accu: 0.9716796875\n",
      "step: 294  loss: 0.09984710067510605  accu: 0.9892578125\n",
      "step: 295  loss: 0.13020139932632446  accu: 0.982421875\n",
      "step: 296  loss: 0.12034372240304947  accu: 0.9833984375\n",
      "step: 297  loss: 0.09801134467124939  accu: 0.9853515625\n",
      "step: 298  loss: 0.09464436769485474  accu: 0.9931640625\n",
      "step: 299  loss: 0.09093447774648666  accu: 0.9912109375\n",
      "step: 300  loss: 0.08815933018922806  accu: 0.994140625\n",
      "\n",
      "\n",
      "Evaluation accuracy: 0.38515381347017935\n",
      "\n",
      "\n",
      "step: 301  loss: 0.11815348267555237  accu: 0.984375\n",
      "step: 302  loss: 0.09879506379365921  accu: 0.9873046875\n",
      "step: 303  loss: 0.09727916866540909  accu: 0.9853515625\n",
      "step: 304  loss: 0.10370448231697083  accu: 0.984375\n",
      "step: 305  loss: 0.11673897504806519  accu: 0.9794921875\n",
      "step: 306  loss: 0.09320975095033646  accu: 0.986328125\n",
      "step: 307  loss: 0.09491986036300659  accu: 0.98828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 308  loss: 0.08454903960227966  accu: 0.9912109375\n",
      "step: 309  loss: 0.12060148268938065  accu: 0.98046875\n",
      "step: 310  loss: 0.08520105481147766  accu: 0.98828125\n",
      "step: 311  loss: 0.09579800814390182  accu: 0.984375\n",
      "step: 312  loss: 0.08985547721385956  accu: 0.990234375\n",
      "step: 313  loss: 0.08534594625234604  accu: 0.9873046875\n",
      "step: 314  loss: 0.09979809075593948  accu: 0.982421875\n",
      "step: 315  loss: 0.08885108679533005  accu: 0.98828125\n",
      "step: 316  loss: 0.0866524800658226  accu: 0.9912109375\n",
      "step: 317  loss: 0.08490806072950363  accu: 0.986328125\n",
      "step: 318  loss: 0.08909434080123901  accu: 0.9873046875\n",
      "step: 319  loss: 0.10428498685359955  accu: 0.98046875\n",
      "step: 320  loss: 0.09092546999454498  accu: 0.9873046875\n",
      "step: 321  loss: 0.12563425302505493  accu: 0.9873046875\n",
      "step: 322  loss: 0.09819359332323074  accu: 0.984375\n",
      "step: 323  loss: 0.08511895686388016  accu: 0.9873046875\n",
      "step: 324  loss: 0.10679618269205093  accu: 0.98046875\n",
      "step: 325  loss: 0.08973217010498047  accu: 0.98828125\n",
      "step: 326  loss: 0.10385342687368393  accu: 0.98046875\n",
      "step: 327  loss: 0.0858520120382309  accu: 0.9873046875\n",
      "step: 328  loss: 0.08672516793012619  accu: 0.98828125\n",
      "step: 329  loss: 0.08440376073122025  accu: 0.986328125\n",
      "step: 330  loss: 0.08257055282592773  accu: 0.990234375\n",
      "step: 331  loss: 0.07312262058258057  accu: 0.9912109375\n",
      "step: 332  loss: 0.08408057689666748  accu: 0.990234375\n",
      "step: 333  loss: 0.08563163131475449  accu: 0.9892578125\n",
      "step: 334  loss: 0.07127715647220612  accu: 0.9912109375\n",
      "step: 335  loss: 0.0742734745144844  accu: 0.9912109375\n",
      "step: 336  loss: 0.08930175751447678  accu: 0.9873046875\n",
      "step: 337  loss: 0.09799022227525711  accu: 0.9833984375\n",
      "step: 338  loss: 0.08122071623802185  accu: 0.990234375\n",
      "step: 339  loss: 0.09433721005916595  accu: 0.9853515625\n",
      "step: 340  loss: 0.07031979411840439  accu: 0.990234375\n",
      "step: 341  loss: 0.10858901590108871  accu: 0.984375\n",
      "step: 342  loss: 0.0793522372841835  accu: 0.9892578125\n",
      "step: 343  loss: 0.0673750787973404  accu: 0.9912109375\n",
      "step: 344  loss: 0.08799808472394943  accu: 0.9853515625\n",
      "step: 345  loss: 0.09305369853973389  accu: 0.9873046875\n",
      "step: 346  loss: 0.06474415212869644  accu: 0.9912109375\n",
      "step: 347  loss: 0.08589844405651093  accu: 0.986328125\n",
      "step: 348  loss: 0.07042119652032852  accu: 0.9931640625\n",
      "step: 349  loss: 0.06680075824260712  accu: 0.9921875\n",
      "step: 350  loss: 0.08424656838178635  accu: 0.984375\n",
      "step: 351  loss: 0.0671139732003212  accu: 0.994140625\n",
      "step: 352  loss: 0.06601846218109131  accu: 0.98828125\n",
      "step: 353  loss: 0.07852872461080551  accu: 0.98828125\n",
      "step: 354  loss: 0.07513970881700516  accu: 0.9912109375\n",
      "step: 355  loss: 0.07219558954238892  accu: 0.9912109375\n",
      "step: 356  loss: 0.08148004859685898  accu: 0.990234375\n",
      "step: 357  loss: 0.06424686312675476  accu: 0.990234375\n",
      "step: 358  loss: 0.07894132286310196  accu: 0.990234375\n",
      "step: 359  loss: 0.07669151574373245  accu: 0.9921875\n",
      "step: 360  loss: 0.07466474920511246  accu: 0.9873046875\n",
      "step: 361  loss: 0.0770813450217247  accu: 0.98828125\n",
      "step: 362  loss: 0.06878941506147385  accu: 0.9931640625\n",
      "step: 363  loss: 0.06053838133811951  accu: 0.9931640625\n",
      "step: 364  loss: 0.08453986793756485  accu: 0.984375\n",
      "step: 365  loss: 0.06823181360960007  accu: 0.994140625\n",
      "step: 366  loss: 0.0613914392888546  accu: 0.9912109375\n",
      "step: 367  loss: 0.07334205508232117  accu: 0.984375\n",
      "step: 368  loss: 0.06102750822901726  accu: 0.9912109375\n",
      "step: 369  loss: 0.08210215717554092  accu: 0.9853515625\n",
      "step: 370  loss: 0.08698297291994095  accu: 0.9873046875\n",
      "step: 371  loss: 0.08196553587913513  accu: 0.986328125\n",
      "step: 372  loss: 0.06128404662013054  accu: 0.9912109375\n",
      "step: 373  loss: 0.060023173689842224  accu: 0.9931640625\n",
      "step: 374  loss: 0.057614970952272415  accu: 0.9951171875\n",
      "step: 375  loss: 0.06814081221818924  accu: 0.9873046875\n",
      "step: 376  loss: 0.07871731370687485  accu: 0.9892578125\n",
      "step: 377  loss: 0.0537932887673378  accu: 0.9912109375\n",
      "step: 378  loss: 0.06080125644803047  accu: 0.990234375\n",
      "step: 379  loss: 0.07258106768131256  accu: 0.9892578125\n",
      "step: 380  loss: 0.05128835514187813  accu: 0.9970703125\n",
      "step: 381  loss: 0.07638485729694366  accu: 0.990234375\n",
      "step: 382  loss: 0.0793713703751564  accu: 0.9853515625\n",
      "step: 383  loss: 0.08477867394685745  accu: 0.986328125\n",
      "step: 384  loss: 0.08097714185714722  accu: 0.986328125\n",
      "step: 385  loss: 0.06708336621522903  accu: 0.9873046875\n",
      "step: 386  loss: 0.07015754282474518  accu: 0.990234375\n",
      "step: 387  loss: 0.09364549070596695  accu: 0.9853515625\n",
      "step: 388  loss: 0.06137603893876076  accu: 0.994140625\n",
      "step: 389  loss: 0.05511804297566414  accu: 0.9931640625\n",
      "step: 390  loss: 0.06068957597017288  accu: 0.9921875\n",
      "step: 391  loss: 0.07883863151073456  accu: 0.9853515625\n",
      "step: 392  loss: 0.07753592729568481  accu: 0.98828125\n",
      "step: 393  loss: 0.0536460354924202  accu: 0.9951171875\n",
      "step: 394  loss: 0.07079364359378815  accu: 0.986328125\n",
      "step: 395  loss: 0.06320525705814362  accu: 0.9892578125\n",
      "step: 396  loss: 0.09092357754707336  accu: 0.986328125\n",
      "step: 397  loss: 0.05514761433005333  accu: 0.9921875\n",
      "step: 398  loss: 0.0634068101644516  accu: 0.9921875\n",
      "step: 399  loss: 0.05397019907832146  accu: 0.9931640625\n",
      "step: 400  loss: 0.06466073542833328  accu: 0.986328125\n",
      "\n",
      "\n",
      "Evaluation accuracy: 0.3805622483020414\n",
      "\n",
      "\n",
      "step: 401  loss: 0.0479816310107708  accu: 0.9951171875\n",
      "step: 402  loss: 0.0578218549489975  accu: 0.994140625\n",
      "step: 403  loss: 0.052776265889406204  accu: 0.9951171875\n",
      "step: 404  loss: 0.05044863745570183  accu: 0.9951171875\n",
      "step: 405  loss: 0.06315775960683823  accu: 0.990234375\n",
      "step: 406  loss: 0.06337596476078033  accu: 0.9912109375\n",
      "step: 407  loss: 0.06467940658330917  accu: 0.9921875\n",
      "step: 408  loss: 0.0721505805850029  accu: 0.986328125\n",
      "step: 409  loss: 0.059182148426771164  accu: 0.990234375\n",
      "step: 410  loss: 0.08665110170841217  accu: 0.990234375\n",
      "step: 411  loss: 0.056171417236328125  accu: 0.9921875\n",
      "step: 412  loss: 0.05832468718290329  accu: 0.9912109375\n",
      "step: 413  loss: 0.04771803691983223  accu: 0.998046875\n",
      "step: 414  loss: 0.06045107915997505  accu: 0.990234375\n",
      "step: 415  loss: 0.0867062658071518  accu: 0.9873046875\n",
      "step: 416  loss: 0.06529997289180756  accu: 0.9931640625\n",
      "step: 417  loss: 0.044301871210336685  accu: 0.9951171875\n",
      "step: 418  loss: 0.0687938928604126  accu: 0.9921875\n",
      "step: 419  loss: 0.06234213337302208  accu: 0.9892578125\n",
      "step: 420  loss: 0.06390997022390366  accu: 0.9892578125\n",
      "step: 421  loss: 0.06535710394382477  accu: 0.9892578125\n",
      "step: 422  loss: 0.06215351074934006  accu: 0.990234375\n",
      "step: 423  loss: 0.03942016139626503  accu: 0.9970703125\n",
      "step: 424  loss: 0.04793413355946541  accu: 0.99609375\n",
      "step: 425  loss: 0.06486167013645172  accu: 0.98828125\n",
      "step: 426  loss: 0.06608108431100845  accu: 0.9873046875\n",
      "step: 427  loss: 0.07809412479400635  accu: 0.9853515625\n",
      "step: 428  loss: 0.06567264348268509  accu: 0.9921875\n",
      "step: 429  loss: 0.046887509524822235  accu: 0.9970703125\n",
      "step: 430  loss: 0.04047005623579025  accu: 0.9990234375\n",
      "step: 431  loss: 0.0692085549235344  accu: 0.9921875\n",
      "step: 432  loss: 0.044113729149103165  accu: 0.9970703125\n",
      "step: 433  loss: 0.07611313462257385  accu: 0.98828125\n",
      "step: 434  loss: 0.07685721665620804  accu: 0.9873046875\n",
      "step: 435  loss: 0.04878847673535347  accu: 0.9951171875\n",
      "step: 436  loss: 0.059887174516916275  accu: 0.9912109375\n",
      "step: 437  loss: 0.04255996644496918  accu: 0.99609375\n",
      "step: 438  loss: 0.045454591512680054  accu: 0.99609375\n",
      "step: 439  loss: 0.04738953709602356  accu: 0.9931640625\n",
      "step: 440  loss: 0.0464935265481472  accu: 0.994140625\n",
      "step: 441  loss: 0.04658924788236618  accu: 0.9951171875\n",
      "step: 442  loss: 0.058559782803058624  accu: 0.9912109375\n",
      "step: 443  loss: 0.043113842606544495  accu: 0.99609375\n",
      "step: 444  loss: 0.060384687036275864  accu: 0.994140625\n",
      "step: 445  loss: 0.06361042708158493  accu: 0.990234375\n",
      "step: 446  loss: 0.05930691584944725  accu: 0.9892578125\n",
      "step: 447  loss: 0.06002644822001457  accu: 0.98828125\n",
      "step: 448  loss: 0.0552368089556694  accu: 0.994140625\n",
      "step: 449  loss: 0.06076433137059212  accu: 0.9912109375\n",
      "step: 450  loss: 0.04220670089125633  accu: 0.994140625\n",
      "step: 451  loss: 0.0620371475815773  accu: 0.9931640625\n",
      "step: 452  loss: 0.05652270093560219  accu: 0.9912109375\n",
      "step: 453  loss: 0.06102685257792473  accu: 0.9921875\n",
      "step: 454  loss: 0.057407185435295105  accu: 0.990234375\n",
      "step: 455  loss: 0.053270939737558365  accu: 0.9921875\n",
      "step: 456  loss: 0.04169763624668121  accu: 0.9931640625\n",
      "step: 457  loss: 0.04096585512161255  accu: 0.994140625\n",
      "step: 458  loss: 0.05806155502796173  accu: 0.9873046875\n",
      "step: 459  loss: 0.0377197302877903  accu: 0.994140625\n",
      "step: 460  loss: 0.04247045889496803  accu: 0.99609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 461  loss: 0.06919383257627487  accu: 0.9892578125\n",
      "step: 462  loss: 0.06006919592618942  accu: 0.98828125\n",
      "step: 463  loss: 0.04837641492486  accu: 0.99609375\n",
      "step: 464  loss: 0.049837395548820496  accu: 0.9921875\n",
      "step: 465  loss: 0.044343285262584686  accu: 0.9951171875\n",
      "step: 466  loss: 0.04446916654706001  accu: 0.9931640625\n",
      "step: 467  loss: 0.03809773176908493  accu: 0.9931640625\n",
      "step: 468  loss: 0.04920855164527893  accu: 0.9951171875\n",
      "step: 469  loss: 0.03982819616794586  accu: 0.99609375\n",
      "step: 470  loss: 0.04347032308578491  accu: 0.994140625\n",
      "step: 471  loss: 0.044612783938646317  accu: 0.9970703125\n",
      "step: 472  loss: 0.04375750198960304  accu: 0.994140625\n",
      "step: 473  loss: 0.034561432898044586  accu: 0.998046875\n",
      "step: 474  loss: 0.04676268622279167  accu: 0.99609375\n",
      "step: 475  loss: 0.04529407247900963  accu: 0.9970703125\n",
      "step: 476  loss: 0.04496853053569794  accu: 0.9931640625\n",
      "step: 477  loss: 0.050967857241630554  accu: 0.9921875\n",
      "step: 478  loss: 0.04266749322414398  accu: 0.9951171875\n",
      "step: 479  loss: 0.04337001219391823  accu: 0.9912109375\n",
      "step: 480  loss: 0.05369235575199127  accu: 0.9931640625\n",
      "step: 481  loss: 0.04119660332798958  accu: 0.9921875\n",
      "step: 482  loss: 0.034183140844106674  accu: 0.9970703125\n",
      "step: 483  loss: 0.04326579347252846  accu: 0.994140625\n",
      "step: 484  loss: 0.04307303950190544  accu: 0.994140625\n",
      "step: 485  loss: 0.04277963563799858  accu: 0.994140625\n",
      "step: 486  loss: 0.05519302934408188  accu: 0.9912109375\n",
      "step: 487  loss: 0.056689027696847916  accu: 0.9873046875\n",
      "step: 488  loss: 0.055321384221315384  accu: 0.9921875\n",
      "step: 489  loss: 0.04348786920309067  accu: 0.9951171875\n",
      "step: 490  loss: 0.04121923819184303  accu: 0.9951171875\n",
      "step: 491  loss: 0.045832354575395584  accu: 0.99609375\n",
      "step: 492  loss: 0.044143591076135635  accu: 0.994140625\n",
      "step: 493  loss: 0.037241365760564804  accu: 0.9931640625\n",
      "step: 494  loss: 0.03875495120882988  accu: 0.9970703125\n",
      "step: 495  loss: 0.04297094792127609  accu: 0.994140625\n",
      "step: 496  loss: 0.045342277735471725  accu: 0.9931640625\n",
      "step: 497  loss: 0.054601866751909256  accu: 0.9912109375\n",
      "step: 498  loss: 0.03736589103937149  accu: 0.9951171875\n",
      "step: 499  loss: 0.03224515914916992  accu: 0.9951171875\n",
      "step: 500  loss: 0.04734817519783974  accu: 0.9931640625\n",
      "\n",
      "\n",
      "Evaluation accuracy: 0.3663919458293034\n",
      "\n",
      "\n",
      "step: 501  loss: 0.058925237506628036  accu: 0.9873046875\n",
      "step: 502  loss: 0.04482758790254593  accu: 0.9921875\n",
      "step: 503  loss: 0.03650104999542236  accu: 0.99609375\n",
      "step: 504  loss: 0.03699547052383423  accu: 0.994140625\n",
      "step: 505  loss: 0.051225341856479645  accu: 0.9951171875\n",
      "step: 506  loss: 0.04162849858403206  accu: 0.99609375\n",
      "step: 507  loss: 0.048296600580215454  accu: 0.9931640625\n",
      "step: 508  loss: 0.042612817138433456  accu: 0.9931640625\n",
      "step: 509  loss: 0.0565006248652935  accu: 0.98828125\n",
      "step: 510  loss: 0.047602467238903046  accu: 0.9931640625\n",
      "step: 511  loss: 0.05846446752548218  accu: 0.9931640625\n",
      "step: 512  loss: 0.030896389856934547  accu: 0.9970703125\n",
      "step: 513  loss: 0.03132769465446472  accu: 0.99609375\n",
      "step: 514  loss: 0.04886208102107048  accu: 0.9931640625\n",
      "step: 515  loss: 0.04329432174563408  accu: 0.9951171875\n",
      "step: 516  loss: 0.03573870658874512  accu: 0.9970703125\n",
      "step: 517  loss: 0.048397451639175415  accu: 0.9931640625\n",
      "step: 518  loss: 0.038872506469488144  accu: 0.9951171875\n",
      "step: 519  loss: 0.04879258573055267  accu: 0.9912109375\n",
      "step: 520  loss: 0.04189510643482208  accu: 0.9951171875\n",
      "step: 521  loss: 0.0418655127286911  accu: 0.990234375\n",
      "step: 522  loss: 0.03437133505940437  accu: 0.9951171875\n",
      "step: 523  loss: 0.038956597447395325  accu: 0.998046875\n",
      "step: 524  loss: 0.04424453526735306  accu: 0.9931640625\n",
      "step: 525  loss: 0.05015210062265396  accu: 0.9912109375\n",
      "step: 526  loss: 0.042874667793512344  accu: 0.994140625\n",
      "step: 527  loss: 0.041487473994493484  accu: 0.994140625\n",
      "step: 528  loss: 0.03082263097167015  accu: 0.998046875\n",
      "step: 529  loss: 0.0323914960026741  accu: 0.99609375\n",
      "step: 530  loss: 0.04494678974151611  accu: 0.994140625\n",
      "step: 531  loss: 0.036280158907175064  accu: 0.99609375\n",
      "step: 532  loss: 0.0392904169857502  accu: 0.9921875\n",
      "step: 533  loss: 0.04946426302194595  accu: 0.9912109375\n",
      "step: 534  loss: 0.041987478733062744  accu: 0.9951171875\n",
      "step: 535  loss: 0.04994189366698265  accu: 0.9912109375\n",
      "step: 536  loss: 0.033721692860126495  accu: 0.9970703125\n",
      "step: 537  loss: 0.026243600994348526  accu: 0.9970703125\n",
      "step: 538  loss: 0.039401039481163025  accu: 0.998046875\n",
      "step: 539  loss: 0.03636245056986809  accu: 0.9951171875\n",
      "step: 540  loss: 0.033195678144693375  accu: 0.994140625\n",
      "step: 541  loss: 0.03794551640748978  accu: 0.9951171875\n",
      "step: 542  loss: 0.05370059236884117  accu: 0.990234375\n",
      "step: 543  loss: 0.04156358167529106  accu: 0.9970703125\n",
      "step: 544  loss: 0.0427527129650116  accu: 0.9931640625\n",
      "step: 545  loss: 0.0407370924949646  accu: 0.99609375\n",
      "step: 546  loss: 0.04467928409576416  accu: 0.9931640625\n",
      "step: 547  loss: 0.029068300500512123  accu: 0.9970703125\n",
      "step: 548  loss: 0.03615531325340271  accu: 0.9951171875\n",
      "step: 549  loss: 0.03223884105682373  accu: 0.9951171875\n",
      "step: 550  loss: 0.027826430276036263  accu: 0.998046875\n",
      "step: 551  loss: 0.033360257744789124  accu: 0.9951171875\n",
      "step: 552  loss: 0.03068975731730461  accu: 0.99609375\n",
      "step: 553  loss: 0.04539617896080017  accu: 0.9931640625\n",
      "step: 554  loss: 0.042276397347450256  accu: 0.994140625\n",
      "step: 555  loss: 0.04068389907479286  accu: 0.994140625\n",
      "step: 556  loss: 0.037922777235507965  accu: 0.9951171875\n",
      "step: 557  loss: 0.031243767589330673  accu: 0.994140625\n",
      "step: 558  loss: 0.04728752747178078  accu: 0.9921875\n",
      "step: 559  loss: 0.04998244345188141  accu: 0.9912109375\n",
      "step: 560  loss: 0.028959831222891808  accu: 0.998046875\n",
      "step: 561  loss: 0.030664127320051193  accu: 0.99609375\n",
      "step: 562  loss: 0.04343825951218605  accu: 0.9970703125\n",
      "step: 563  loss: 0.02629263326525688  accu: 0.9990234375\n",
      "step: 564  loss: 0.035810645669698715  accu: 0.9951171875\n",
      "step: 565  loss: 0.0390901118516922  accu: 0.9931640625\n",
      "step: 566  loss: 0.032906677573919296  accu: 0.9970703125\n",
      "step: 567  loss: 0.03650827705860138  accu: 0.9951171875\n",
      "step: 568  loss: 0.03497892990708351  accu: 0.994140625\n",
      "step: 569  loss: 0.03191530331969261  accu: 0.9970703125\n",
      "step: 570  loss: 0.038168519735336304  accu: 0.9931640625\n",
      "step: 571  loss: 0.04198908805847168  accu: 0.994140625\n",
      "step: 572  loss: 0.03730258718132973  accu: 0.9951171875\n",
      "step: 573  loss: 0.04541408643126488  accu: 0.9931640625\n",
      "step: 574  loss: 0.05378574877977371  accu: 0.9912109375\n",
      "step: 575  loss: 0.03906916454434395  accu: 0.994140625\n",
      "step: 576  loss: 0.04425206780433655  accu: 0.9951171875\n",
      "step: 577  loss: 0.03831547126173973  accu: 0.9951171875\n",
      "step: 578  loss: 0.04806629940867424  accu: 0.9931640625\n",
      "step: 579  loss: 0.04775853082537651  accu: 0.9921875\n",
      "step: 580  loss: 0.03503105044364929  accu: 0.99609375\n",
      "step: 581  loss: 0.048152487725019455  accu: 0.9921875\n",
      "step: 582  loss: 0.03638261929154396  accu: 0.99609375\n",
      "step: 583  loss: 0.039302341639995575  accu: 0.9931640625\n",
      "step: 584  loss: 0.04511776939034462  accu: 0.9912109375\n",
      "step: 585  loss: 0.033617131412029266  accu: 0.994140625\n",
      "step: 586  loss: 0.034514009952545166  accu: 0.99609375\n",
      "step: 587  loss: 0.043790627270936966  accu: 0.9931640625\n",
      "step: 588  loss: 0.032858189195394516  accu: 0.994140625\n",
      "step: 589  loss: 0.03626727685332298  accu: 0.9951171875\n",
      "step: 590  loss: 0.031890369951725006  accu: 0.99609375\n",
      "step: 591  loss: 0.04614453390240669  accu: 0.9912109375\n",
      "step: 592  loss: 0.033554982393980026  accu: 0.9970703125\n",
      "step: 593  loss: 0.043606724590063095  accu: 0.994140625\n",
      "step: 594  loss: 0.02662700042128563  accu: 0.9951171875\n",
      "step: 595  loss: 0.025610554963350296  accu: 0.99609375\n",
      "step: 596  loss: 0.033584583550691605  accu: 0.99609375\n",
      "step: 597  loss: 0.03892361372709274  accu: 0.9912109375\n",
      "step: 598  loss: 0.043590277433395386  accu: 0.9921875\n",
      "step: 599  loss: 0.04106634110212326  accu: 0.9921875\n",
      "step: 600  loss: 0.04176342487335205  accu: 0.9951171875\n",
      "\n",
      "\n",
      "Evaluation accuracy: 0.3488402158890388\n",
      "\n",
      "\n",
      "step: 601  loss: 0.03365820646286011  accu: 0.9951171875\n",
      "step: 602  loss: 0.04971948638558388  accu: 0.9931640625\n",
      "step: 603  loss: 0.04311833530664444  accu: 0.99609375\n",
      "step: 604  loss: 0.03114090859889984  accu: 0.9951171875\n",
      "step: 605  loss: 0.038376811891794205  accu: 0.9921875\n",
      "step: 606  loss: 0.03859258443117142  accu: 0.9951171875\n",
      "step: 607  loss: 0.034628063440322876  accu: 0.9970703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 608  loss: 0.04323248937726021  accu: 0.9931640625\n",
      "step: 609  loss: 0.03463099151849747  accu: 0.99609375\n",
      "step: 610  loss: 0.027712874114513397  accu: 0.998046875\n",
      "step: 611  loss: 0.055524345487356186  accu: 0.98828125\n",
      "step: 612  loss: 0.031027166172862053  accu: 0.99609375\n",
      "step: 613  loss: 0.04517051950097084  accu: 0.9892578125\n",
      "step: 614  loss: 0.026956038549542427  accu: 0.9970703125\n",
      "step: 615  loss: 0.027233174070715904  accu: 0.9970703125\n",
      "step: 616  loss: 0.04304413124918938  accu: 0.9921875\n",
      "step: 617  loss: 0.05577976256608963  accu: 0.9912109375\n",
      "step: 618  loss: 0.03609348088502884  accu: 0.994140625\n",
      "step: 619  loss: 0.042129334062337875  accu: 0.9951171875\n",
      "step: 620  loss: 0.05439337342977524  accu: 0.9892578125\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "class Engine():\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        self.dataGen = DataGen(self.config)\n",
    "        self.dataGen.read_data()\n",
    "        self.sess = None\n",
    "        self.global_step = 0\n",
    "        \n",
    "    def train_step(self, sess, train_op, train_model, params):\n",
    "        \n",
    "        feed_dict = {\n",
    "            train_model.inputs: params[\"source_batch\"],\n",
    "            train_model.targets: params[\"target_batch\"],\n",
    "            train_model.dropout_prob: self.config.model.dropout_prob,\n",
    "            train_model.source_sequence_length: params[\"source_sequence_length\"],\n",
    "            train_model.target_sequence_length: params[\"target_sequence_length\"],\n",
    "        }\n",
    "        \n",
    "        _, loss, accu = sess.run([train_op, train_model.loss, train_model.accu], feed_dict)\n",
    "        \n",
    "        return loss, accu\n",
    "    \n",
    "    def infer_step(self, sess, infer_model, params):\n",
    "        \n",
    "        feed_dict = {\n",
    "            infer_model.inputs: params[\"source_batch\"],\n",
    "            infer_model.targets: params[\"target_batch\"],\n",
    "            infer_model.dropout_prob: 1.0,\n",
    "            infer_model.source_sequence_length: params[\"source_sequence_length\"],\n",
    "            infer_model.target_sequence_length: params[\"target_sequence_length\"],\n",
    "        }\n",
    "        \n",
    "        logits = sess.run([infer_model.infer_logits], feed_dict)\n",
    "        predictions = logits[0]\n",
    "        \n",
    "        prediction = [sequence[:end] for sequence in predictions for end in params[\"target_sequence_length\"]]\n",
    "        target = [sequence[:end] for sequence in params[\"target_batch\"] for end in params[\"target_sequence_length\"]]\n",
    "        \n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i in range(len(prediction)):\n",
    "            for j in range(len(prediction[i])):\n",
    "                if prediction[i][j] == target[i][j]:\n",
    "                    correct += 1\n",
    "            total += len(prediction[i])\n",
    "            \n",
    "        accu = correct / total\n",
    "        \n",
    "        return accu\n",
    "    \n",
    "    def run_epoch(self):\n",
    "        config = self.config\n",
    "        dataGen = self.dataGen\n",
    "        \n",
    "        source_data = dataGen.source_data\n",
    "        target_data = dataGen.target_data\n",
    "        \n",
    "        train_split = int(len(source_data) * config.infer_prob)\n",
    "        \n",
    "        train_source_data = source_data[train_split:]\n",
    "        infer_source_data = source_data[: train_split]\n",
    "        \n",
    "        train_target_data = target_data[train_split:]\n",
    "        infer_target_data = target_data[: train_split]\n",
    "        \n",
    "        source_char_to_int = dataGen.source_char_to_int\n",
    "        target_char_to_int = dataGen.target_char_to_int\n",
    "        \n",
    "        encoder_vocab_size = len(source_char_to_int)\n",
    "        \n",
    "        batch_size = config.batch_size\n",
    "        \n",
    "        with tf.Graph().as_default():\n",
    "            session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "            sess = tf.Session(config=session_conf)\n",
    "            with sess.as_default():\n",
    "                with tf.name_scope(\"train\"):\n",
    "                    with tf.variable_scope(\"seq2seq\"):\n",
    "                        train_model = Seq2SeqModel(config, encoder_vocab_size, target_char_to_int, is_infer=False)\n",
    "                        \n",
    "                with tf.name_scope(\"infer\"):\n",
    "                    with tf.variable_scope(\"seq2seq\", reuse=True):\n",
    "                        infer_model = Seq2SeqModel(config, encoder_vocab_size, target_char_to_int, is_infer=True)\n",
    "                \n",
    "                global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "                \n",
    "                optimizer = tf.train.AdamOptimizer(config.train.learning_rate)\n",
    "                grads_and_vars = optimizer.compute_gradients(train_model.loss)\n",
    "                grads_and_vars = [(tf.clip_by_norm(g, config.train.max_grad_norm), v) for g, v in grads_and_vars if g is not None]\n",
    "                train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name=\"train_op\")\n",
    "                \n",
    "                saver = tf.train.Saver(tf.global_variables())\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                print(\"初始化完成，开始训练模型\")\n",
    "                for i in range(config.train.epochs):\n",
    "                    for params in next_batch(train_source_data, train_target_data, batch_size, source_char_to_int, target_char_to_int):\n",
    "                        loss, accu = self.train_step(sess, train_op, train_model, params)\n",
    "                        current_step = tf.train.global_step(sess, global_step)\n",
    "                        print(\"step: {}  loss: {}  accu: {}\".format(current_step, loss, accu))\n",
    "                        \n",
    "                        if current_step % config.train.every_checkpoint == 0:\n",
    "                            accus = []\n",
    "                            for params in next_batch(infer_source_data, infer_target_data, batch_size, source_char_to_int, target_char_to_int):\n",
    "                                accu = self.infer_step(sess, infer_model, params)\n",
    "                                accus.append(accu)\n",
    "                            print(\"\\n\")\n",
    "                            print(\"Evaluation accuracy: {}\".format(sum(accus) / len(accus)))\n",
    "                            print(\"\\n\")\n",
    "                            saver.save(sess, \"model/my-model\", global_step=current_step)\n",
    "                            \n",
    "engine = Engine()\n",
    "engine.run_epoch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
